### Example config for threshold estimation from single observations.
# Assuming you are doing threshold estimation and the intensity dimension
# is the final dimension, the only things that need to be changed for a
# typical experiment are:
# 1. parnames, lb and ub under [common], and optionally target.
# 2. n_trials under SobolStrategy.
# 3. n_trials under ModelWrapperStrategy.

## The common section includes global server parameters and parameters
## reused in multiple other classes
[common]
parnames = [par1, par2] # names of the parameters
lb = [0, 0] # lower bounds of the parameters, in the same order as above
ub = [1, 1] # upper bounds of parameter, in the same order as above
outcome_type = single_probit # currently this is always single_probit (standard "1AFC" in psychophysics)
target = 0.75 # desired threshold, for threshold estimation. Otherwise is ignored or can be removed.

# Configuration for the initialization strategy, which we use to gather initial points
# before we start doing model-based acquisition
[SobolStrategy]
n_trials = 10 # number of sobol trials to run

# Configuration for the optimization strategy, our model-based acquisition
[ModelWrapperStrategy]
# number of model-based trials to run
n_trials = 20
# On every trial, AEPsych can either initialize model hyperparameters randomly and refit
# from scratch, or initialize them to the previous model's parameters. This governs
# how often the former happens vs the latter. Refitting is often faster (leading to less
# waiting between trials) but occasional refitting is useful for avoiding local minima
# in hyperparameters, especially early in the experiment.
refit_every = 5

## The experiment section covers the main components of the experiment:
## the strategies used for initialization and optimization, the
## modelbridge and model used for the modeling part, and the acquisition
## function
[experiment]
# Acquisition function: the objective we use to decide where to sample next. We recommend
# MonotonicMCLSE for threshold finding, MonotonicMCPosteriorVariance for global exploration,
# qNoisyExpectedImprovement for optimization. For other options, check out the botorch and
# aepsych docs
acqf = MonotonicMCLSE
# initialization strategy. Currently SobolStrategy (a quasi-random grid) is the only
# available option here.
init_strat_cls = SobolStrategy
# Optimization strategy. ModelWrapperStrategy is the default. The other supported option
# is EpsilonGreedyModelWrapperStrategy, which interleaves the model with a random trial
# with probability=epsilon (by default 0.1).
opt_strat_cls = ModelWrapperStrategy
# Modelbridge: the wrapper that combines model and acquisition together. The
# options are SingleProbitModelbridge (nonmonotonic baseline; also used for the linear-additive
# model of Song et al.), MonotonicSingleProbitModelbridge (our monotonic model), and
# SingleProbitModelbridgeWithSongHeuristic, which adds the exploration heuristic from
# Song et al. 2018.
modelbridge_cls = MonotonicSingleProbitModelbridge
# The model used as part of the modelbridge. MonotonicRejectionGP or MonotonicRejectionGPLSETS
# are the options for monotonic, otherwise GPClassificationModel.
model = MonotonicRejectionGP

## Below this section are configurations of all the classes defined in the section above,
## matching the API in the code.

## Acquisition function settings; we recommend not changing this.
[MonotonicMCLSE]
# A sort of "temperature" parameter that affects how widely around the predicted
# threshold we try to sample, analogous to beta in the UCB family of algorithms.
# The 3.98 default corresponds to the "straddle" heuristic in Bryan et al. 2005.
beta = 3.98
# The transformation of the latent function before threshold estimation. ProbitObjective
# lets us express the target threshold in probability.
objective = ProbitObjective

## MonotonicRejectionGP model settings.
[MonotonicRejectionGP]
# Number of inducing points for approximate inference. 100 is fine for 2d and overkill for 1d;
# for larger dimensions, scale this up.
inducing_size = 100
# A "factory" function (defined in aepsych) that builds the GP mean and covariance.
# other options are default_mean_covar_factory (for non-monotonic RBF kernel, for use with
# GPClassificationModel) and song_mean_covar_factory (for the linear-additive model of some
# previous work).
mean_covar_factory = monotonic_mean_covar_factory
# The set of monotonic indices, in 0-based indexing. The model works fine for any indices
# here but some of the AEPsych plotting code assumes there is only one monotonic dimension
# and it's the last dimension.
monotonic_idxs = [1]

## Modelbridge settings, governing acquisition function optimization.
[MonotonicSingleProbitModelbridge]
# number of restarts for acquisition function optimization.
restarts = 10
# number of samples for quasi-random initialization of the acquisition function optimizer
samps = 1000
