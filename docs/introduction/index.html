<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Introduction · AEPsych</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="AEPsych (short for **A**daptive **E**xperimentation in **Psych**ophysics) is a modeling and human-in-the-loop experimentation framework focused on human psychophysics, as well as related domains such as perceptually-informed preferences. It combines state of the art flexible models for the psychometric field, novel active learning objectives targeting psychophysics problems like threshold estimation, and a client-server architecture that supports stimulus presentation using standard packages such as PsychToolbox."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Introduction · AEPsych"/><meta property="og:type" content="website"/><meta property="og:url" content="https://aepsych.org/"/><meta property="og:description" content="AEPsych (short for **A**daptive **E**xperimentation in **Psych**ophysics) is a modeling and human-in-the-loop experimentation framework focused on human psychophysics, as well as related domains such as perceptually-informed preferences. It combines state of the art flexible models for the psychometric field, novel active learning objectives targeting psychophysics problems like threshold estimation, and a client-server architecture that supports stimulus presentation using standard packages such as PsychToolbox."/><meta property="og:image" content="https://aepsych.org/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://aepsych.org/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/code_block_buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/cookie_consent.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/cookie_consent.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="https://unpkg.com/vanilla-back-to-top@7.1.14/dist/vanilla-back-to-top.min.js"></script><script>
        document.addEventListener('DOMContentLoaded', function() {
          addBackToTop(
            {"zIndex":100}
          )
        });
        </script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/static-logo.png" alt="AEPsych"/><h2 class="headerTitleWithLogo">AEPsych</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/demos/" target="_self">Demos</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/docs/introduction" target="_self">Docs</a></li><li class=""><a href="/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/api/" target="_self">API Reference</a></li><li class=""><a href="https://github.com/facebookresearch/aepsych" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>About</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">About</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/docs/introduction">Introduction</a></li><li class="navListItem"><a class="navItem" href="/docs/papers">Papers related to AEPsych</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">General</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/getting_started">Getting Started with AEPsych</a></li><li class="navListItem"><a class="navItem" href="/docs/configs">Writing Config Files</a></li><li class="navListItem"><a class="navItem" href="/docs/clients">AEPsych clients</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Background materials</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/psychophysics_intro">A brief introduction to Psychophysics</a></li><li class="navListItem"><a class="navItem" href="/docs/gp_intro">A brief introduction to Gaussian Process active learning</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">For developers</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/api_overview">API Overview</a></li><li class="navListItem"><a class="navItem" href="/docs/db_overview">Database Overview</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Advanced topics</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/finish_criteria">Advanced Strategy Configuration</a></li><li class="navListItem"><a class="navItem" href="/docs/parameters">Advanced Parameter Configuration</a></li><li class="navListItem"><a class="navItem" href="/docs/speed">Active Learning Speedups</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/facebookresearch/aepsych/blob/main/docs/introduction.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 id="__docusaurus" class="postHeaderTitle">Introduction</h1></header><article><div><span><p>AEPsych (short for <strong>A</strong>daptive <strong>E</strong>xperimentation in <strong>Psych</strong>ophysics) is a modeling and human-in-the-loop experimentation framework focused on human psychophysics, as well as related domains such as perceptually-informed preferences. It combines state of the art flexible models for the psychometric field, novel active learning objectives targeting psychophysics problems like threshold estimation, and a client-server architecture that supports stimulus presentation using standard packages such as PsychToolbox.</p>
<h2><a class="anchor" aria-hidden="true" id="why-aepsych-for-psychophysics-researchers"></a><a href="#why-aepsych-for-psychophysics-researchers" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why AEPsych (for psychophysics researchers)?</h2>
<ul>
<li><strong>High-d psychophysics in tractable time.</strong> AEPsych shines in high-dimensional psychophysics (as many as 6 or 8 dimensions such as stimulus orientation, size, eccentricity or temporal frequency for visual stimuli, frequency for auditory or vibrotactile stimuli, etc). In these settings, it can be used to characterize psychometric functions orders of magnitude faster than the method of constant stimuli, and substantially faster than staircase methods as well.</li>
<li><strong>As flexible as MOCS, faster than staircases</strong>. Unlike parametric or heuristic staircase methods, the models in AEPsych make fewer assumptions about the psychometric function, which makes them safer to use in domains where we don’t know if Weber-type scaling laws will hold. At the same time, the use of flexible interpolation and model-based active learning means data collections can take orders of magnitude less time than the method of constant stimuli.</li>
<li><strong>Always estimate the psychometric field.</strong> AEPsych always builds a model of the full psychometric field, so even if you focus on threshold estimation you can get some useful signal on JNDs and other properties.</li>
<li><strong>Plotting and interactive visualization built in</strong>. Spend more time interpreting your data and less time with boilerplate data loading, saving, and plotting code.</li>
<li><strong>Use whatever stimulus display package you like.</strong> While the AEPsych modeling is in PyTorch, our innovative client-server architecture means you can write your stimulus presentation code in carefully controlled other environments such as <a href="http://psychtoolbox.org/">PsychToolbox</a> or <a href="https://www.psychopy.org/">PsychoPy</a>, and even run the modeling and stimulus adaptation code on a completely different computer from your presentation computer.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="why-aepsych-for-statistics--ml-researchers"></a><a href="#why-aepsych-for-statistics--ml-researchers" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Why AEPsych (for statistics / ML researchers)</h2>
<ul>
<li><strong>Minimal overhead for bringing models and methods to human experiments</strong>. Full API compatibility with <a href="https://botorch.org/">BoTorch</a> and <a href="https://gpytorch.ai/">GPyTorch</a> and a full benchmarking suite with novel test functions mean you can evaluate your methods in the ways you are used to while also making it accessible to experimentalists who are not ML experts.</li>
<li><strong>Benchmarking and evaluation built in.</strong>. AEPsych includes <a href="../api/benchmark.html#module-aepsych.benchmark.test_functions">novel test functions</a> for the psychophysics domain you can use to evaluate your models. A <a href="../api/benchmark.html">full benchmarking suite</a> compatible with parallel execution means you can use AEPsych to run your benchmarks, and our text-based experiment configuration registry means the same code you use for benchmarking can be used by experimentalists in human-in-the-loop experiments, without them writing any python code.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="target-audiences-and-design-philosophy"></a><a href="#target-audiences-and-design-philosophy" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Target audience(s) and design philosophy</h2>
<ul>
<li><strong>Bridge domains.</strong> We recognize that pursuing interdisciplinary work requires paying a sometimes-substantial coordination cost between domains in terms of definitions and problem formulations, domain and technical expertise, understanding of common tools, etc. AEPsych is built in part to help bridge this divide between experimentalists in psychophysics and psychology domains on the one hand, and machine learning and statistics researchers on the other hand. This means AEPsych maintains compatibility with standard stimulus display paradigms (e.g. <a href="http://psychtoolbox.org/">PsychToolbox</a>) via its client-server architecture while also maintaining API compatibility with ML tools such as BoTorch.</li>
<li><strong>Good research code.</strong> Research code does not mean bad code. AEPsych moves quickly to make new methods available, but we try to maintain high code readability and good testing coverage, so that you can trust your results.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="aepsych-relative-to-other-methods-and-packages"></a><a href="#aepsych-relative-to-other-methods-and-packages" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>AEPsych relative to other methods and packages</h2>
<ul>
<li><p><strong>Other adaptive psychophysics methods</strong>. In contrast with parametric adaptive psychophysics methods such as <a href="https://github.com/wichmann-lab/psignifit">psignifit</a> and <a href="https://jov.arvojournals.org/article.aspx?articleid=2611972">QUEST+</a>, (which tend to use some flavor of the Generalized Linear Model), AEPsych defaults to flexible, nonparametric <a href="../gp_intro.html">Gaussian Process</a> models, which can approximate arbitrary functions given enough data, and have sensible smoothing behavior in low-data regimes. Relative to recent methods using GPs (notably, <a href="https://github.com/cambridge-mlg/BALaudiogram">BALaudiogram</a>), AEPsych uses black box variational inference rather than expectation propagation for inference, which makes it easier to introduce new models without having to hand-derive inference algorithms. AEPsych does support the models and acquisition functions in that prior work, however.</p></li>
<li><p><strong>Other GP active learning</strong>. In contrast with Bayesian Optimization, which finds the maximum or minimum of a function, AEPsych’s primary focus is on other kinds of active learning problems, notably level set (threshold) estimation and global uncertainty reduction. In addition, while continuous outcome models are supported, AEPsych’s primary focus is on other kinds of observations more common in psychophysics experiments (e.g. yes/no Bernoulli trials). While it is not the package's primary focus, AEPsych can still be used to enable human-in-the-loop Bayesian Optimization with continuous outcomes via full compatibility with our upstream packages <a href="https://botorch.org/">BoTorch</a> and <a href="https://gpytorch.ai/">GPyTorch</a>.</p></li>
</ul>
</span></div></article></div><div class="docs-prevnext"><a class="docs-next button" href="/docs/papers"><span>Papers related to AEPsych</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#why-aepsych-for-psychophysics-researchers">Why AEPsych (for psychophysics researchers)?</a></li><li><a href="#why-aepsych-for-statistics--ml-researchers">Why AEPsych (for statistics / ML researchers)</a></li><li><a href="#target-audiences-and-design-philosophy">Target audience(s) and design philosophy</a></li><li><a href="#aepsych-relative-to-other-methods-and-packages">AEPsych relative to other methods and packages</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/favicon.ico" alt="AEPsych" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/introduction">Introduction</a><a href="/docs/getting_started">Getting Started</a><a href="/tutorials/">Tutorials</a><a href="/api/">API Reference</a></div><div><h5>Legal</h5><a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></div><div><div class="footerSection"><h5>Social</h5><div class="social"><a class="github-button" href="https://github.com/facebookresearch/aepsych" data-count-href="https://github.com/facebookresearch/aepsych/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star AEPsych on GitHub">aepsych</a></div></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2024 Meta, Inc.  Built with Docusaurus.</section><div class="cookie-container"><p>We use cookies to enhance your experience, and to analyse the use of our website. By clicking or navigating, you agree to allow our usage of cookies.</p><button class="cookie-btn">accept</button></div></footer></div></body></html>