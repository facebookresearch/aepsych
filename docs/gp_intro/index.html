<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>A brief introduction to Gaussian Process active learning · AEPsych</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="# The core AEPsych ingredients: models and acquisition functions"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="A brief introduction to Gaussian Process active learning · AEPsych"/><meta property="og:type" content="website"/><meta property="og:url" content="https://aepsych.org/"/><meta property="og:description" content="# The core AEPsych ingredients: models and acquisition functions"/><meta property="og:image" content="https://aepsych.org/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://aepsych.org/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/code_block_buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/cookie_consent.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/cookie_consent.js"></script><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="https://unpkg.com/vanilla-back-to-top@7.1.14/dist/vanilla-back-to-top.min.js"></script><script>
        document.addEventListener('DOMContentLoaded', function() {
          addBackToTop(
            {"zIndex":100}
          )
        });
        </script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/static-logo.png" alt="AEPsych"/><h2 class="headerTitleWithLogo">AEPsych</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/docs/introduction" target="_self">Docs</a></li><li class=""><a href="/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/api/" target="_self">API Reference</a></li><li class=""><a href="https://github.com/facebookresearch/aepsych" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Background materials</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">About</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/introduction">Introduction</a></li><li class="navListItem"><a class="navItem" href="/docs/papers">Papers related to AEPsych</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">General</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/getting_started">Getting Started with the AEPsych service</a></li><li class="navListItem"><a class="navItem" href="/docs/configs">Writing Config Files</a></li><li class="navListItem"><a class="navItem" href="/docs/clients">AEPsych clients</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Background materials</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/psychophysics_intro">A brief introduction to Psychophysics</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/gp_intro">A brief introduction to Gaussian Process active learning</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">For developers</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/api_overview">API Overview</a></li><li class="navListItem"><a class="navItem" href="/docs/db_overview">Database Overview</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Advanced topics</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/finish_criteria">Advanced Strategy Configuration</a></li><li class="navListItem"><a class="navItem" href="/docs/ax_backend">The Ax Backend</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/facebookresearch/aepsych/blob/main/docs/gp_intro.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 id="__docusaurus" class="postHeaderTitle">A brief introduction to Gaussian Process active learning</h1></header><article><div><span><h1><a class="anchor" aria-hidden="true" id="the-core-aepsych-ingredients-models-and-acquisition-functions"></a><a href="#the-core-aepsych-ingredients-models-and-acquisition-functions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The core AEPsych ingredients: models and acquisition functions</h1>
<p>In AEPsych, we decompose the active learning problem into two questions: [1] what is the model we are using for the data? and [2] how are we using this model to select the next point to measure? The example configs make reasonable choices for both items, but it is important to remember that AEPsych is not a single monolithic method, but rather a platform for constructing model-based active learning algorithms.</p>
<p>The perspective on models in AEPsych is strongly influenced by <a href="https://towardsdatascience.com/generalized-linear-models-9cbf848bb8ab">generalized linear models (GLMs)</a>. In a GLM, there is a latent linear function that is transformed through a link function, and a likelihood distribution that describes how data is generated given the latent function taken through the link. The function describing the latent psychometric or preference function is what <a href="https://psignifit.sourceforge.net/PSYCHOMETRICFUNCTIONS.html">psignifit</a> calls a <em>core</em>, and the link is what psignifit calls the <em>sigmoid</em>. In AEPsych, we usually use a flexible nonparametric Gaussian Process (GP) model for the latent function, but other options exist and are described below. For the psychophysics domain, the likelihood in AEPsych is typically Bernoulli, with a Probit (Gaussian CDF) link, but a few other options exist (also described below). For more on generalized GPs, see <a href="http://visal.cs.cityu.edu.hk/static/pubs/conf/cvpr11-ggpm.pdf">Chan &amp; Dong 2011</a> and <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.2014.889021">Wang &amp; Shi 2014</a>.</p>
<h1><a class="anchor" aria-hidden="true" id="models-in-aepsych"></a><a href="#models-in-aepsych" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models in AEPsych</h1>
<p>For a model with a GP prior, identity link, and Gaussian observation likelihood, the posterior is available in closed form. For all other models, AEPsych relies on <a href="https://arxiv.org/abs/1601.00670">variational inference</a>, which solves an optimization problem to construct approximate posteriors quickly enough for human-in-the-loop usage (for the specific approaches AEPsych benefits from, see <a href="https://proceedings.mlr.press/v38/hensman15.pdf">Hensman et al. 2015</a> and the <a href="https://botorch.org/api/fit.html#botorch.fit.fit_gpytorch_mll">BoTorch docs</a>). Every model class under in []<code>AEPsych.models</code>](<a href="https://aepsych.org/api/models.html">https://aepsych.org/api/models.html</a>) is a subclass of either the variational or non-variational GP class, with some specification of likelihood and link (which we call <code>Objective</code> in the code to match the BoTorch terminology). For example, <code>GPBetaRegressionModel</code> is identical to instantiating <code>GPClassificationModel</code> with <code>likelihood=BetaLikelihood()</code>.</p>
<p>This section discusses the models AEPsych has for the (latent) psychometric or preference function. Below, we discuss the links and likelihoods. Models in AEPsych operate in various points of the bias-variance (or equivalently, prior-data) tradeoff. More flexible models are useful when you know fewer things about your data generating process, but their larger hypothesis space means they require more data. On the other hand, less flexible models build more structure in, reduce the size of the hypothesis space, and thereby require less data. In approximate order from most to least flexible, the AEPsych models are:</p>
<ul>
<li>A GP model with the Radial Basis Function (RBF) kernel over all dimensions.</li>
<li>A GP model with the RBF kernel and monotonicity imposed on intensity dimensions via posterior projection or derivative constraints.</li>
<li>A &quot;semi-parametric&quot; model that has a classical parametric form for the intensity dimension, and GP priors on the model parameters.</li>
</ul>
<p>An earlier and more restrictive approach to constraining psychometic field intensity dimensions comes from <a href="https://link.springer.com/article/10.3758/s13414-017-1460-0">Song et al. 2017</a>, who decompose the GP kernel additively into an RBF kernel over context dimensions, and a linear kernel over the context dimension. That model assumed that the psychometric function has the same linear appearance over the whole space, and is shifted nonlinearly based on context. At least when using variational inference, <a href="https://arxiv.org/abs/2104.09549">Owen et al. 2021</a> and <a href="https://arxiv.org/abs/2302.01187">Keeley et al. 2023</a> showed that it under-performs the semi-parametric models in the small-data regime, and the full RBF models in the larger-data regime, so while AEPsych implements it, we do not recommend it for usage currently.</p>
<p>In addition, AEPsych provides a model for pairwise data (discussed below), and a way of constructing other custom kernels via kernel factory functions (discussed in the <a href="https://aepsych.org/api/factory.html">API docs</a>).</p>
<h2><a class="anchor" aria-hidden="true" id="conventional-rbf-gp-model"></a><a href="#conventional-rbf-gp-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conventional RBF-GP model</h2>
<p>The default latent model in AEPsych is the Gaussian Process (GP) model, a nonparametric function approximator that assumes the latent percept is smoothly varying with any number of stimulus configuration dimensions. More formally, the generative model is:</p>
<p>$$
\mathbf{f}\mid x \sim \mathcal{N}\left(m, \Sigma\right),
$$</p>
<p>where $\Sigma$  is a $N \times N$  covariance (Gram) matrix whose $(n,n')$ th entry is given by a kernel function $\kappa(\mathbf{x}_n,\mathbf{x}_n')$ evaluated over the $n$th and $n'$th stimuli, $N$  is the total number of stimuli sampled, $m$  is a constant (0 by default). The most common choices for the kernel function are the (RBF) and Matérn, and AEPsych defaults to the former (in informal testing we have not noticed a difference between those two choices). The RBF kernel is defined as:</p>
<p>$$
\kappa(\mathbf{x}_n,\mathbf{x}_n') = v \exp \left(-\frac{1}{2}(\mathbf{x}_n,\mathbf{x}_n')^{T}\Theta^{-2}(\mathbf{x}_n,\mathbf{x}_n')\right)
$$</p>
<p>where $v$ is a positive <em>outputscale</em> parameter and $\Theta$ is a diagonal matrix of per-dimensions <em>lenghtscales</em>. These <em>hyperparameters</em> are estimated from data using <a href="https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html#4_adding_an_informative_prior_for_the_length_scale">informative priors </a> that disprefer overly flat psychometric functions. The outputscale will scale the overall function, while the lengthscales measure the distance at which points are expected to be correlated (smaller lengthscales imply more &quot;wiggly&quot; functions). Estimating per-dimension lengthscales is known as Automatic Relevance Determination (ARD) and is a way of letting the model automatically determine which dimensions are more influential on the predicted outcome (in the sense that the outcome varies more rapidly with these parameters).</p>
<p>Importantly, the “Gaussian” part of GPs is not a standard psychophysical noise assumption, but rather a technical assumption about the joint distribution of latent function observations. AEPsych supports a variety of psychophysical noise models, as discussed in the section on link functions below. GPs are nonparametric in the sense that they do not use a parametric form to describe the psychometric field. Instead, the field is described implicitly by its values at any set of points and their covariance. This means that we take a separate interpolation step to extract estimates such as thresholds or JNDs.</p>
<p><strong>Usage Advice</strong>: A GP model with the RBF kernel is <em>universal</em> in the sense that in the limit of data, it can recover any functions, so it is a good choice if you have no a priori assumptions about the shape of the psychometric field and you would have otherwise used the method of constant stimuli or grid search. Because of the inherent smoothness assumption of the RBF kernel, this model performs most poorly if there are discontinuities or other very sharp transitions in the psychometic field. Because the RBF kernel is <em>stationary</em>, it assumes the covariance between any two points is only a function of their distance in parameter space, so if the characteristic smoothness of the psychometric field changes rapidly this model will likewise struggle. If you can make stronger assumptions about your data generating process that matches one of the more restricted models in AEPsych, you should use that model, as it will require less data. All likelihoods and links in AEPsych are compatible with this prior (for links to specific model code, see below in the discussion of likelihoods).</p>
<h2><a class="anchor" aria-hidden="true" id="rbf-gp-model-with-monotonicity-constraints"></a><a href="#rbf-gp-model-with-monotonicity-constraints" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RBF-GP model with monotonicity constraints</h2>
<p>A slightly stronger assumption that one can make is that one or more dimensions of your problem are monotonically increasing. This is common in psychophysics problems where there is an intensity dimension such as contrast, but less common in preference learning. In practice, we found that monotonicity is very easy to learn for the vanilla RBF model (above) over most of the space, but that without any constraints such models will fail to have detection probabilities saturate to exactly 0 or 1 at the edges of the space. There are a two ways of imposing monotonicity on GPs in the literature: one is to somehow constrain the derivatives of the function (via priors or searching over a limited domain). The second is to project a conventional GP posterior (as above) to the nearest monotonic one. AEPsych implements both methods, as <code>MonotonicRejectionGP</code> and <code>MonotonicProjectionGP</code>.</p>
<p><strong>Usage Advice</strong>: While derivative-constrained GPs are implemented in AEPsych, we found them to be too slow for human-in-the-loop usage while not providing substantial performance benefit. If monotonicity is needed in your setting, the monotonic projection GP is essentially a post-fitting step that will give you a (near-)monotonic psychometric function for plotting and prediction. Informally we suspect that this model is universal over monotonic functions, so should be fairly safe to use for standard psychometric problems when minimal prior knowledge is available.</p>
<h2><a class="anchor" aria-hidden="true" id="semi-parametric-model-with-gp-priors"></a><a href="#semi-parametric-model-with-gp-priors" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Semi-parametric model with GP priors</h2>
<p>In many psychophysics settings, we have reason to assume that conventional intensity dimensions (contrast, brightness, etc) follow a classical psychometric function with a slope and intercept taken through a sigmoid. <a href="https://arxiv.org/abs/2302.01187">Keeley et al. 2023</a> suggested that this means we can define GP priors directly on the parameters of such a psychometric function, where those GPs are a function of the remaining non-intensity dimensions, and showed that such a model can be substantially more performant than the conventional RBF-GP in the small-data regime. This model admits a few additioanl advantages, notably the ability to directly extract slopes (JNDs) and intercepts (thresholds) from the model without an interpolation step and ability to sample actively based on only the threshold posterior (for threshold estimation problems). AEPsych implements both the full semi-parametric model as <code>SemiParametricGPModel</code>, and an approximate model that derives a so-called &quot;psychophysics kernel&quot; based on the semi-parametric model while retaining the multivariate Gaussian form of the GP (as <code>HadamardSemiPModel</code>).</p>
<p><strong>Usage Advice</strong>: The SemiP models pay for their efficiency by having a very strong constraint on the psychometric function in the intensity dimension. T use the SemiP models, you should be very sure that your intensity dimension follows a classical psychometric function, and that your intensity dimension is in the correct units.</p>
<h2><a class="anchor" aria-hidden="true" id="pairwise-models"></a><a href="#pairwise-models" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Pairwise models</h2>
<p>Often in both psychophysics and preference learning participants make binary responses based on pairs of stimuli (e.g. &quot;is X preferred to Y&quot;, or &quot;is X louder than Y&quot;). For this setting, AEPsych provides a pairwise classification model which is a thin wrapper around the <a href="https://botorch.org/tutorials/preference_bo">pairwise preference GP</a> in BoTorch, implemented as <a href="https://aepsych.org/api/models.html#aepsych.models.PairwiseProbitModel"><code>PairwiseProbitModel</code></a>. There are no non-classification and non-probit-link pairwise models in AEPsych currently.</p>
<p><strong>Usage Advice</strong>. This model is currently the only option for paired data in AEPsych. Paired data generally provides more information about the shape of the psychometric function than single-stimulus data, and as such yields faster tuning/optimization runs and faster ability to characterize the full function at super-threshold levels. Intuitively, this is since each observation is informative about two rather than one location in stimulus space. However, paired data can only constrain the latent function up to a constant shift, so at least some single-stimulus observations are needed to reference the function and constrain that shift.</p>
<h1><a class="anchor" aria-hidden="true" id="link-functions-and-likelihoods-in-aepsych"></a><a href="#link-functions-and-likelihoods-in-aepsych" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Link functions and likelihoods in AEPsych</h1>
<p>To get a full model, we need to combine one of the models above for the latent psychometric field with a likelihood through a link function. AEPsych supports a few different likelihoods:</p>
<ul>
<li><strong>Binary</strong>: for yes/no or other kinds of [0,1] responses, AEPsych provides a Bernoulli likelihood with Probit, Logit, or Gumbel links.</li>
<li><strong>Percentage</strong>: for outcomes bounded between 0 and 1, AEPsych provides a Beta likelihood with a Logit link with a learned scale.</li>
<li><strong>Ordinal</strong>: for ordered discrete outcomes (e.g. likert scales)), AEPsych provides a Categorical Likelihood with a Probit, Logit or Gumbel link and learned cutpoints.</li>
<li><strong>Continuous</strong>: for unconstrained continuous outcomes, AEPsych provides a Gaussian likelihood with the identity link.</li>
</ul>
<p>These are described in greater detail below:</p>
<h2><a class="anchor" aria-hidden="true" id="models-for-binary-data"></a><a href="#models-for-binary-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models for binary data</h2>
<p>Given the latent itnensity function $\mathbf{f}(x)$, the likelihood for a binary observation in AEPsych is $y \mid x \sim Bernoulli(\sigma(\mathbf{f}(x))$, where $\sigma$ is a Probit, Logit, or Gumbel sigmoid link function. In the psychophysics setting, these links correspond to the CDF of the sensory noise distribution (Gaussian, Logistic, or Log-Weibull).</p>
<p><strong>Usage Advice</strong> For psychophysics applications, we recommend the Probit as a default, as it is most anchored in classical psychophysics theory. For other applications, the Logistic is more standard and can be faster numerically. However, ultimately this is a choice that should be set by cross-validation or a priori theoretical determination. This is implemented as <a href="https://aepsych.org/api/models.html#aepsych.models.gp_classification.GPClassificationModel"><code>GPClassificationModel</code></a> for the RBF kernel, <a href="https://aepsych.org/api/models.html#aepsych.models.monotonic_rejection_gp.MonotonicRejectionGP"><code>MonotonicRejectionGP</code></a> for the GP-RBF model with monotonicity constraints, <a href="https://aepsych.org/api/models.html#aepsych.models.SemiParametricGPModel"><code>SemiParametricGPModel</code></a> for the semi-parametric model, or <a href="https://aepsych.org/api/models.html#aepsych.models.HadamardSemiPModel"><code>HadamardSemiPModel</code></a> for the approximate-MVN semi-parametric model.</p>
<h2><a class="anchor" aria-hidden="true" id="models-for-percentage-data"></a><a href="#models-for-percentage-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models for percentage data</h2>
<p>For percentage outcomes or other outcome values between 0 and 1, the likelihood is $y \mid x \sim Beta(\sigma(f)s, (1-\sigma(f)s))$. This is a scale-mixture parameterization of the Beta distribution, where the mixture is given by the GP through a logistic sigmoid link, and the scale is a learned hyperparameters. No other links are supported (but we accept pull requests!).</p>
<p><strong>Usage Advice</strong> This is uncommon in psychophysics applications, but can be useful in any sort of setting with continuous bounded outcomes, especially if there are observations close to the bounds (otherwise the regular continuous outcome model may be sufficient and run faster). An example application is a preference learning setting when users give scores on a 0-100 scale. This model is implemented as <a href="https://aepsych.org/api/models.html#aepsych.models.gp_classification.GPBetaRegressionModel"><code>GPBetaRegressionModel</code></a> for the RBF prior. For other priors, simply pass the <code>likelihood=BetaLikelihood()</code> configuration option to one of the models for binary data discussed above.</p>
<h2><a class="anchor" aria-hidden="true" id="models-for-ordinal-data"></a><a href="#models-for-ordinal-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models for ordinal data</h2>
<p>For ordered discrete (ordinal / likert) data, we follow the parameterization given by <a href="https://www.jmlr.org/papers/volume6/chu05a/chu05a.pdf">Chu &amp; Ghahramani 2005</a>, which combines a sigmoid link and learned cutpoints. We review it here from a psychophysics framing, which may be useful for perceptual researchers. In standard psychophysics theory, we have two stimuli, $x_1$ and $x_2$, and we assume the latent representations of both are corrupted by noise to yield latent percepts:
$$
\begin{aligned}
\widetilde{f}(x_1) &amp;= f(x_1) + \epsilon_1, \epsilon_1 \sim \mathcal{N}(0, \sigma)\
\widetilde{f}(x_2) &amp;= f(x_2) + \epsilon_2, \epsilon_2 \sim \mathcal{N}(0, \sigma),
\end{aligned}
$$
where under Weber's law we expect $f(x)$ to be the log function but in AEPsych we assume it is more general. For symmetric noise distributions, this is equivalent to saying $f(x_n) = \widetilde{f}(x_n) + \epsilon_n$, which is equivalent to the following model:</p>
<p>$$
\begin{aligned}
f(x_1) &amp;\sim \mathcal{N}(\widetilde{f}(x_1), \sigma^2)\
f(x_2) &amp;\sim \mathcal{N}(\widetilde{f}(x_2), \sigma^2).
\end{aligned}
$$</p>
<p>Then when we ask the participant to respond based on which stimulus is stronger, we ask them to respond yes if $f(x_1)&gt;f(x_2)$ and respond no otherwise. By definition,</p>
<p>$$
\begin{aligned}
p(f(x_1) &gt; f(x_2)) &amp;=p(f(x_1) - f(x_2) &gt; 0)\
&amp;= \Phi(f(x_1)-f(x_2)).
\end{aligned}
$$</p>
<p>In detection experiments, we treat $f(x_2)$ as a constant, and for an $f(\cdot)$ that can model shifts it can just be zero, yielding $z(x\mid f) = \Phi(f(x))$. We often care about the marginal predictive probability $z(x) = \mathbb{E}_f(z(x\mid f)$, which is given by the following expression:
$$
z(x) = \Phi\left(\frac{\mu(x)}{\sqrt{1+\sigma(x)^2}}\right),
$$
where the mean and variance are the mean and variance of $f(x)$ (i.e. for the prior $\mu(x)=\widetilde{f}(x)$).</p>
<p>Now we extend this for multiple stimuli. Suppose that:
$$
\begin{aligned}
\widetilde{f}(x_1) &amp;= f(x_1) + \epsilon_1, \epsilon_1 \sim \mathcal{N}(0, \sigma^2)\
\ldots\
\widetilde{f}(x_n) &amp;= f(x_n) + \epsilon_n, \epsilon_n \sim \mathcal{N}(0, \sigma^2),
\end{aligned}
$$
for $n$ stimuli, and equivalently $f(x) \sim \mathcal{N}(\widetilde{f}(x), \sigma^2)$ (i.e.\ it's the same exact model). Then we ask the participant to make one of $n$ responses corresponding to the strength of the stimulus. We assume that the participant has a set of internal criteria ${d_1, \cdots, d_{k+1}}$ subdividing their internal perceptual intensity space into $k$ regions. Assuming each region in the space must be assigned to some region, we set $d_1=-\infty$ and $d_{k+1} = \infty$. Then the probability that a participant picks a rating is the probability that the noisy internal percept falls into the appropriate bucket, i.e. $p(y=k\mid x) = p(d_k &lt; f(x) \le d_{k+1})$. This is simply the proportion of $f(x)$ that falls between $d_k$ and $d_{k+1}$, i.e.\ it is the difference between the CDF of $f(x)$ evaluated at $d_{k+1}$ and $d_k$:
$$
z_k(x\mid f) := p(d_k &lt; f(x) \le d_{k+1}) = \Phi(d_{k+1}-f(x)) - \Phi(d_{k}-f(x)).
$$
Supposing that we want the marginal response probability as before, due to the linearity of expectation it is:
$$
z_k(x) = \Phi\left(\frac{d_{k+1}-\mu(x)}{\sqrt{1+\sigma(x)^2}}\right) - \Phi\left(\frac{d_{k}-\mu(x)}{\sqrt{1+\sigma(x)^2}}\right),
$$</p>
<p><strong>Usage Advice</strong>: Likert-scale data is common in user experience and other preference-oriented studies, but we believe based on the framing above that it has some promise in psychophysics as well. For example, Ordinal-response models could be used to model subjective intensity judgments such as how bright or loud a stimulus is on a likert scale. Note that the ordinal model is unconstrained w.r.t. a shift (similarly to the Pairwise model) and also an arbitrary multiplicative scaling of the function. This model with the RBF kernel is exposed as <a href="https://aepsych.org/api/models.html#aepsych.models.OrdinalGPModel"><code>OrdinalGPModel</code></a>. For other likelihoods and links, you would need to subclass from one of the classification models above, and pass in a kernel without an outputscale to account for the scale-invariance. Currently such models can be constructed in python code but not using the AEPsych configuration system -- we accept pull requests!</p>
<h2><a class="anchor" aria-hidden="true" id="models-for-continuous-data"></a><a href="#models-for-continuous-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Models for continuous data</h2>
<p>For continuous data, we use a Gaussian likelihood with the identity link, i.e. $y \mid x \sim \mathcal{N}(\mathbf{f}(x)+ \sigma^2)$ with a learned variance $\sigma^2$. This is analogous to the standard GP regression model typically used in Bayesian Optimization.</p>
<p><strong>Usage Advice</strong>: this model can be suitable for continuous psychophysics and preference ratings, and can be a convenient way to integrate continuous tuning into a project already using the AEPsych client-server interface. It is implemented as <a href="https://aepsych.org/api/models.html#aepsych.models.GPRegressionModel"><code>GPRegressionModel</code></a> for the RBF-GP setting. It is not implemented or tested with the monotonic or semi-parametric priors, though it should be straightforward to do so if it that is needed for your application and we are happy to review implementation plans and PRs on this front. If your continuous ratings are on a likert scale, we recommend the Ordinal model instead. If your continuous ratings are over a limited range (e.g. 0-100) and you expect a nontrivial amount of them to be close to the boundaries, you should use the Beta regression model instead. If you purely need to apply Bayesian Optimization or continuous GP models without the other AEPsych psychophysics machinery or clients, you can also be well-served using the <a href="https://ax.dev/">Ax</a> adaptive experimentation package (which AEPsych extends) directly.</p>
<h2><a class="anchor" aria-hidden="true" id="model-based-active-learning-in-aepsych"></a><a href="#model-based-active-learning-in-aepsych" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model-based active learning in AEPsych</h2>
<p>AEPsych is a platform for model-based experiment design, i.e. the use of a model (as discussed above) to determine the point to sample next. Currently, AEPsych defaults to <em>query-based</em> active learning, which means that there is no fixed set of stimuli over which we are searching (as would be the case in <em>pool-based</em> active learning more common in classical adaptive psychophysics). Rather, we can query any stimulus in the space. To determine where to sample next, we define an <em>acquisition function</em> which, given a model, assigns a value to each point in the parameter space that tells us how useful sampling at that point would be for achieving our goals. By optimizing the acquisition function w.r.t. the stimulus configuration, we find the best stimulus to sample next. We then sample this stimulus, update the model, re-optimize the acquisition function, rinse and repeat.</p>
<p>By way of example, if our goal is to estimate the psychometric function at every point in the space, a simple acquisition function could be given by the current uncertainty over the function at each point. By always sampling at the points where the uncertainty is highest, we may be able to get an accurate model in fewer trials than by just sampling the space randomly. In practice, however, creating new acquisition functions is an active area of research because model uncertainty often interacts with data noise in nontrivial ways (especially in psychophysics) -- for example, high-uncertainty points for psychonmetric function estimation are often close to where the response probability is 0.5, which is where the noisiest responses are, so sampling in other locations may be more effective. Furthermore, focusing on local uncertainty for choosing points may be less effective than considering how observing one location might affect uncertainty over the entire psychometric field. As a result, AEPsych contains a growing number of state of the art acquisition functions.</p>
<p>From a technical perspective, there are two categories of acquisition functions in AEPsych:</p>
<ol>
<li>Monte Carlo (MC) acquisition functions. These acquisition functions can be evaluated by sampling from the model posterior. In the simplest case we sample from the latent perceptual field posterior, but we can also sample from other posteriors such as the response probability posterior, or response threshold posterior. The most common acquisition functions of this kind is BALD, or information maximization w.r.t. the response probability posterior (aka <a href="https://aepsych.org/api/acquisition.html#aepsych.acquisition.mutual_information.BernoulliMCMutualInformation"><code>BernoulliMCMutualInformation</code></a> in AEPsych), or BALV, or variance maximization (aka <a href="https://aepsych.org/api/acquisition.html#aepsych.acquisition.mc_posterior_variance.MCPosteriorVariance"><code>MCPosteriorVariance</code></a> in AEPsych). <a href="https://arxiv.org/abs/2302.01187">Keeley et al. 2023</a> additionally introduced <code>ThresholdBALV</code>, which is BALV defined over the threshold rather than probability posterior for the semi-parametric GP models in AEPsych. To use <code>ThresholdBALV</code>, use <code>MCPosteriorVariance</code> with the <code>objective=[SemiPThresholdObjective](https://aepsych.org/api/acquisition.html#aepsych.acquisition.objective.SemiPThresholdObjective)</code> option. Because they only require sampling from the posterior, they can be used with any model which admits sampling from the posterior.</li>
<li>Analytic acquisition functions. These acquisition functions require an explicit multivariate normal (MVN) posterior to be evaluated. They can often be faster to evaluate, and enable efficient computation of  <em>lookahead</em> approaches, which compute the acquisition in expectation of the posterior as it will be given the next observation. As with MC acquisition functions, they can be evaluated w.r.t. the current or lookahead function posterior (as in <a href="https://proceedings.neurips.cc/paper_files/paper/2021/hash/50d2e70cdf7dd05be85e1b8df3f8ced4-Abstract.html">Zhao et al. 2021</a>, the lookahead level set (threshold) posterior, as described in <a href="https://arxiv.org/abs/2203.09751">Letham et al. 2022</a>, or any other posterior that can be approximated as MVN (e.g. the full semi-parametric posterior, with the MVN approximation taken after posterior inference.</li>
</ol>
<p>For more on analytic and MC acquisition functions, you can check the <a href="https://botorch.org/docs/acquisition">BoTorch docs</a>, and AEPsych supports all BoTorch acquisition functions, as well as a growing number of acquisition functions developed specifically for human psychophysics. From an application perspective, acquisition functions in AEPsych could be sorted based on their goals:</p>
<ul>
<li><strong>Optimization.</strong> While pure optimization is not a common psychophysics task, it is very common in preference learning. If you would like to use AEPsych for optimizing something, we recommend <code>qNoisyExpectedImpprovement</code> for acquisition for all but the continuous-outcome Gaussian-noise model, where we recommend <code>qKnowledgeGradient</code>. The former samples the point most likely to be better than the current best sample observed, where the current best is also estimated from the current (noisy) model. The latter, a lookahead approach, exploits the MVN assumption to sample the point most likely to be better than the current best known function value (even if it had not been observed). Both are directly imported into AEPsych from <a href="https://botorch.org/">BoTorch</a>.</li>
<li><strong>Level-set estimation</strong>, or threshold estimation, is only supported in AEPsych for Bernoulli observation models. If you are interested in threshold (level set) estimation , we recommend using the <a href="https://aepsych.org/api/acquisition.html#aepsych.acquisition.GlobalMI"><code>GlobalMI</code></a> or <a href="https://aepsych.org/api/acquisition.html#aepsych.acquisition.EAVC"><code>EAVC</code></a> acquisition function. Both were shown to be similarly competitive by <a href="https://arxiv.org/abs/2203.09751">Letham et al., 2022</a> but in practice we found sometimes one works better than the other in specific applications. <code>GlobalMI</code> attempts to sample at the point that is most informative about the threshold location over the full range of non-intensity variables when accounting for the possible human responses to the query. <code>EAVC</code> attempts to maximize the expected absolute volume change between the sublevel-set volume (i.e. volume below threshold) before and after taking the next observations, in expectation over the value of the next observation. Both <code>EAVC</code> and <code>GlobalMI</code> are lookahead acquisition functions, and theoretically require a probit link, though in practice they seem to work with other links as well (using one of them with a non-probit link implicitly uses a probit link for computing the acquisition function).</li>
<li><strong>Global uncertainty reduction</strong>. If you are interested in JNDs or global function estimation, the best approach is likely <code>GlobalMI</code> or <code>EAVC</code> with <code>acqf_kwargs={lookahead_type:'posterior'}</code> in instantiating your generator, or <code>lookahead_type = posterior</code> under your generator section in the AEPsych configuration. This option is not not yet extensively tested -- a safe but less efficient option is <code>BernoulliMCMutualInformation</code> (also known as BALD in the literature) for smaller experiments (1-2d). BALD is similar to standard mutual-information-based strategies from adaptive psychophysics, and we find that it empirically performs poorly above 2-3d, so we recommend falling back to quasi-random search in higher dimensions. We would be grateful for reports of successful use of <code>GlobalMI</code> or <code>EAVC</code> for global uncertainty reduction in the wild.</li>
</ul>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/psychophysics_intro"><span class="arrow-prev">← </span><span>A brief introduction to Psychophysics</span></a><a class="docs-next button" href="/docs/api_overview"><span>API Overview</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#conventional-rbf-gp-model">Conventional RBF-GP model</a></li><li><a href="#rbf-gp-model-with-monotonicity-constraints">RBF-GP model with monotonicity constraints</a></li><li><a href="#semi-parametric-model-with-gp-priors">Semi-parametric model with GP priors</a></li><li><a href="#pairwise-models">Pairwise models</a></li><li><a href="#models-for-binary-data">Models for binary data</a></li><li><a href="#models-for-percentage-data">Models for percentage data</a></li><li><a href="#models-for-ordinal-data">Models for ordinal data</a></li><li><a href="#models-for-continuous-data">Models for continuous data</a></li><li><a href="#model-based-active-learning-in-aepsych">Model-based active learning in AEPsych</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/favicon.ico" alt="AEPsych" width="66" height="58"/></a><div><h5>Docs</h5><a href="/docs/introduction">Introduction</a><a href="/docs/getting_started">Getting Started</a><a href="/tutorials/">Tutorials</a><a href="/api/">API Reference</a></div><div><h5>Legal</h5><a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></div><div><div class="footerSection"><h5>Social</h5><div class="social"><a class="github-button" href="https://github.com/facebookresearch/aepsych" data-count-href="https://github.com/facebookresearch/aepsych/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star AEPsych on GitHub">aepsych</a></div></div></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2023 Meta, Inc.  Built with Docusaurus.</section><div class="cookie-container"><p>We use cookies to enhance your experience, and to analyse the use of our website. By clicking or navigating, you agree to allow our usage of cookies.</p><button class="cookie-btn">accept</button></div></footer></div></body></html>