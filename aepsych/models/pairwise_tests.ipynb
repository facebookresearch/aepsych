{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pairwise_probit import PairwiseGP\n",
    "import torch\n",
    "from variational_gp import BinaryClassificationGP\n",
    "from aepsych.config import Config\n",
    "from gpytorch.likelihoods import BernoulliLikelihood\n",
    "from botorch.models.likelihoods.pairwise import PairwiseProbitLikelihood, PairwiseLikelihood\n",
    "from aepsych.models.base import AEPsychModel\n",
    "from aepsych.models.surrogate import AEPsychSurrogate\n",
    "from aepsych.models.pairwise_probit import PairwiseProbitModel\n",
    "from sklearn.datasets import make_classification\n",
    "from aepsych.utils import promote_0d, _process_bounds\n",
    "from scipy.stats import norm\n",
    "from aepsych.models.utils import select_inducing_points\n",
    "\n",
    "import time, gpytorch, numpy as np\n",
    "from typing import Any, Dict, Optional, Union\n",
    "from aepsych.factory import default_mean_covar_factory\n",
    "from aepsych.config import Config\n",
    "from aepsych.utils_logging import getLogger\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.models import PairwiseGP, PairwiseLaplaceMarginalLogLikelihood\n",
    "from botorch.models.transforms.input import Normalize\n",
    "\n",
    "\n",
    "import dataclasses\n",
    "import time\n",
    "from typing import Dict, List, Optional\n",
    "from aepsych.utils_logging import getLogger\n",
    "from ax.core.search_space import SearchSpaceDigest\n",
    "from ax.core.types import TCandidateMetadata\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.utils.datasets import SupervisedDataset\n",
    "from torch import Tensor\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "class PairwiseGPModel(PairwiseGP, AEPsychModel, AEPsychSurrogate):\n",
    "    name = \"PairwiseProbitModel\"\n",
    "    outcome_type = \"binary\"\n",
    "    stimuli_per_trial = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lb: Union[np.ndarray, torch.Tensor],\n",
    "        ub: Union[np.ndarray, torch.Tensor],\n",
    "        dim: Optional[int] = None,\n",
    "        covar_module: Optional[gpytorch.kernels.Kernel] = None,\n",
    "        max_fit_time: Optional[float] = None,\n",
    "    ):\n",
    "        self.lb, self.ub, dim = _process_bounds(lb, ub, dim)\n",
    "\n",
    "        self.max_fit_time = max_fit_time\n",
    "\n",
    "        bounds = torch.stack((self.lb, self.ub))\n",
    "        input_transform = Normalize(d=dim, bounds=bounds)\n",
    "        if covar_module is None:\n",
    "            config = Config(\n",
    "                config_dict={\n",
    "                    \"default_mean_covar_factory\": {\n",
    "                        \"lb\": str(self.lb.tolist()),\n",
    "                        \"ub\": str(self.ub.tolist()),\n",
    "                    }\n",
    "                }\n",
    "            )  # type: ignore\n",
    "            _, covar_module = default_mean_covar_factory(config)\n",
    "        \n",
    "        self.botorch_model_class = None\n",
    "\n",
    "        super().__init__(\n",
    "            datapoints=None,\n",
    "            comparisons=None,\n",
    "            covar_module=covar_module,\n",
    "            jitter=1e-3,\n",
    "            input_transform=input_transform,\n",
    "        )\n",
    "\n",
    "        self.dim = dim  # The Pairwise constructor sets self.dim = None.\n",
    "            \n",
    "\n",
    "    @classmethod\n",
    "    def get_mll_class(cls):\n",
    "        return PairwiseLaplaceMarginalLogLikelihood\n",
    "    \n",
    "    def predict(\n",
    "        self, x, probability_space=False, num_samples=1000, rereference=\"x_min\"\n",
    "    ):\n",
    "        if rereference is not None:\n",
    "            samps = self.sample(x, num_samples, rereference)\n",
    "            fmean, fvar = samps.mean(0).squeeze(), samps.var(0).squeeze()\n",
    "        else:\n",
    "            post = self.posterior(x)\n",
    "            fmean, fvar = post.mean.squeeze(), post.variance.squeeze()\n",
    "\n",
    "        if probability_space:\n",
    "            return (\n",
    "                promote_0d(norm.cdf(fmean)),\n",
    "                promote_0d(norm.cdf(fvar)),\n",
    "            )\n",
    "        else:\n",
    "            return fmean, fvar\n",
    "    \n",
    "    # def fit(\n",
    "    #     self,\n",
    "    #     train_x: torch.Tensor,\n",
    "    #     train_y: torch.Tensor,\n",
    "    #     optimizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    #     **kwargs,\n",
    "    # ):\n",
    "    #     self.train()\n",
    "    #     mll = PairwiseLaplaceMarginalLogLikelihood(self.likelihood, self)\n",
    "    #     datapoints, comparisons = PairwiseProbitModel._pairs_to_comparisons(train_x, train_y)\n",
    "    #     self.set_train_data(datapoints, comparisons)\n",
    "\n",
    "    #     optimizer_kwargs = {} if optimizer_kwargs is None else optimizer_kwargs.copy()\n",
    "    #     max_fit_time = kwargs.pop(\"max_fit_time\", self.max_fit_time)\n",
    "    #     if max_fit_time is not None:\n",
    "    #         # figure out how long evaluating a single samp\n",
    "    #         starttime = time.time()\n",
    "    #         _ = mll(self(datapoints), comparisons)\n",
    "    #         single_eval_time = time.time() - starttime\n",
    "    #         n_eval = int(max_fit_time / single_eval_time)\n",
    "    #         optimizer_kwargs[\"maxfun\"] = n_eval\n",
    "    #         logger.info(f\"fit maxfun is {n_eval}\")\n",
    "\n",
    "    #     logger.info(\"Starting fit...\")\n",
    "    #     starttime = time.time()\n",
    "    #     fit_gpytorch_mll(mll, **kwargs, **optimizer_kwargs)\n",
    "    #     logger.info(f\"Fit done, time={time.time()-starttime}\")\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        datasets: List[SupervisedDataset],\n",
    "        metric_names: List[str],\n",
    "        search_space_digest: SearchSpaceDigest,\n",
    "        candidate_metadata: Optional[List[List[TCandidateMetadata]]] = None,\n",
    "        state_dict: Optional[Dict[str, Tensor]] = None,\n",
    "        refit: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.train()\n",
    "        self._outcomes = metric_names\n",
    "        if state_dict:\n",
    "            self.model.load_state_dict(state_dict)\n",
    "\n",
    "        if state_dict is None or refit:\n",
    "            mll = self.get_mll_class()(self.likelihood, self)\n",
    "            optimizer_kwargs = {}\n",
    "            if self.max_fit_time is not None:\n",
    "                # figure out how long evaluating a single samp\n",
    "                starttime = time.time()\n",
    "                \n",
    "                if isinstance(self, PairwiseGPModel):\n",
    "                    datapoints, comparisons = PairwiseProbitModel._pairs_to_comparisons(datasets[0].X(), datasets[0].Y().squeeze())\n",
    "                    self.set_train_data(datapoints, comparisons)\n",
    "                    _ = mll(self.model(datapoints), comparisons)\n",
    "                else:\n",
    "                    _ = mll(self.model(datasets[0].X()), datasets[0].Y().squeeze())\n",
    "                single_eval_time = time.time() - starttime\n",
    "                n_eval = int(self.max_fit_time / single_eval_time)\n",
    "                logger.info(f\"fit maxfun is {n_eval}\")\n",
    "                optimizer_kwargs[\"options\"] = {\"maxfun\": n_eval}\n",
    "\n",
    "            logger.info(\"Starting fit...\")\n",
    "            starttime = time.time()\n",
    "            fit_gpytorch_mll(\n",
    "                mll, optimizer_kwargs=optimizer_kwargs\n",
    "            )  # TODO: Support flexible optimizers\n",
    "            logger.info(f\"Fit done, time={time.time()-starttime}\")\n",
    "\n",
    "    @classmethod\n",
    "    def construct_inputs(cls, training_data, **kwargs):\n",
    "        inputs = super().construct_inputs(training_data=training_data, **kwargs)\n",
    "\n",
    "        inducing_size = kwargs.get(\"inducing_size\")\n",
    "        inducing_point_method = kwargs.get(\"inducing_point_method\")\n",
    "        bounds = kwargs.get(\"bounds\")\n",
    "        inducing_points = select_inducing_points(\n",
    "            inducing_size,\n",
    "            inputs[\"covar_module\"],\n",
    "            inputs[\"train_X\"],\n",
    "            bounds,\n",
    "            inducing_point_method,\n",
    "        )\n",
    "\n",
    "        inputs.update(\n",
    "            {\n",
    "                \"inducing_points\": inducing_points,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    @classmethod\n",
    "    def get_config_options(cls, config: Config, name: str = None):\n",
    "        options = super().get_config_options(config, name)\n",
    "        classname = cls.__class__.__name__\n",
    "\n",
    "        inducing_point_method = config.get(\n",
    "            classname, \"inducing_point_method\", fallback=\"auto\"\n",
    "        )\n",
    "        inducing_size = config.getint(classname, \"inducing_size\", fallback=10)\n",
    "        learn_inducing_points = config.getboolean(\n",
    "            classname, \"learn_inducing_points\", fallback=False\n",
    "        )\n",
    "\n",
    "        options.update(\n",
    "            {\n",
    "                \"inducing_size\": inducing_size,\n",
    "                \"inducing_point_method\": inducing_point_method,\n",
    "                \"learn_inducing_points\": learn_inducing_points,\n",
    "                \"likelihood\" : PairwiseProbitLikelihood()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lb': tensor([0., 0.], dtype=torch.float32),\n",
       " 'ub': tensor([1., 1.], dtype=torch.float32),\n",
       " 'max_fit_time': None,\n",
       " 'botorch_model_class': None,\n",
       " 'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict([('datapoints', None),\n",
       "              ('comparisons', None),\n",
       "              ('D', None),\n",
       "              ('DT', None),\n",
       "              ('utility', None),\n",
       "              ('covar_chol', None),\n",
       "              ('likelihood_hess', None),\n",
       "              ('hlcov_eye', None),\n",
       "              ('covar', None),\n",
       "              ('covar_inv', None)]),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict([(77,\n",
       "               <torch.nn.modules.module._WrappedHook at 0x27d8d707910>)]),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('input_transform', Normalize()),\n",
       "              ('likelihood', PairwiseProbitLikelihood()),\n",
       "              ('mean_module', ConstantMean()),\n",
       "              ('covar_module',\n",
       "               ScaleKernel(\n",
       "                 (base_kernel): RBFKernel(\n",
       "                   (lengthscale_prior): GammaPrior()\n",
       "                   (raw_lengthscale_constraint): GreaterThan(1.000E-04)\n",
       "                 )\n",
       "                 (outputscale_prior): SmoothedBoxPrior()\n",
       "                 (raw_outputscale_constraint): Positive()\n",
       "               ))]),\n",
       " '_added_loss_terms': OrderedDict(),\n",
       " '_priors': OrderedDict(),\n",
       " '_constraints': OrderedDict(),\n",
       " '_strict_init': True,\n",
       " '_load_strict_shapes': True,\n",
       " 'train_inputs': [],\n",
       " 'train_targets': None,\n",
       " 'pred_cov_fac_need_update': True,\n",
       " 'dim': 2,\n",
       " '_jitter': 0.001,\n",
       " '_xtol': None,\n",
       " '_maxfev': None,\n",
       " '_x0': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_classification(\n",
    "            n_samples=10,\n",
    "            n_features=1,\n",
    "            n_redundant=0,\n",
    "            n_informative=1,\n",
    "            random_state=1,\n",
    "            n_clusters_per_class=1,\n",
    "        )\n",
    "x, y = torch.Tensor(X), torch.Tensor(y)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "datapoints = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "comparisons = torch.Tensor([[0, 1], [2, 1]])\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "config_file = \"../../configs/ax_pairwise_opt_example.ini\"\n",
    "config = Config(config_fnames=[config_file])\n",
    "lb = config.getlist(\"common\", \"lb\", element_type=float)\n",
    "ub = config.getlist(\"common\", \"ub\", element_type=float)\n",
    "\n",
    "pairwise_model = PairwiseGPModel(lb, ub)\n",
    "\n",
    "dataset = SupervisedDataset(x, y)\n",
    "search_space_digest = SearchSpaceDigest(lb, ub)\n",
    "\n",
    "# pairwise_model.fit([dataset], [\"y\"], search_space_digest)\n",
    "vars(pairwise_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aepsych",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
