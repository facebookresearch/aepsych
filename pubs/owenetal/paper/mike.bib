@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2017 - Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2242--2251},
pmid = {18243891},
title = {{Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}},
volume = {2017-Octob},
year = {2017}
}
@article{Lee2017,
abstract = {We present a model for random simple graphs with a degree distribution that obeys a power law (i.e., is heavy-tailed). To attain this behavior, the edge probabilities in the graph are constructed from Bertoin-Fujita-Roynette-Yor (BFRY) random variables, which have been recently utilized in Bayesian statistics for the construction of power law models in several applications. Our construction readily extends to capture the structure of latent factors, similarly to stochastic blockmodels, while maintaining its power law degree distribution. The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption. By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data. In order to further scale our inference procedure, we adopt stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph.},
archivePrefix = {arXiv},
arxivId = {1702.08239},
author = {Lee, Juho and Heaukulani, Creighton and Ghahramani, Zoubin and James, Lancelot F. and Choi, Seungjin},
eprint = {1702.08239},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2017 - Bayesian inference on random simple graphs with power law degree distributions.pdf:pdf},
isbn = {9781510855144},
title = {{Bayesian inference on random simple graphs with power law degree distributions}},
url = {http://arxiv.org/abs/1702.08239},
year = {2017}
}
@article{Krishnan2016a,
abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
archivePrefix = {arXiv},
arxivId = {1609.09869},
author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
doi = {10.1016/j.micinf.2011.07.011.Innate},
eprint = {1609.09869},
isbn = {0471142735},
title = {{Structured Inference Networks for Nonlinear State Space Models}},
year = {2016}
}
@article{Kazlauskaite2018,
abstract = {We present a model that can automatically learn alignments between high-dimensional data in an unsupervised manner. Our proposed method casts alignment learning in a framework where both alignment and data are modelled simultaneously. Further, we automatically infer groupings of different types of sequences within the same dataset. We derive a probabilistic model built on non-parametric priors that allows for flexible warps while at the same time providing means to specify interpretable constraints. We demonstrate the efficacy of our approach with superior quantitative performance to the state-of-the-art approaches and provide examples to illustrate the versatility of our model in automatic inference of sequence groupings, absent from previous approaches, as well as easy specification of high level priors for different modalities of data.},
archivePrefix = {arXiv},
arxivId = {1803.02603},
author = {Kazlauskaite, Ieva and Ek, Carl Henrik and Campbell, Neill D. F.},
eprint = {1803.02603},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kazlauskaite, Ek, Campbell - 2018 - Gaussian Process Latent Variable Alignment Learning.pdf:pdf},
title = {{Gaussian Process Latent Variable Alignment Learning}},
url = {http://arxiv.org/abs/1803.02603},
volume = {89},
year = {2018}
}
@article{Charles,
abstract = {Theoretical understanding of deep learning is one of the most important tasks facing the statis-tics and machine learning communities. While multilayer, or deep, neural networks (DNNs) orig-inated as models of biological networks in neuroscience [1, 2, 3, 4] and psychology [5, 6], and as engineering methods [7, 8], they have become a centerpiece of the machine learning (ML) toolbox. In ML, DNNs are simultaneously one of the simplest and most complex methods. They consist of many interconnected nodes that are grouped into layers (see Figure 1a), whose operations are stun-ningly simple; the n th node of the network at a given layer i, x i (n) is simply a nonlinear function f ({\textperiodcentered}) (e.g. saturating nonlinearity) applied to an affine function of the previous layer x i (n) = f (w i (n)x i−1 + b i (n)) , where x i−1 ∈ R N i is the network node values at the previous layer, w i (n) ∈ R N i is the linear weight matrix that projects the previous layer to the n th node of the current matrix and b i (n) is the offset for node n. Even with such simple functions connecting the nodes between layers, the sheer number of nodes creates an explosion in the number of parameters (w i (n) and b i (n) for all i and n) and amplifies the effects of the nonlinearities. To add to the complexity, the parameters of the network (i.e. the weights and offsets across layers) are learned with respect to a cost function relating the inputs and outputs by gradient descent methods, i.e. various flavors of back-propagation [9]. Despite the resulting complexity, researchers have utilized DNNs to great effect in many important applications. The relatively recent success of DNNs in ML, despite their long history, can be attributed to a " perfect storm " of large labeled datasets [10, 11]; improved hardware [12]; clever parame-ter constraints [13]; advancements in optimization algorithms [14, 15, 16]; and more open sharing of stable, reliable code [17] leveraging the latest in methods such as automatic differentiation [18]. Original tasks in which DNNs first provided state-of-the-art results centered around image classifica-tion [13, 19], which powers devices such as ATMs. While DNNs have spread well beyond to many other applications (e.g. audio classification [20], probability distribution approximation [21, 22] etc.), the well publicized success in image classification has encouraged continued work that has provided other amazing technologies such as real-time text translation [23]. Unfortunately, DNN adoption powered by these successes combined with the open-source nature of the machine learning community, has outpaced our theoretical understanding. We cannot reliably identify when and why DNNs will make mistakes. In some applications like text translation these mistakes may be comical and provide for fun fodder in research talks, a single error can be very costly in tasks like medical imaging [24]. Additionally, DNNs shown susceptibility to so-called},
archivePrefix = {arXiv},
arxivId = {1806.00148},
author = {Charles, Adam S},
eprint = {1806.00148},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Charles - Unknown - Interpreting Deep Learning The Machine Learning Rorschach Test.pdf:pdf},
pages = {1--13},
title = {{Interpreting Deep Learning: The Machine Learning Rorschach Test?}},
url = {https://arxiv.org/pdf/1806.00148.pdf},
volume = {08544}
}
@article{Pillonetto2010,
abstract = {Standard single-task kernel methods have recently been extended to the case of multitask learning in the context of regularization theory. There are experimental results, especially in biomedicine, showing the benefit of the multitask approach compared to the single-task one. However, a possible drawback is computational complexity. For instance, when regularization networks are used, complexity scales as the cube of the overall number of training data, which may be large when several tasks are involved. The aim of this paper is to derive an efficient computational scheme for an important class of multitask kernels. More precisely, a quadratic loss is assumed and each task consists of the sum of a common term and a task-specific one. Within a Bayesian setting, a recursive online algorithm is obtained, which updates both estimates and confidence intervals as new data become available. The algorithm is tested on two simulated problems and a real data set relative to xenobiotics administration in human patients.},
author = {Pillonetto, Gianluigi and Dinuzzo, Francesco and {De Nicolao}, Giuseppe},
doi = {10.1109/TPAMI.2008.297},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pillonetto, Dinuzzo, De Nicolao - 2010 - Bayesian online multitask learning of gaussian processes.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Collaborative filtering,Gaussian processes,Kalman filtering,Kernel methods,Mixed effects model,Multitask learning,Pharmacokinetic data.,Regularization},
number = {2},
pages = {193--205},
publisher = {IEEE},
title = {{Bayesian online multitask learning of gaussian processes}},
volume = {32},
year = {2010}
}
@article{Wang,
archivePrefix = {arXiv},
arxivId = {arXiv:1905.12681v4},
author = {Wang, Weiyao and Tran, Du and Feiszli, Matt},
eprint = {arXiv:1905.12681v4},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Tran, Feiszli - Unknown - What Makes Training Multi-modal Classification Networks Hard.pdf:pdf},
title = {{What Makes Training Multi-modal Classification Networks Hard?}}
}
@article{Oates2017,
abstract = {A non-parametric extension of control variates is presented. These leverage gradient information on the sampling density to achieve substantial variance reduction. It is not required that the sampling density be normalised. The novel contribution of this work is based on two important insights; (i) a trade-off between random sampling and deterministic approximation and (ii) a new gradient-based function space derived from Stein's identity. Unlike classical control variates, our estimators achieve super-root-{\$}n{\$} convergence, often requiring orders of magnitude fewer simulations to achieve a fixed level of precision. Theoretical and empirical results are presented, the latter focusing on integration problems arising in hierarchical models and models based on non-linear ordinary differential equations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.2392v5},
author = {Oates, Chris J. and Girolami, Mark and Chopin, Nicolas},
doi = {10.1111/rssb.12185},
eprint = {arXiv:1410.2392v5},
isbn = {0090-4295},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
number = {3},
pmid = {19914698},
title = {{Control functionals for Monte Carlo integration}},
volume = {79},
year = {2017}
}
@article{Scheepers2009,
author = {Scheepers, Maatje},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Scheepers - 2009 - W Orking M Emory a Comparison Between Dyslexic and Non - Dyslexic.pdf:pdf},
keywords = {bs,ences,gender differ-,luine,luyi zhou,memory,non-spatial memory,object recognition,phd,sex differences,spatial,victoria n,wei-lun sun},
number = {June},
pages = {1--132},
title = {{W Orking M Emory : a Comparison Between Dyslexic and Non - Dyslexic}},
year = {2009}
}
@article{Basso2009,
abstract = {In this paper we propose a neural architecture for solving contin-uous time and space reinforcement learning problems in non-stationary envi-ronments. The method is based on a mechanism for creating, updating and selecting partial models of the environment. The partial models are incremen-tally estimated using linear approximation functions and are built according to the system's capability of making predictions regarding a given sequence of observations. We propose, formalize and show the efficiency of this method in the non-stationary pendulum task. We show that the neural architecture with context detection performs better than a model-based RL algorithm and that it performs almost as well as the optimum, that is, a hypothetical system with ex-tended sensor capabilities in a way that the environment effectively appears to be stationary. Finally, we present known limitations of the method and future works.},
author = {Basso, Eduardo W. and Engel, Paulo M.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Basso, Engel - 2009 - Reinforcement learning in non-stationary continuous time and space scenarios.pdf:pdf},
journal = {Artificial Intelligence National Meeting (Enia)},
pages = {1--8},
title = {{Reinforcement learning in non-stationary continuous time and space scenarios}},
url = {http://csbc2009.inf.ufrgs.br/anais/pdf/enia/st02{\_}03.pdf},
year = {2009}
}
@article{Sudre2017,
abstract = {Deep-learning has proved in recent years to be a powerful tool for image analysis and is now widely used to segment both 2D and 3D medical images. Deep-learning segmentation frameworks rely not only on the choice of network architecture but also on the choice of loss function. When the segmentation process targets rare observations, a severe class imbalance is likely to occur between candidate labels, thus resulting in sub-optimal performance. In order to mitigate this issue, strategies such as the weighted cross-entropy function, the sensitivity function or the Dice loss function, have been proposed. In this work, we investigate the behavior of these loss functions and their sensitivity to learning rate tuning in the presence of different rates of label imbalance across 2D and 3D segmentation tasks. We also propose to use the class re-balancing properties of the Generalized Dice overlap, a known metric for segmentation assessment, as a robust and accurate deep-learning loss function for unbalanced tasks.},
archivePrefix = {arXiv},
arxivId = {1707.03237},
author = {Sudre, Carole H. and Li, Wenqi and Vercauteren, Tom and Ourselin, Sebastien and {Jorge Cardoso}, M.},
doi = {10.1007/978-3-319-67558-9_28},
eprint = {1707.03237},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sudre et al. - 2017 - Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations.pdf:pdf},
isbn = {9783319675572},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {240--248},
title = {{Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations}},
volume = {10553 LNCS},
year = {2017}
}
@article{Giguere2013,
abstract = {Some decisions, such as predicting the winner of a baseball game, are challenging in part because outcomes are probabilistic. When making such decisions, one view is that humans stochastically and selectively retrieve a small set of relevant memories that provides evidence for competing options. We show that optimal performance at test is impossible when retrieving information in this fashion, no matter how extensive training is, because limited retrieval introduces noise into the decision process that cannot be overcome. One implication is that people should be more accurate in predicting future events when trained on idealized rather than on the actual distributions of items. In other words, we predict the best way to convey information to people is to present it in a distorted, idealized form. Idealization of training distributions is predicted to reduce the harmful noise induced by immutable bottlenecks in people's memory retrieval processes. In contrast, machine learning systems that selectively weight (i.e., retrieve) all training examples at test should not benefit from idealization. These conjectures are strongly supported by several studies and supporting analyses. Unlike machine systems, people's test performance on a target distribution is higher when they are trained on an idealized version of the distribution rather than on the actual target distribution. Optimal machine classifiers modified to selectively and stochastically sample from memory match the pattern of human performance. These results suggest firm limits on human rationality and have broad implications for how to train humans tasked with important classification decisions, such as radiologists, baggage screeners, intelligence analysts, and gamblers.},
author = {Giguere, G. and Love, B. C.},
doi = {10.1073/pnas.1219674110},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Giguere, Love - 2013 - Limits in decision making arise from limits in memory retrieval.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {19},
pages = {7613--7618},
title = {{Limits in decision making arise from limits in memory retrieval}},
volume = {110},
year = {2013}
}
@article{Barachant2012,
author = {Barachant, Alexandre and Congedo, Marco and Jutten, Christian and Brain-, Christian Jutten Multiclass},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Barachant et al. - 2012 - Multiclass Brain-Computer Interface Classification by Riemannian Geometry To cite this version Multi-class Br.pdf:pdf},
number = {4},
pages = {920--928},
title = {{Multiclass Brain-Computer Interface Classification by Riemannian Geometry To cite this version : Multi-class Brain Computer Interface Classification by Riemannian Geometry}},
volume = {59},
year = {2012}
}
@article{Tampuu2015,
abstract = {Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efficiently. Agents trained under collaborative rewarding schemes find an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.},
archivePrefix = {arXiv},
arxivId = {1511.08779},
author = {Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
eprint = {1511.08779},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tampuu et al. - 2015 - Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:pdf},
isbn = {1111111111},
pages = {1--12},
title = {{Multiagent Cooperation and Competition with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1511.08779},
year = {2015}
}
@article{Chizat2018,
abstract = {In a series of recent theoretical works, it has been shown that strongly over-parameterized neural networks trained with gradient-based methods could converge linearly to zero training loss, with their parameters hardly varying. In this note, our goal is to exhibit the simple structure that is behind these results. In a simplified setting, we prove that "lazy training" essentially solves a kernel regression. We also show that this behavior is not so much due to over-parameterization than to a choice of scaling, often implicit, that allows to linearize the model around its initialization. These theoretical results complemented with simple numerical experiments make it seem unlikely that "lazy training" is behind the many successes of neural networks in high dimensional tasks.},
archivePrefix = {arXiv},
arxivId = {1812.07956},
author = {Chizat, Lenaic and Bach, Francis},
eprint = {1812.07956},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chizat, Bach - 2018 - A Note on Lazy Training in Supervised Differentiable Programming.pdf:pdf},
month = {dec},
pages = {1--18},
title = {{A Note on Lazy Training in Supervised Differentiable Programming}},
url = {http://arxiv.org/abs/1812.07956},
year = {2018}
}
@article{Chen2009,
abstract = {Traditional dynamical systems used for motion tracking cannot effectively handle high dimensionality of the motion states and composite dynamics. In this paper, to address both issues simultaneously, we propose the marriage of the switching dynamical system and recent Gaussian Process Dynamic Models (GPDM), yielding a new model called the switching GPDM (SGPDM). The proposed switching variables enable the SGPDM to capture diverse motion dynamics effectively, and also allow to identify the motion class (e.g. walk or run in the human motion tracking, smile or angry in the facial motion tracking), which naturally leads to the idea of simultaneous motion tracking and classification. Moreover, each of GPDMs in SGPDM can faithfully model its corresponding primitive motion, while performing tracking in the low-dimensional latent space, therefore significantly improving the tracking efficiency. The proposed SGPDM is then applied to human body motion tracking and classification, and facial motion tracking and recognition. We demonstrate the performance of our model on several composite body motion videos obtained from the CMU database, including exercises and salsa dance. We also demonstrate the robustness of our model in terms of both facial feature tracking and facial expression/pose recognition performance on real videos under diverse scenarios including pose change, low frame rate and low quality videos. {\textcopyright}2009 IEEE.},
author = {Chen, Jixu and Kim, Minyoung and Wang, Yu and Ji, Qiang},
doi = {10.1109/CVPRW.2009.5206580},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2009 - Switching gaussian process dynamic models for simultaneous composite motion tracking and recognition.pdf:pdf},
isbn = {9781424439935},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
number = {1},
pages = {2655--2662},
publisher = {IEEE},
title = {{Switching gaussian process dynamic models for simultaneous composite motion tracking and recognition}},
volume = {2009 IEEE },
year = {2009}
}
@article{Shi2018,
abstract = {Spatiotemporal systems are common in the real-world. Forecasting the multi-step future of these spatiotemporal systems based on the past observations, or, Spatiotemporal Sequence Forecasting (STSF), is a significant and challenging problem. Although lots of real-world problems can be viewed as STSF and many research works have proposed machine learning based methods for them, no existing work has summarized and compared these methods from a unified perspective. This survey aims to provide a systematic review of machine learning for STSF. In this survey, we define the STSF problem and classify it into three subcategories: Trajectory Forecasting of Moving Point Cloud (TF-MPC), STSF on Regular Grid (STSF-RG) and STSF on Irregular Grid (STSF-IG). We then introduce the two major challenges of STSF: 1) how to learn a model for multi-step forecasting and 2) how to adequately model the spatial and temporal structures. After that, we review the existing works for solving these challenges, including the general learning strategies for multi-step forecasting, the classical machine learning based methods for STSF, and the deep learning based methods for STSF. We also compare these methods and point out some potential research directions.},
archivePrefix = {arXiv},
arxivId = {1808.06865},
author = {Shi, Xingjian and Yeung, Dit-Yan},
eprint = {1808.06865},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shi, Yeung - 2018 - Machine Learning for Spatiotemporal Sequence Forecasting A Survey.pdf:pdf},
number = {1},
pages = {1--20},
title = {{Machine Learning for Spatiotemporal Sequence Forecasting: A Survey}},
url = {http://arxiv.org/abs/1808.06865},
year = {2018}
}
@article{Blankertz2008,
abstract = {Abstract—Due to the volume conduction multi-channel elec-$\backslash$ntroencephalogram (EEG) recordings give a rather blurred image$\backslash$nof brain activity. Therefore spatial filters are extremely useful$\backslash$nin single-trial analysis in order to improve the signal-to-noise$\backslash$nratio. There are powerful methods from machine learning and$\backslash$nsignal processing that permit the optimization of spatio-temporal$\backslash$nfilters for each subject in a data dependent fashion beyond$\backslash$nthe fixed filters based on the sensor geometry, e.g., Laplacians.$\backslash$nHere we elucidate the theoretical background of the Common$\backslash$nSpatial Pattern (CSP) algorithm, a popular method in Brain-$\backslash$nComputer Interface (BCI) research. Apart from reviewing several$\backslash$nvariants of the basic algorithm, we reveal tricks of the trade$\backslash$nfor achieving a powerful CSP performance, briefly elaborate$\backslash$non theoretical aspects of CSP and demonstrate the application$\backslash$nof CSP-type preprocessing in our studies of the Berlin Brain-$\backslash$nComputer Interface project.},
author = {Blankertz, Benjamin and Tomioka, Ryota and Lemm, Steven and Kawanabe, Motoaki and M{\"{u}}ller, Klaus Robert},
doi = {10.1109/MSP.2008.4408441},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Blankertz et al. - 2008 - Optimizing spatial filters for robust EEG single-trial analysis.pdf:pdf},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {1},
pages = {41--56},
pmid = {16189967},
title = {{Optimizing spatial filters for robust EEG single-trial analysis}},
volume = {25},
year = {2008}
}
@article{Li2018a,
abstract = {{\textless}p{\textgreater}While functional magnetic resonance imaging (fMRI) is important for
healthcare/neuroscience applications, it is challenging to classify or
interpret due to its multi-dimensional structure, high dimensionality, and
small number of samples available. Recent sparse multilinear regression methods
based on tensor are emerging as promising solutions for fMRI, yet existing
works rely on unfolding/folding operations and a tensor rank relaxation with
limited tightness. The newly proposed tensor singular value decomposition
(t-SVD) sheds light on new directions. In this work, we study t-SVD for sparse
multilinear regression and propose a Sparse tubal-regularized multilinear
regression (Sturm) method for fMRI. Specifically, the Sturm model performs
multilinear regression with two regularization terms: a tubal tensor nuclear
norm based on t-SVD and a standard L1 norm. We further derive the algorithm
under the alternating direction method of multipliers framework. We perform
experiments on four classification problems, including both resting-state fMRI
for disease diagnosis and task-based fMRI for neural decoding. The results show
the superior performance of Sturm in classifying fMRI using just a small number
of voxels.
{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {1812.01496},
author = {Li, Wenwen and Lou, Jian and Zhou, Shuo and Lu, Haiping},
doi = {arXiv:1812.01496v1},
eprint = {1812.01496},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - Sturm Sparse Tubal-Regularized Multilinear Regression for fMRI.pdf:pdf},
journal = {arXiv Computer Vision and Pattern Recognition},
title = {{Sturm: Sparse Tubal-Regularized Multilinear Regression for fMRI.}},
url = {http://arxiv.org/abs/1812.01496},
year = {2018}
}
@article{Meyerson2018,
abstract = {Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learning to the setting where only a single task is available. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect of training towards closely-related tasks drawn from the same universe. In a suite of experiments, pseudo-task augmentation is shown to improve performance on single-task learning problems. When combined with multitask learning, further improvements are achieved, including state-of-the-art performance on the CelebA dataset, showing that pseudo-task augmentation and multitask learning have complementary value. All in all, pseudo-task augmentation is a broadly applicable and efficient way to boost performance in deep learning systems.},
archivePrefix = {arXiv},
arxivId = {1803.04062},
author = {Meyerson, Elliot and Miikkulainen, Risto},
eprint = {1803.04062},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Meyerson, Miikkulainen - 2018 - Pseudo-task Augmentation From Deep Multitask Learning to Intratask Sharing---and Back.pdf:pdf},
issn = {1938-7228},
number = {1},
title = {{Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing---and Back}},
url = {http://arxiv.org/abs/1803.04062},
year = {2018}
}
@article{WingKam1995,
author = {{Wing Kam}, Liu and Sukky, Jun and {Yi Fei}, Zhang},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wing Kam, Sukky, Yi Fei - 1995 - Reproducing kernel particle methods.pdf:pdf},
journal = {International Journal for Numerical Methods in Fluids},
keywords = {correction function,free particle methods,function,mesh-,multi-resolution analysis,multiple scale decomposition,or grid-,reproducing kernel,wavelet},
number = {8-9},
pages = {1081--1106},
title = {{Reproducing kernel particle methods}},
volume = {20},
year = {1995}
}
@article{Greicius2003,
abstract = {Investigators have recently begun to examine the differential role of subregions of the hippocampus in episodic memory. Two distinct models have gained prominence in the field. One model, outlined by Moser and Moser (Hippocampus 1998;8:608-619), based mainly on animal studies, has proposed that episodic memory is subserved by the posterior two-thirds of the hippocampus alone. A second model, derived by Lepage et al. (Hippocampus 1998;8:313-322) from their review of 52 PET studies, has suggested that the anterior hippocampus is activated by memory encoding while the posterior hippocampus is activated by memory retrieval. Functional magnetic resonance imaging (fMRI) studies have tended to show limited activation in the anteriormost regions of the hippocampus, providing support for the Moser and Moser model. A potential confounding factor in these fMRI studies, however, is that susceptibility artifact may differentially reduce signal in the anterior versus the posterior hippocampus. In the present study, we examined activation differences between hippocampal subregions during encoding and retrieval of words and interpreted our findings within the context of these two models. We also examined the extent to which susceptibility artifact affects the analysis and interpretation of hippocampal activation by demonstrating its differential effect on the anterior versus the posterior hippocampus. Both voxel-by-voxel and region-of-interest analyses were conducted, allowing us to quantify differences between the anterior and posterior aspects of the hippocampus. We detected significant hippocampal activation in both the encoding and retrieval conditions. Our data do not provide evidence for regional anatomic differences in activation between encoding and retrieval. The data do suggest that, even after accounting for susceptibility artifact, both encoding and retrieval of verbal stimuli activate the middle and posterior hippocampus more strongly than the anterior hippocampus. Finally, this study is the first to quantify the effects of susceptibility-induced signal loss on hippocampal activation and suggests that this artifact has significantly biased the interpretation of earlier fMRI studies. {\textcopyright} 2003 Wiley-Liss, Inc.},
author = {Greicius, Michael D. and Krasnow, Ben and Boyett-Anderson, Jesse M. and Eliez, Stephan and Schatzberg, Alan F. and Reiss, Alan L. and Menon, Vinod},
doi = {10.1002/hipo.10064},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Greicius et al. - 2003 - Regional analysis of hippocampal activation during memory encoding and retrieval fMRI study.pdf:pdf},
issn = {10509631},
journal = {Hippocampus},
keywords = {Hippocampus functional imaging,Memory,Susceptibility artifact},
number = {1},
pages = {164--174},
pmid = {12625466},
title = {{Regional analysis of hippocampal activation during memory encoding and retrieval: fMRI study}},
volume = {13},
year = {2003}
}
@article{Gehrke2019,
abstract = {Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user's subjective interpretation of unspecific, yet standardized, questions. Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25{\%} of the trials. We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user's immersion.},
author = {Gehrke, Lukas and Akman, Sezen and Lopes, Pedro and Chen, Albert and Singh, Avinash Kumar and Chen, Hsiang Ting and Lin, Chin Teng and Gramann, Klaus},
doi = {10.1145/3290605.3300657},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gehrke et al. - 2019 - Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain.pdf:pdf},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {EEG,ERP,Elecrical muscle stimulation,Force feedback,Prediction error,Virtual reality},
pages = {1--11},
title = {{Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials}},
year = {2019}
}
@article{Zhang2018a,
abstract = {We explore transferring learning between fMRI datasets. A method is introduced to improve prediction accuracy on a primary fMRI dataset by jointly learning a model using other secondary fMRI datasets. We assume the secondary datasets are directly or indirectly linked to the primary dataset through sets of partially shared subjects. This method is particularly useful when the primary dataset is small. Using six fMRI datasets linked by various subsets of shared subjects, we show that the method yields improved performance in various predictive tasks. Our tests are performed on a variety of regions of interest in the brain and across various stimuli.},
author = {Zhang, Hejia and Chen, Po-Hsuan and Ramadge, Peter},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Chen, Ramadge - 2018 - Transfer Learning on fMRI Datasets.pdf:pdf},
journal = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
pages = {595--603},
title = {{Transfer Learning on fMRI Datasets}},
url = {http://proceedings.mlr.press/v84/zhang18b.html},
volume = {84},
year = {2018}
}
@article{Deshpande2017,
abstract = {A Brain-Computer Interface (BCI) is a setup permitting the control of external devices by decoding brain activity. Electroencephalography (EEG) has been used for decoding brain activity since it is non-invasive, cheap, portable, and has high temporal resolution to allow real-time operation. Due to its poor spatial specificity, BCIs based on EEG can require extensive training and multiple trials to decode brain activity (consequently slowing down the operation of the BCI). On the other hand, BCIs based on functional magnetic resonance imaging (fMRI) are more accurate owing to its superior spatial resolution and sensitivity to underlying neuronal processes which are functionally localized. However, due to its relatively low temporal resolution, high cost, and lack of portability, fMRI is unlikely to be used for routine BCI. We propose a new approach for transferring the capabilities of fMRI to EEG, which includes simultaneous EEG/fMRI sessions for finding a mapping from EEG to fMRI, followed by a BCI run from only EEG data, but driven by fMRI-like features obtained from the mapping identified previously. Our novel data-driven method is likely to discover latent linkages between electrical and hemodynamic signatures of neural activity hitherto unexplored using model-driven methods, and is likely to serve as a template for a novel multi-modal strategy wherein cross-modal EEG-fMRI interactions are exploited for the operation of a unimodal EEG system, leading to a new generation of EEG-based BCIs.},
author = {Deshpande, Gopikrishna and Rangaprakash, D. and Oeding, Luke and Cichocki, Andrzej and Hu, Xiaoping P.},
doi = {10.3389/fnins.2017.00246},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Deshpande et al. - 2017 - A New Generation of Brain-Computer Interfaces Driven by Discovery of Latent EEG-fMRI Linkages Using Tensor Dec.pdf:pdf},
issn = {1662-453X},
journal = {Frontiers in Neuroscience},
keywords = {Brain-computer interface,EEG,Functional MRI,Simultaneous EEG/fMRI,Tensor decomposition},
month = {jun},
number = {JUN},
pages = {1--13},
pmid = {28638316},
title = {{A New Generation of Brain-Computer Interfaces Driven by Discovery of Latent EEG-fMRI Linkages Using Tensor Decomposition}},
url = {http://journal.frontiersin.org/article/10.3389/fnins.2017.00246/full},
volume = {11},
year = {2017}
}
@article{Wu2017a,
abstract = {Bayesian optimization has been successful at global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to decrease the number of objective function evaluations required for good performance. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for which we show one-step Bayes-optimality, asymptotic consistency, and greater one-step value of information than is possible in the derivative-free setting. Our procedure accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the d-KG acquisition function and its gradient using a novel fast discretization-free technique. We show d-KG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
archivePrefix = {arXiv},
arxivId = {1703.04389},
author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew Gordon and Frazier, Peter I.},
eprint = {1703.04389},
issn = {10495258},
title = {{Bayesian Optimization with Gradients}},
year = {2017}
}
@article{Carlberg2019,
abstract = {This work introduces the network uncertainty quantification (NetUQ) method for performing uncertainty propagation in systems composed of interconnected components. The method assumes the existence of a collection of components, each of which is characterized by exogenous-input random variables, endogenous-input random variables, output random variables, and a local uncertainty-propagation operator that computes output random variables from input random variables. The method assembles the full-system network by connecting components, which is achieved simply by associating endogenous-input random variables for each component with output random variables from other components; no other inter-component compatibility conditions are required. The network uncertainty-propagation problem is: Compute output random variables for all components given all exogenous-input random variables. To solve this problem, the method applies classical relaxation methods (i.e., Jacobi and Gauss--Seidel iteration with Anderson acceleration), which require only black-box evaluations of component uncertainty-propagation operators. Compared with other available methods, this approach is applicable to any network topology, promotes component independence by enabling components to employ tailored uncertainty-propagation operators, supports general functional representations of random variables, and requires no offline preprocessing stage. Also, because the method propagates functional representations of random variables throughout the network (and not, e.g., probability density functions), the joint distribution of any set of random variables throughout the network can be estimated a posteriori in a straightforward manner. We perform supporting convergence and error analysis and execute numerical experiments that demonstrate the weak- and strong-scaling performance of the method.},
archivePrefix = {arXiv},
arxivId = {1908.11476},
author = {Carlberg, Kevin and Guzzetti, Sofia and Khalil, Mohammad and Sargsyan, Khachik},
eprint = {1908.11476},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Carlberg et al. - 2019 - The network uncertainty quantification method for propagating uncertainties in component-based systems.pdf:pdf},
keywords = {1,35r60,49m20,49m27,60h15,60h35,65c20,65n55,ams subject classifications,and engineering,anderson acceleration,domain decomposition,introduction,many systems in science,network,ranging from power grids,relaxation methods,to gas,uncertainty propagation,uncertainty quantification},
pages = {1--34},
title = {{The network uncertainty quantification method for propagating uncertainties in component-based systems}},
url = {http://arxiv.org/abs/1908.11476},
year = {2019}
}
@article{Danks2013,
abstract = {Arguments, claims, and discussions about the “level of description” of a theory are ubiquitous in cognitive science. Such talk is typically expressed more precisely in terms of the granularity of the theory, or in terms of Marr's (1982) three levels (computational, algorithmic, and implementation). I argue that these ways of understanding levels of description are insufficient to capture the range of different types of theoretical commitments that one can have in cognitive science. When we understand these commitments as points in a multi-dimensional space, we find that we must also reconsider our understanding of intertheoretic relations. In particular, we should understand cognitive theories as constraining one another, rather than reducing to one another.},
author = {Danks, David},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Danks - 2013 - Moving from Levels {\&} Reduction to Dimensions {\&} Constraints.pdf:pdf},
journal = {Proceedings of the 35th annual conference of the cognitive science society},
keywords = {cognitive science,intertheoretic constraint,it is customary within,level of description,limitations of levels,marr,our theories as,philosophy of,reduction,science to talk about},
pages = {2124--2129},
title = {{Moving from Levels {\&} Reduction to Dimensions {\&} Constraints}},
year = {2013}
}
@article{Peirce2019,
abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and H{\"{o}}chenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindel{\o}v, Jonas Kristoffer},
doi = {10.3758/s13428-018-01193-y},
file = {:Users/mshvarts/Downloads/Peirce2019{\_}Article{\_}PsychoPy2ExperimentsInBehavior.pdf:pdf},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Experiment,Open science,Open-source,Psychology,Reaction time,Software,Timing},
number = {1},
pages = {195--203},
pmid = {30734206},
publisher = {Behavior Research Methods},
title = {{PsychoPy2: Experiments in behavior made easy}},
volume = {51},
year = {2019}
}
@article{Chkrebtii2016,
author = {Chkrebtii, Oksana A. and Campbell, David A. and Calderhead, Ben and Girolami, Mark A.},
doi = {10.1214/16-BA1017},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chkrebtii et al. - 2016 - Bayesian Solution Uncertainty Quantification for Differential Equations.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Diffusion bridge sampling,Probabilistic solution,Stochastic differential equations},
month = {dec},
number = {4},
pages = {1239--1267},
title = {{Bayesian Solution Uncertainty Quantification for Differential Equations}},
url = {https://projecteuclid.org/euclid.ba/1480474948 https://projecteuclid.org/euclid.ba/1473276259},
volume = {11},
year = {2016}
}
@article{Khan2017,
abstract = {Variational inference is computationally challenging in models that contain both conjugate and non-conjugate terms. Methods specifically designed for conjugate models, even though computationally efficient, find it difficult to deal with non-conjugate terms. On the other hand, stochastic-gradient methods can handle the nonconjugate terms but they usually ignore the conjugate structure of the model which might result in slow convergence. In this paper, we propose a new algorithm called Conjugate-computation Variational Inference (CVI) which brings the best of the two worlds together – it uses conjugate computations for the conjugate terms and employs stochastic gradients for the rest. We derive this algorithm by using a stochastic mirror-descent method in the mean-parameter space, and then expressing each gradient step as a variational inference in a conjugate model. We demonstrate our algorithm's applicability to a large class of models and establish its convergence. Our experimental results show that our method converges much faster than the methods that ignore the conjugate structure of the model.},
archivePrefix = {arXiv},
arxivId = {1703.04265},
author = {Khan, Mohammad Emtiyaz and Lin, Wu},
eprint = {1703.04265},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Khan, Lin - 2017 - Conjugate-computation variational inference Converting variational inference in non-conjugate models to inferences in.pdf:pdf},
journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
title = {{Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models}},
volume = {54},
year = {2017}
}
@article{Zhang2016a,
abstract = {Multi-spatial-resolution change detection is a newly proposed issue and it is of great significance in remote sensing, environmental and land use monitoring, etc. Though multi-spatial-resolution image-pair are two kinds of representations of the same reality, they are often incommensurable superficially due to their different modalities and properties. In this paper, we present a novel multi-spatial-resolution change detection framework, which incorporates deep-architecture-based unsupervised feature learning and mapping-based feature change analysis. Firstly, we transform multi-resolution image-pair into the same pixel-resolution through co-registration, followed by details recovery, which is designed to remedy the spatial details lost in the registration. Secondly, the denoising autoencoder is stacked to learn local and high-level representation/feature from the local neighborhood of the given pixel, in an unsupervised fashion. Thirdly, motivated by the fact that multi-resolution image-pair share the same reality in the unchanged regions, we try to explore the inner relationships between them by building a mapping neural network. And it can be used to learn a mapping function based on the most-unlikely-changed feature-pairs, which are selected from all the feature-pairs via a coarse initial change map generated in advance. The learned mapping function can bridge the different representations and highlight changes. Finally, we can build a robust and contractive change map through feature similarity analysis, and the change detection result is obtained through the segmentation of the final change map. Experiments are carried out on four real datasets, and the results confirmed the effectiveness and superiority of the proposed method.},
author = {Zhang, Puzhao and Gong, Maoguo and Su, Linzhi and Liu, Jia and Li, Zhizhou},
doi = {10.1016/j.isprsjprs.2016.02.013},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2016 - Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remo.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Change detection,Deep neural networks,Denoising autoencoder,Feature transformation,Spatial-resolution,Stacked denoising autoencoder},
pages = {24--41},
publisher = {International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
title = {{Change detection based on deep feature representation and mapping transformation for multi-spatial-resolution remote sensing images}},
url = {http://dx.doi.org/10.1016/j.isprsjprs.2016.02.013},
volume = {116},
year = {2016}
}
@article{Athreya2018,
abstract = {We define a latent structure model (LSM) random graph as a random dot product graph (RDPG) in which the latent position distribution incorporates both probabilistic and geometric constraints, delineated by a family of underlying distributions on some fixed Euclidean space, and a structural support submanifold from which the latent positions for the graph are drawn. For a one-dimensional latent structure model with known structural support, we show how spectral estimates of the latent positions of an RDPG can be used for efficient estimation of the paramaters of the LSM. We describe how to estimate or learn the structural support in cases where it is unknown, with an illustrative focus on graphs with latent positions along the Hardy-Weinberg curve. Finally, we use the latent structure model formulation to test bilateral homology in the Drosophila connectome.},
archivePrefix = {arXiv},
arxivId = {1806.01401},
author = {Athreya, Avanti and Tang, Minh and Park, Youngser and Priebe, Carey E.},
eprint = {1806.01401},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Athreya et al. - 2018 - On estimation and inference in latent structure random graphs.pdf:pdf},
pages = {1--29},
title = {{On estimation and inference in latent structure random graphs}},
url = {http://arxiv.org/abs/1806.01401},
volume = {21218},
year = {2018}
}
@article{DeCarlo1998,
abstract = {Generalized linear models are a general class of regressionlike models for continuous and categorical response variables. Signal detection models can be formulated as a subclass of generalized linear models, and the result is a rich class of signal detection models based on different underlying distributions. An example is a signal detection model based on the extreme value distribution. The extreme value model is shown to yield unit slope receiver operating characteristic (ROC) curves for several classic data sets that are commonly given as examples of normal or logistic ROC curves with slopes that differ from unity. The result is an additive model with a simple interpretation in terms of a shift in the location of an under-lying distribution. The models can also be extended in several ways, such as to recognize response dependencies, to include random coefficients, or to allow for more general underlying probability distributions.},
author = {DeCarlo, Lawrence T.},
doi = {10.1037/1082-989X.3.2.186},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/DeCarlo - 1998 - Signal Detection Theory and Generalized Linear Models.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
number = {2},
pages = {186--205},
title = {{Signal Detection Theory and Generalized Linear Models}},
volume = {3},
year = {1998}
}
@article{Gershman2015,
abstract = {After growing up together, and mostly growing apart in the second half of the 20th century, the fields of artificial intelligence (AI), cognitive science, and neuroscience are reconverging on a shared view of the computational foundations of intelligence that promotes valuable cross-disciplinary exchanges on questions, methods, and results. We chart advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation. Advances include the development of representations and inferential procedures for large-scale probabilistic inference and machinery for enabling reflection and decisions about tradeoffs in effort, precision, and timeliness of computations. These tools are deployed toward the goal of computational rationality: identifying decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated. We highlight key concepts with examples that show the potential for interchange between computer science, cognitive science, and neuroscience.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gershman, Samuel J. and Horvitz, Eric J. and Tenenbaum, Joshua B.},
doi = {10.1126/science.aac6076},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gershman, Horvitz, Tenenbaum - 2015 - Computational rationality A converging paradigm for intelligence in brains, minds, and machines.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {10959203},
journal = {Science},
number = {6245},
pages = {273--278},
pmid = {26185246},
title = {{Computational rationality: A converging paradigm for intelligence in brains, minds, and machines}},
volume = {349},
year = {2015}
}
@article{Treutwein1995,
abstract = {Improvements in measuring thresholds, or points on a psychometric function, have advanced the field of psychophysics in the last 30 years. The arrival of laboratory computers allowed the introduction of adaptive procedures, where the presentation of the next stimulus depends on previous responses of the subject. Unfortunately, these procedures present themselves in a bewildering variety, though some of them differ only slightly. Even someone familiar with several methods cannot easily name the differences, or decide which method would be best suited for a particular application. This review tries to illuminate the historical background of adaptive procedures, explain their differences and similarities, and provide criteria for choosing among the various techniques. {\textcopyright} 1995 Elsevier Science Ltd.},
author = {Treutwein, Bernhard},
doi = {10.1016/0042-6989(95)00016-X},
file = {:Users/mshvarts/Downloads/1-s2.0-004269899500016X-main.pdf:pdf},
issn = {00426989},
journal = {Vision Research},
keywords = {Binary responses,Efficiency,Forced-choice methods,Psychometric functions,Psychophysical threshold,Sequential estimate,Yes-no methods},
number = {17},
pages = {2503--2522},
pmid = {8594817},
title = {{Adaptive psychophysical procedures}},
volume = {35},
year = {1995}
}
@article{Salimbeni2017a,
abstract = {Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to over-fitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm, which does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.},
archivePrefix = {arXiv},
arxivId = {1705.08933},
author = {Salimbeni, Hugh and Deisenroth, Marc},
eprint = {1705.08933},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salimbeni, Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep Gaussian Processes.pdf:pdf},
number = {Nips},
title = {{Doubly Stochastic Variational Inference for Deep Gaussian Processes}},
url = {http://arxiv.org/abs/1705.08933},
year = {2017}
}
@article{Madarasz2019,
abstract = {Humans and animals show remarkable flexibility in adjusting their behaviour when their goals, or rewards in the environment change. While such flexibility is a hallmark of intelligent behaviour, these multi-task scenarios remain an important challenge for machine learning algorithms and neurobiological models alike. We investigated two approaches that could enable this flexibility: factorized representations, which abstract away general aspects of a task from those prone to change, and nonparametric, memory-based approaches, which can provide a principled way of using similarity to past experiences to guide current behaviour. In particular, we combine the successor representation (SR), that factors the value of actions into expected outcomes and corresponding rewards, with evaluating task similarity through clustering the space of rewards. The proposed algorithm inverts a generative model over tasks, and dynamically samples from a flexible number of distinct SR maps while accumulating evidence about the current task context through amortized inference. It improves SR's transfer capabilities and outperforms competing algorithms and baselines in settings with both known and unsignalled rewards changes. Further, as a neurobiological model of spatial coding in the hippocampus, it explains important signatures of this representation, such as the "flickering" behaviour of hippocampal maps, and trajectory-dependent place cells (so-called splitter cells) and their dynamics. We thus provide a novel algorithmic approach for multi-task learning, as well as a common normative framework that links together these different characteristics of the brain's spatial representation.},
archivePrefix = {arXiv},
arxivId = {1906.07663},
author = {Madarasz, Tamas J.},
eprint = {1906.07663},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Madarasz - 2019 - Better transfer learning with inferred successor maps.pdf:pdf},
number = {NeurIPS},
title = {{Better transfer learning with inferred successor maps}},
url = {http://arxiv.org/abs/1906.07663},
year = {2019}
}
@article{Yang2018,
abstract = {Hand image synthesis and pose estimation from RGB images are both highly challenging tasks due to the large discrepancy between factors of variation ranging from image background content to camera viewpoint. To better analyze these factors of variation, we propose the use of disentangled representations and propose a disentangled variational autoencoder (dVAE) that allows for specific sampling and inference of these factors. The derived objective from the variational lower bound as well as the proposed training strategy are highly flexible, allowing us to handle cross-modal encoders and decoders as well as semi-supervised learning scenarios. Experiments show that our dVAE can synthesize highly realistic images of the hand specifiable by both pose and image background content and also estimate 3D hand poses from RGB images with accuracy competitive with state-of-the-art on two public benchmarks.},
archivePrefix = {arXiv},
arxivId = {1812.01002},
author = {Yang, Linlin and Yao, Angela},
eprint = {1812.01002},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Yao - 2018 - Disentangling Latent Hands for Image Synthesis and Pose Estimation.pdf:pdf},
title = {{Disentangling Latent Hands for Image Synthesis and Pose Estimation}},
url = {http://arxiv.org/abs/1812.01002},
year = {2018}
}
@article{Liu2018,
abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evo-lution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive ex-periments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques.},
archivePrefix = {arXiv},
arxivId = {1806.09055},
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
eprint = {1806.09055},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Simonyan, Yang - 2018 - DARTS Differentiable Architecture Search.pdf:pdf},
title = {{DARTS: Differentiable Architecture Search}},
url = {https://arxiv.org/pdf/1806.09055.pdf},
year = {2018}
}
@article{Solin2018a,
abstract = {Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension {\$}m{\$} which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension {\$}m{\$} to {\$}\backslashmathcal{\{}O{\}}(m{\^{}}2){\$} per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100{\~{}}Hz.},
archivePrefix = {arXiv},
arxivId = {1811.06588},
author = {Solin, Arno and Hensman, James and Turner, Richard E.},
eprint = {1811.06588},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Solin, Hensman, Turner - 2018 - Infinite-Horizon Gaussian Processes.pdf:pdf},
title = {{Infinite-Horizon Gaussian Processes}},
url = {http://arxiv.org/abs/1811.06588},
year = {2018}
}
@article{Knoblauch2019,
abstract = {This paper introduces a generalized representation of Bayesian inference. It is derived axiomatically, recovering existing Bayesian methods as special cases. We then use it to prove that variational inference (VI) based on the Kullback-Leibler Divergence with a variational family Q produces the uniquely optimal Q-constrained approximation to the exact Bayesian inference problem. Surprisingly, this implies that standard VI dominates any other Q-constrained approximation to the exact Bayesian inference problem. This means that alternative Q-constrained approximations such as VI minimizing other divergences and Expectation Propagation can produce better posteriors than VI only by implicitly targeting more appropriate Bayesian inference problems. Inspired by this, we introduce Generalized Variational Inference (GVI), a modular approach for instead solving such alternative inference problems explicitly. We explore some applications of GVI, including robustness and better marginals. Lastly, we derive black box GVI and apply it to Bayesian Neural Networks and Deep Gaussian Processes, where GVI can comprehensively outperform competing methods.},
archivePrefix = {arXiv},
arxivId = {1904.02063},
author = {Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
eprint = {1904.02063},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Knoblauch, Jewson, Damoulas - 2019 - Generalized Variational Inference.pdf:pdf},
title = {{Generalized Variational Inference}},
url = {http://arxiv.org/abs/1904.02063},
year = {2019}
}
@article{Gal2015,
abstract = {We show that a neural network with arbitrary depth and non-linearities, with dropout applied before every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation might offer an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. This document is an appendix for the main paper "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning" by Gal and Ghahramani, 2015.},
archivePrefix = {arXiv},
arxivId = {1506.02157},
author = {Gal, Yarin and Ghahramani, Zoubin},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1506.02157},
isbn = {1506.02142},
issn = {10414347},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Appendix}},
year = {2015}
}
@article{Stuhlmuller2014,
abstract = {A wide range of human reasoning patterns can be explained as conditioning in probabilistic models; however, conditioning has traditionally been viewed as an operation applied to such models, not represented in such models. We describe how probabilistic programs can explicitly represent conditioning as part of a model. This enables us to describe reasoning about others' reasoning using nested conditioning. Much of human reasoning is about the beliefs, desires, and intentions of other people; we use probabilistic programs to formalize these inferences in a way that captures the flexibility and inherent uncertainty of reasoning about other agents. We express examples from game theory, artificial intelligence, and linguistics as recursive probabilistic programs and illustrate how this representation language makes it easy to explore new directions in each of these fields. We discuss the algorithmic challenges posed by these kinds of models and describe how dynamic programming techniques can help address these challenges. {\textcopyright} 2014.},
author = {Stuhlm{\"{u}}ller, A. and Goodman, N. D.},
doi = {10.1016/j.cogsys.2013.07.003},
isbn = {1389-0417},
issn = {13890417},
journal = {Cognitive Systems Research},
number = {1},
title = {{Reasoning about reasoning by nested conditioning: Modeling theory of mind with probabilistic programs}},
volume = {28},
year = {2014}
}
@article{Zhang2015,
abstract = {Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this article is to provide a comprehensive study and an updated review on sparse representation and to supply a guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: sparse representation with {\$}l{\_}0{\$}-norm minimization, sparse representation with {\$}l{\_}p{\$}-norm (0{\$}{\textless}{\$}p{\$}{\textless}{\$}1) minimization, sparse representation with {\$}l{\_}1{\$}-norm minimization and sparse representation with {\$}l{\_}{\{}2,1{\}}{\$}-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: greedy strategy approximation, constrained optimization, proximity algorithm-based optimization, and homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. Specifically, an experimentally comparative study of these sparse representation algorithms was presented. The Matlab code used in this paper can be available at: http://www.yongxu.org/lunwen.html.},
archivePrefix = {arXiv},
arxivId = {1602.07017},
author = {Zhang, Zheng and Xu, Yong and Yang, Jian and Li, Xuelong and Zhang, David},
doi = {10.1109/ACCESS.2015.2430359},
eprint = {1602.07017},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2015 - A Survey of Sparse Representation Algorithms and Applications.pdf:pdf},
isbn = {2014041717},
issn = {21693536},
journal = {IEEE Access},
keywords = {Sparse representation,compressive sensing,constrained optimization,dictionary learning,greedy algorithm,homotopy algorithm,proximal algorithm},
pages = {490--530},
pmid = {16935566},
title = {{A Survey of Sparse Representation: Algorithms and Applications}},
volume = {3},
year = {2015}
}
@article{Bui2014,
abstract = {Gaussian process regression can be accelerated by constructing a small pseudodataset to summarize the observed data. This idea sits at the heart of many approximation schemes, but such an approach requires the number of pseudo-datapoints to be scaled with the range of the input space if the accuracy of the approximation is to be maintained. This presents problems in time-series settings or in spatial datasets where large numbers of pseudo-datapoints are required since computation typically scales quadratically with the pseudo-dataset size. In this paper we devise an approximation whose complexity grows linearly with the number of pseudo-datapoints. This is achieved by imposing a tree or chain structure on the pseudo-datapoints and calibrating the approximation using a Kullback-Leibler (KL) minimization. Inference and learning can then be performed efficiently using the Gaussian belief propagation algorithm. We demonstrate the validity of our approach on a set of challenging regression tasks including missing data imputation for audio and spatial datasets. We trace out the speed-accuracy trade-off for the new method and show that the frontier dominates those obtained from a large number of existing approximation techniques.},
author = {Bui, Thang and Turner, Richard},
issn = {10495258},
journal = {Advances in Nueral Information Processing Systems 27 (Proceedings of NIPS)},
title = {{Tree-structured Gaussian Process Approximations}},
year = {2014}
}
@inproceedings{Sykacek1999,
author = {Sykacek, Peter},
booktitle = {NIPS 12},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sykacek - 1999 - On input selection with reversible jump Markov chain Monte Carlo sampling.pdf:pdf},
title = {{On input selection with reversible jump Markov chain Monte Carlo sampling}},
year = {1999}
}
@article{Laurent2017,
abstract = {We study the loss surface of neural networks equipped with a hinge loss criterion and ReLU or leaky ReLU nonlinearities. Any such network defines a piecewise multilinear form in parameter space. By appealing to harmonic analysis we show that all local minima of such network are non-differentiable, except for those minima that occur in a region of parameter space where the loss surface is perfectly flat. Non-differentiable minima are therefore not technicalities or pathologies; they are heart of the problem when investigating the loss of ReLU networks. As a consequence, we must employ techniques from nonsmooth analysis to study these loss surfaces. We show how to apply these techniques in some illustrative cases.},
archivePrefix = {arXiv},
arxivId = {1712.10132},
author = {Laurent, Thomas and von Brecht, James},
eprint = {1712.10132},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Laurent, von Brecht - 2017 - The Multilinear Structure of ReLU Networks.pdf:pdf},
title = {{The Multilinear Structure of ReLU Networks}},
url = {http://arxiv.org/abs/1712.10132},
year = {2017}
}
@article{Koh2017,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Koh, Pang Wei and Liang, Percy},
eprint = {1703.04730},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Koh, Liang - 2017 - Understanding Black-box Predictions via Influence Functions.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
month = {mar},
title = {{Understanding Black-box Predictions via Influence Functions}},
url = {http://arxiv.org/abs/1703.04730},
year = {2017}
}
@article{Ng2006,
abstract = {Helicopter flight is widely regarded to be a challenging control problem. As heli- copters are highly unstable at low speeds, it is particularly difficult to design con- trollers for low speed aerobatic maneuvers. In this paper, we describe a successful application of reinforcement learning to designing a controller for sustained in- verted flight on an autonomous helicopter. Using data collected from the helicopter in flight, we began by learning a stochastic, nonlinear model of the helicopters dynamics. Then, a reinforcement learning algorithm was applied to automatically learn a controller for autonomous inverted hovering. Finally, the resulting controller was successfully tested on our autonomous helicopter platform.},
archivePrefix = {arXiv},
arxivId = {astro-ph.HE/0911.4714},
author = {Ng, Andrew Y. and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
doi = {10.1007/11552246_35},
eprint = {0911.4714},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ng et al. - 2006 - Autonomous inverted helicopter flight via reinforcement earning.pdf:pdf},
isbn = {3540288163},
issn = {16107438},
journal = {Springer Tracts in Advanced Robotics},
pages = {363--372},
primaryClass = {astro-ph.HE},
title = {{Autonomous inverted helicopter flight via reinforcement earning}},
volume = {21},
year = {2006}
}
@inproceedings{Maclaurin2015,
abstract = {Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose tool for Bayesian inference. However, MCMC cannot be practically applied to large data sets because of the prohibitive cost of evaluating every likelihood term at every iteration. Here we present Firefly Monte Carlo (FlyMC) an auxiliary variable MCMC algorithm that only queries the likelihoods of a potentially small subset of the data at each iteration yet simulates from the exact posterior distribution, in contrast to recent proposals that are approximate even in the asymptotic limit. FlyMC is compatible with a wide variety of modern MCMC algorithms, and only requires a lower bound on the per-datum likelihood factors. In experiments, we find that FlyMC generates samples from the posterior more than an order of magnitude faster than regular MCMC, opening up MCMC methods to larger datasets than were previously considered feasible.},
author = {Maclaurin, Dougal and Adams, Ryan P.},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
isbn = {9781577357384},
issn = {10450823},
title = {{Firefly Monte Carlo: Exact MCMC with subsets of data}},
volume = {2015-Janua},
year = {2015}
}
@inproceedings{Liu2016b,
address = {Philadelphia, PA},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Liu, Zitao and Hauskrecht, Milos},
booktitle = {Proceedings of the 2016 SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611974348.91},
eprint = {15334406},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Hauskrecht - 2016 - Learning Linear Dynamical Systems from Multivariate Time Series A Matrix Factorization Based Framework.pdf:pdf},
isbn = {978-1-61197-434-8},
issn = {1527-5418},
keywords = {bold,fmri,methods,resting-state},
month = {jun},
number = {3},
pages = {810--818},
pmid = {24655651},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Learning Linear Dynamical Systems from Multivariate Time Series: A Matrix Factorization Based Framework}},
url = {https://epubs.siam.org/doi/10.1137/1.9781611974348.91},
volume = {25},
year = {2016}
}
@article{Kingma2015,
abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
archivePrefix = {arXiv},
arxivId = {1506.02557},
author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
doi = {10.1016/S0733-8619(03)00096-3},
eprint = {1506.02557},
isbn = {1506.02557},
issn = {10495258},
pmid = {15062530},
title = {{Variational Dropout and the Local Reparameterization Trick}},
year = {2015}
}
@article{Evans2015,
abstract = {An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.},
archivePrefix = {arXiv},
arxivId = {1512.05832},
author = {Evans, Owain and Stuhlmueller, Andreas and Goodman, Noah D.},
eprint = {1512.05832},
isbn = {9781577357605},
title = {{Learning the Preferences of Ignorant, Inconsistent Agents}},
year = {2015}
}
@article{Ferdous2017,
abstract = {The visual lifelogging activity enables a user, the lifelogger, to passively capture images from a first-person perspective and ultimately create a visual diary encoding every possible aspect of her life with unprecedented details. In recent years, it has gained popularities among different groups of users. However, the possibility of ubiquitous presence of lifelogging devices specifically in private spheres has raised serious concerns with respect to personal privacy. In this article, we have presented a thorough discussion of privacy with respect to visual lifelogging. We have re-adjusted the existing definition of lifelogging to reflect different aspects of privacy and introduced a first-ever privacy threat model identifying several threats with respect to visual lifelogging. We have also shown how the existing privacy guidelines and approaches are inadequate to mitigate the identified threats. Finally, we have outlined a set of requirements and guidelines that can be used to mitigate the identified threats while designing and developing a privacy-preserving framework for visual lifelogging.},
author = {Ferdous, Md Sadek and Chowdhury, Soumyadeb and Jose, Joemon M.},
doi = {10.1016/j.pmcj.2017.03.003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ferdous, Chowdhury, Jose - 2017 - Analysing privacy in visual lifelogging.pdf:pdf},
issn = {15741192},
journal = {Pervasive and Mobile Computing},
keywords = {Lifelogging,OECD privacy guidelines,Privacy,Privacy-by-design},
pages = {430--449},
publisher = {Elsevier B.V.},
title = {{Analysing privacy in visual lifelogging}},
url = {http://dx.doi.org/10.1016/j.pmcj.2017.03.003},
volume = {40},
year = {2017}
}
@article{Sinha2018a,
abstract = {In this paper, we provide a novel approach to capture causal interaction in a dynamical system from time-series data. In $\backslash$cite{\{}sinha{\_}IT{\_}CDC2016{\}}, we have shown that the existing measures of information transfer, namely directed information, granger causality and transfer entropy fail to capture true causal interaction in dynamical system and proposed a new definition of information transfer that captures true causal interaction. The main contribution of this paper is to show that the proposed definition of information transfer in $\backslash$cite{\{}sinha{\_}IT{\_}CDC2016{\}}$\backslash$cite{\{}sinha{\_}IT{\_}ICC{\}} can be computed from time-series data. We use transfer operator theoretic framework involving Perron-Frobenius and Koopman operators for the data-driven approximation of the system dynamics and for the computation of information transfer. Several examples involving linear and nonlinear system dynamics are presented to verify the efficiency of the developed algorithm.},
archivePrefix = {arXiv},
arxivId = {1803.08558},
author = {Sinha, Subhrajit and Vaidya, Umesh},
eprint = {1803.08558},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sinha, Vaidya - 2018 - On Data-Driven Computation of Information Transfer for Causal Inference in Dynamical Systems.pdf:pdf},
number = {1},
title = {{On Data-Driven Computation of Information Transfer for Causal Inference in Dynamical Systems}},
url = {http://arxiv.org/abs/1803.08558},
year = {2018}
}
@article{Laird2017,
abstract = {The purpose of this article is to begin the process of engaging the international research community in developing what can be called a standard model of the mind, where the mind we have in mind here is human-like. The notion of a standard model has its roots in physics, where over more than a half-century the international community has developed and tested a standard model that combines much of what is known about particles. This model is assumed to be internally consistent, yet still have major gaps. Its function is to serve as a cumulative reference point for the field while also driving efforts to both extend and break it.},
author = {Laird, John E and Lebiere, Christian and Rosenbloom, Paul S},
doi = {10.1609/aimag.v38i4.2744},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Laird, Lebiere, Rosenbloom - 2017 - A Standard Model of the Mind Toward a Common Computational Framework across Artificial Intelligence,.pdf:pdf},
issn = {0738-4602},
journal = {AI Magazine},
keywords = {artificial intelligence,cognitive architecture,cognitive science,robotics,standard model},
month = {dec},
number = {4},
pages = {13},
title = {{A Standard Model of the Mind: Toward a Common Computational Framework across Artificial Intelligence, Cognitive Science, Neuroscience, and Robotics}},
url = {http://aaai.org/ojs/index.php/aimagazine/article/view/2744},
volume = {38},
year = {2017}
}
@article{Bratieres2015,
abstract = {We introduce a conceptually novel structured prediction model, GPstruct, which is kernelized, non-parametric and Bayesian, by design. We motivate the model with respect to existing approaches, among others, conditional random fields (CRFs), maximum margin Markov networks (M3N), and structured support vector machines (SVMstruct), which embody only a subset of its properties. We present an inference procedure based on Markov Chain Monte Carlo. The framework can be instantiated for a wide range of structured objects such as linear chains, trees, grids, and other general graphs. As a proof of concept, the model is benchmarked on several natural language processing tasks and a video gesture segmentation task involving a linear chain structure. We show prediction accuracies for GPstruct which are comparable to or exceeding those of CRFs and SVMstruct.},
author = {Brati{\`{e}}res, S{\'{e}}bastien and Quadrianto, Novi and Ghahramani, Zoubin},
doi = {10.1109/TPAMI.2014.2366151},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {7},
title = {{GPstruct: Bayesian Structured Prediction Using Gaussian Processes}},
volume = {37},
year = {2015}
}
@article{Fawaz2018,
abstract = {Transfer learning for deep neural networks is the process of first training a base network on a source dataset, and then transferring the learned features (the network's weights) to a second network to be trained on a target dataset. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike for image recognition problems, transfer learning techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved if the model is fine-tuned from a pre-trained neural network instead of training it from scratch. In this paper, we fill this gap by investigating how to transfer deep CNNs for the TSC task. To evaluate the potential of transfer learning, we performed extensive experiments using the UCR archive which is the largest publicly available TSC benchmark containing 85 datasets. For each dataset in the archive, we pre-trained a model and then fine-tuned it on the other datasets resulting in 7140 different deep neural networks. These experiments revealed that transfer learning can improve or degrade the model's predictions depending on the dataset used for transfer. Therefore, in an effort to predict the best source dataset for a given target dataset, we propose a new method relying on Dynamic Time Warping to measure inter-datasets similarities. We describe how our method can guide the transfer to choose the best source dataset leading to an improvement in accuracy on 71 out of 85 datasets.},
archivePrefix = {arXiv},
arxivId = {1811.01533},
author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
doi = {10.1109/BigData.2018.8621990},
eprint = {1811.01533},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Fawaz et al. - 2018 - Transfer learning for time series classification.pdf:pdf},
isbn = {9781538650356},
title = {{Transfer learning for time series classification}},
url = {http://arxiv.org/abs/1811.01533},
year = {2018}
}
@article{Cohn2014,
author = {Cohn, Robert and Durfee, Edmund},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cohn, Durfee - 2014 - Characterizing EVOI-Sufficient k -Response Query Sets in Decision Problems.pdf:pdf},
issn = {15337928},
journal = {Aistats},
title = {{Characterizing EVOI-Sufficient k -Response Query Sets in Decision Problems}},
volume = {33},
year = {2014}
}
@article{Sykacek2002,
author = {Sykacek, Peter and Roberts, Stephen},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sykacek, Roberts - 2002 - Bayesian time series classification.pdf:pdf},
journal = {Advances in Neural Processing Systems},
pages = {937--944},
title = {{Bayesian time series classification}},
volume = {14},
year = {2002}
}
@article{Neiswanger2019,
abstract = {Optimizing an expensive-to-query function is a common task in science and engineering, where it is beneficial to keep the number of queries to a minimum. A popular strategy is Bayesian optimization (BO), which leverages probabilistic models for this task. Most BO today uses Gaussian processes (GPs), or a few other surrogate models. However, there is a broad set of Bayesian modeling techniques that we may want to use to capture complex systems and reduce the number of queries. Probabilistic programs (PPs) are modern tools that allow for flexible model composition, incorporation of prior information, and automatic inference. In this paper, we develop ProBO, a framework for BO using only standard operations common to most PPs. This allows a user to drop in an arbitrary PP implementation and use it directly in BO. To do this, we describe black box versions of popular acquisition functions that can be used in our framework automatically, without model-specific derivation, and show how to optimize these functions. We also introduce a model, which we term the Bayesian Product of Experts, that integrates into ProBO and can be used to combine information from multiple models implemented with different PPs. We show empirical results using multiple PP implementations, and compare against standard BO methods.},
archivePrefix = {arXiv},
arxivId = {1901.11515},
author = {Neiswanger, Willie and Kandasamy, Kirthevasan and Poczos, Barnabas and Schneider, Jeff and Xing, Eric},
eprint = {1901.11515},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Neiswanger et al. - 2019 - ProBO a Framework for Using Probabilistic Programming in Bayesian Optimization.pdf:pdf},
isbn = {1901.11515v1},
title = {{ProBO: a Framework for Using Probabilistic Programming in Bayesian Optimization}},
url = {http://arxiv.org/abs/1901.11515},
year = {2019}
}
@article{Al-Shedivat2016,
abstract = {Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.},
archivePrefix = {arXiv},
arxivId = {1610.08936},
author = {Al-Shedivat, Maruan and Wilson, Andrew Gordon and Saatchi, Yunus and Hu, Zhiting and Xing, Eric P.},
eprint = {1610.08936},
issn = {15337928},
title = {{Learning Scalable Deep Kernels with Recurrent Structure}},
year = {2016}
}
@article{Zeng,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.01256v2},
author = {Zeng, Guanxiong and Chen, Yang and Cui, Bo and Yu, Shan},
eprint = {arXiv:1810.01256v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zeng et al. - Unknown - Continuous Learning of Context-dependent Processing in Neural Networks.pdf:pdf},
pages = {1--16},
title = {{Continuous Learning of Context-dependent Processing in Neural Networks}}
}
@article{Nguyen2019,
abstract = {To make decisions organisms often accumulate information across multiple timescales. However, most experimental and modeling studies of decision-making focus on sequences of independent trials. On the other hand, natural environments are characterized by long temporal correlations, and evidence used to make a present choice is often relevant to future decisions. To understand decision-making under these conditions we analyze how a model ideal observer accumulates evidence to freely make choices across a sequence of correlated trials. We use principles of probabilistic inference to show that an ideal observer incorporates information obtained on one trial as an initial bias on the next. This bias decreases the time, but not the accuracy of the next decision. Furthermore, in finite sequences of trials the rate of reward is maximized when the observer deliberates longer for early decisions, but responds more quickly towards the end of the sequence. Our model also explains experimentally observed patterns in decision times and choices, thus providing a mathematically principled foundation for evidence-accumulation models of sequential decisions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.03872v1},
author = {Nguyen, Khanh P. and Josi{\'{c}}, Kre{\v{s}}imir and Kilpatrick, Zachary P.},
doi = {10.1016/j.jmp.2018.11.001},
eprint = {arXiv:1806.03872v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen, Josi{\'{c}}, Kilpatrick - 2019 - Optimizing sequential decisions in the drift–diffusion model.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Decision-making,Drift–diffusion model,Reward rate,Sequential correlations},
pages = {32--47},
title = {{Optimizing sequential decisions in the drift–diffusion model}},
volume = {88},
year = {2019}
}
@article{Howard2018,
abstract = {Evidence accumulation models of simple decision-making have long assumed that the brain estimates a scalar decision variable corresponding to the log-likelihood ratio of the two alternatives. Typical neural implementations of this algorithmic cognitive model assume that large numbers of neurons are each noisy exemplars of the scalar decision variable. Here we propose a neural implementation of the diffusion model in which many neurons construct and maintain the Laplace transform of the distance to each of the decision bounds. As in classic findings from brain regions including LIP, the firing rate of neurons coding for the Laplace transform of net accumulated evidence grows to a bound during random dot motion tasks. However, rather than noisy exemplars of a single mean value, this approach makes the novel prediction that firing rates grow to the bound exponentially, across neurons there should be a distribution of different rates. A second set of neurons records an approximate inversion of the Laplace transform, these neurons directly estimate net accumulated evidence. In analogy to time cells and place cells observed in the hippocampus and other brain regions, the neurons in this second set have receptive fields along a "decision axis." This finding is consistent with recent findings from rodent recordings. This theoretical approach places simple evidence accumulation models in the same mathematical language as recent proposals for representing time and space in cognitive models for memory.},
archivePrefix = {arXiv},
arxivId = {1806.04122},
author = {Howard, Marc W and Luzardo, Andre and Tiganj, Zoran},
eprint = {1806.04122},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Luzardo, Tiganj - 2018 - Evidence accumulation in a Laplace domain decision space.pdf:pdf},
month = {jun},
number = {1968},
title = {{Evidence accumulation in a Laplace domain decision space}},
url = {http://arxiv.org/abs/1806.04122},
year = {2018}
}
@article{Khrulkov2017,
abstract = {Deep neural networks are surprisingly efficient at solving practical tasks, but the theory behind this phenomenon is only starting to catch up with the practice. Numerous works show that depth is the key to this efficiency. A certain class of deep convolutional networks -- namely those that correspond to the Hierarchical Tucker (HT) tensor decomposition -- has been proven to have exponentially higher expressive power than shallow networks. I.e. a shallow network of exponential width is required to realize the same score function as computed by the deep architecture. In this paper, we prove the expressive power theorem (an exponential lower bound on the width of the equivalent shallow network) for a class of recurrent neural networks -- ones that correspond to the Tensor Train (TT) decomposition. This means that even processing an image patch by patch with an RNN can be exponentially more efficient than a (shallow) convolutional network with one hidden layer. Using theoretical results on the relation between the tensor decompositions we compare expressive powers of the HT- and TT-Networks. We also implement the recurrent TT-Networks and provide numerical evidence of their expressivity.},
archivePrefix = {arXiv},
arxivId = {1711.00811},
author = {Khrulkov, Valentin and Novikov, Alexander and Oseledets, Ivan},
eprint = {1711.00811},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Khrulkov, Novikov, Oseledets - 2017 - Expressive power of recurrent neural networks.pdf:pdf},
pages = {1--12},
title = {{Expressive power of recurrent neural networks}},
url = {http://arxiv.org/abs/1711.00811},
year = {2017}
}
@article{Baudart2018,
abstract = {Stan is a popular probabilistic programming language with a self-contained syntax and semantics that is close to graphical models. Unfortunately, existing embeddings of Stan in Python use multi-line strings. That approach forces users to switch between two different language styles, with no support for syntax highlighting or simple error reporting within the Stan code. This paper tackles the question of whether Stan could use Python syntax while retaining its self-contained semantics. The answer is yes, that can be accomplished by reinterpreting the Python syntax. This paper introduces Yaps, a new frontend to Stan based on reinterpreted Python. We tested Yaps on over a thousand Stan models and made it available open-source.},
archivePrefix = {arXiv},
arxivId = {1812.04125},
author = {Baudart, Guillaume and Hirzel, Martin and Kate, Kiran and Mandel, Louis and Shinnar, Avraham},
eprint = {1812.04125},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Baudart et al. - 2018 - Yaps Python Frontend to Stan.pdf:pdf},
title = {{Yaps: Python Frontend to Stan}},
url = {http://arxiv.org/abs/1812.04125},
year = {2018}
}
@unpublished{Salakhutdinov2015,
abstract = {Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many artificial intelligence–related tasks, including object recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires models with deep architectures that involve many layers of nonlinear processing. In this article, we review several popular deep learning models, including deep belief networks and deep Boltzmann machines. We show that (a) these deep generative models, which contain many layers of latent variables and millions of parameters, can be learned efficiently, and (b) the learned high-level feature representations can be successfully applied in many application domains, including visual object recognition, information retrieval, classification, and regression tasks. },
archivePrefix = {arXiv},
arxivId = {1803.03324},
author = {Salakhutdinov, Ruslan},
booktitle = {SSRN},
doi = {10.1146/annurev-statistics-010814-020120},
eprint = {1803.03324},
isbn = {978-0-494-61080-0},
issn = {2326-8298},
pmid = {16840546},
title = {{Learning Deep Generative Models}},
year = {2015}
}
@article{Wang2014a,
abstract = {In this paper we present a novel non-parametric approach to Bayesian filtering, where the prediction and observation models are learned in an online fashion. Our approach is able to handle multimodal distributions over both models by employing a mixture model representation with Gaussian Processes (GP) based components. To cope with the increasing complexity of the estimation process, we explore two computationally efficient GP variants, sparse online GP and local GP, which help to manage computation requirements for each mixture component. Our experiments demonstrate that our approach can track human motion much more accurately than existing approaches that learn the prediction and observation models offline and do not update these models with the incoming data stream.},
author = {Wang, Y. and Brubaker, Marcus A. and Chaib-Draa, B. and Urtasun, Raquel},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2014 - Bayesian filtering with online Gaussian process latent variable models.pdf:pdf},
isbn = {9780974903910},
journal = {Uncertainty in Artificial Intelligence},
title = {{Bayesian filtering with online Gaussian process latent variable models}},
year = {2014}
}
@article{VanVaerenbergh2016,
abstract = {We study the relationship between online Gaussian process (GP) regression and kernel least mean squares (KLMS) algorithms. While the latter have no capacity of storing the entire posterior distribution during online learning, we discover that their operation corresponds to the assumption of a fixed posterior covariance that follows a simple parametric model. Interestingly, several well-known KLMS algorithms correspond to specific cases of this model. The probabilistic perspective allows us to understand how each of them handles uncertainty, which could explain some of their performance differences.},
archivePrefix = {arXiv},
arxivId = {arXiv:1609.03164v1},
author = {{Van Vaerenbergh}, Steven and Fernandez-Bes, Jesus and Elvira, Victor},
doi = {10.1109/MLSP.2016.7738823},
eprint = {arXiv:1609.03164v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Van Vaerenbergh, Fernandez-Bes, Elvira - 2016 - On the relationship between online Gaussian process regression and kernel least mean squ.pdf:pdf},
isbn = {9781509007462},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Gaussian processes,kernel least-mean squares,online learning,regression},
pages = {0--5},
title = {{On the relationship between online Gaussian process regression and kernel least mean squares algorithms}},
volume = {2016-Novem},
year = {2016}
}
@article{Pawlowski2017,
abstract = {Modern neural networks tend to be overconfident on unseen, noisy or incorrectly labelled data and do not produce meaningful uncertainty measures. Bayesian deep learning aims to address this shortcoming with variational approximations (such as Bayes by Backprop or Multiplicative Normalising Flows). However, current approaches have limitations regarding flexibility and scalability. We introduce Bayes by Hypernet (BbH), a new method of variational approximation that interprets hypernetworks as implicit distributions. It naturally uses neural networks to model arbitrarily complex distributions and scales to modern deep learning architectures. In our experiments, we demonstrate that our method achieves competitive accuracies and predictive uncertainties on MNIST and a CIFAR5 task, while being the most robust against adversarial attacks.},
archivePrefix = {arXiv},
arxivId = {1711.01297},
author = {Pawlowski, Nick and Brock, Andrew and Lee, Matthew C. H. and Rajchl, Martin and Glocker, Ben},
eprint = {1711.01297},
isbn = {9781510810587},
issn = {1938-7228},
title = {{Implicit Weight Uncertainty in Neural Networks}},
year = {2017}
}
@article{Russell2016,
abstract = {Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.},
archivePrefix = {arXiv},
arxivId = {1602.03506},
author = {Russell, Stuart and Dewey, Daniel and Tegmark, Max},
doi = {10.1609/aimag.v36i4.2577},
eprint = {1602.03506},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Russell, Dewey, Tegmark - 2016 - Research Priorities for Robust and Beneficial Artificial Intelligence.pdf:pdf},
isbn = {9783642032257},
issn = {0738-4602},
pages = {16--17},
title = {{Research Priorities for Robust and Beneficial Artificial Intelligence}},
url = {http://arxiv.org/abs/1602.03506},
year = {2016}
}
@article{Lu2010,
abstract = {Common spatial pattern (CSP) is a popular algorithm for classifying electroencephalogram (EEG) signals in the context of brain-computer interfaces (BCIs). This paper presents a regularization and aggregation technique for CSP in a small-sample setting (SSS). Conventional CSP is based on a sample-based covariance-matrix estimation. Hence, its performance in EEG classification deteriorates if the number of training samples is small. To address this concern, a regularized CSP (R-CSP) algorithm is proposed, where the covariance-matrix estimation is regularized by two parameters to lower the estimation variance while reducing the estimation bias. To tackle the problem of regularization parameter determination, R-CSP with aggregation (R-CSP-A) is further proposed, where a number of R-CSPs are aggregated to give an ensemble-based solution. The proposed algorithm is evaluated on data set IVa of BCI Competition III against four other competing algorithms. Experiments show that R-CSP-A significantly outperforms the other methods in average classification performance in three sets of experiments across various testing scenarios, with particular superiority in SSS.},
author = {Lu, Haiping and Eng, How Lung and Guan, Cuntai and Plataniotis, Konstantinos N. and Venetsanopoulos, Anastasios N.},
doi = {10.1109/TBME.2010.2082540},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lu et al. - 2010 - Regularized common spatial pattern with aggregation for EEG Classification in small-sample setting.pdf:pdf},
isbn = {0018-9294},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Aggregation,braincomputer interface (BCI),common spatial pattern (CSP),electroencephalogram (EEG),generic learning,regularization,small sample},
number = {12},
pages = {2936--2946},
pmid = {20889425},
title = {{Regularized common spatial pattern with aggregation for EEG Classification in small-sample setting}},
volume = {57},
year = {2010}
}
@article{Cohen2015,
abstract = {It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community.},
archivePrefix = {arXiv},
arxivId = {1509.05009},
author = {Cohen, Nadav and Sharir, Or and Shashua, Amnon},
eprint = {1509.05009},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cohen, Sharir, Shashua - 2015 - On the Expressive Power of Deep Learning A Tensor Analysis.pdf:pdf},
keywords = {arithmetic circuits,deep learning,expressive power,tensor decompositions},
pmid = {116138},
title = {{On the Expressive Power of Deep Learning: A Tensor Analysis}},
url = {http://arxiv.org/abs/1509.05009},
year = {2015}
}
@article{Han2018,
abstract = {Stein variational gradient decent (SVGD) has been shown to be a powerful approximate inference algorithm for complex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be applied when the gradient is unavailable. In this work, we develop a gradient-free variant of SVGD (GF-SVGD), which replaces the true gradient with a surrogate gradient, and corrects the induced bias by re-weighting the gradients in a proper form. We show that our GF-SVGD can be viewed as the standard SVGD with a special choice of kernel, and hence directly inherits the theoretical properties of SVGD. We shed insights on the empirical choice of the surrogate gradient and propose an annealed GF-SVGD that leverages the idea of simulated annealing to improve the performance on high dimensional complex distributions. Empirical studies show that our method consistently outperforms a number of recent advanced gradient-free MCMC methods.},
archivePrefix = {arXiv},
arxivId = {1806.02775},
author = {Han, Jun and Liu, Qiang},
eprint = {1806.02775},
issn = {10495258},
title = {{Stein Variational Gradient Descent Without Gradient}},
year = {2018}
}
@article{Huggins2018,
abstract = {Gaussian processes (GPs) offer a flexible class of priors for nonparametric Bayesian regression, but popular GP posterior inference methods are typically prohibitively slow or lack desirable finite-data guarantees on quality. We develop an approach to scalable approximate GP regression with finite-data guarantees on the accuracy of pointwise posterior mean and variance estimates. Our main contribution is a novel objective for approximate inference in the nonparametric setting: the preconditioned Fisher (pF) divergence. We show that unlike the Kullback--Leibler divergence (used in variational inference), the pF divergence bounds the 2-Wasserstein distance, which in turn provides tight bounds the pointwise difference of the mean and variance functions. We demonstrate that, for sparse GP likelihood approximations, we can minimize the pF divergence efficiently. Our experiments show that optimizing the pF divergence has the same computational requirements as variational sparse GPs while providing comparable empirical performance--in addition to our novel finite-data quality guarantees.},
archivePrefix = {arXiv},
arxivId = {1806.10234},
author = {Huggins, Jonathan H. and Campbell, Trevor and Kasprzak, Miko{\l}aj and Broderick, Tamara},
eprint = {1806.10234},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Huggins et al. - 2018 - Scalable Gaussian Process Inference with Finite-data Mean and Variance Guarantees.pdf:pdf},
title = {{Scalable Gaussian Process Inference with Finite-data Mean and Variance Guarantees}},
url = {http://arxiv.org/abs/1806.10234},
volume = {89},
year = {2018}
}
@article{Gilbert2017,
abstract = {Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable re- construction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.},
archivePrefix = {arXiv},
arxivId = {1705.08664},
author = {Gilbert, Anna C and Zhang, Yi and Lee, Kibok and Zhang, Yuting and Lee, Honglak},
eprint = {1705.08664},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gilbert et al. - 2017 - Towards Understanding the Invertibility of Convolutional Neural Networks.pdf:pdf},
month = {may},
title = {{Towards Understanding the Invertibility of Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1705.08664},
year = {2017}
}
@article{Grigorievskiy,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.08074v1},
author = {Grigorievskiy, Alexander},
eprint = {arXiv:1610.08074v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Grigorievskiy - Unknown - Gaussian Process Kernels for Popular State-Space Time Series Models.pdf:pdf},
title = {{Gaussian Process Kernels for Popular State-Space Time Series Models}}
}
@article{Haghanifar2018,
author = {Haghanifar, Sajad and Cheng, Bolong and Mccourt, Michael and Leu, Paul},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Haghanifar et al. - 2018 - Practical Complications in Applying Bayesian Optimization to Understand Tradeoffs between Substrate Fabricati.pdf:pdf},
title = {{Practical Complications in Applying Bayesian Optimization to Understand Tradeoffs between Substrate Fabrication Processes}},
year = {2018}
}
@inproceedings{Gonzalez2016,
abstract = {We present glasses: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, glasses, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss. We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.},
archivePrefix = {arXiv},
arxivId = {1510.06299},
author = {Gonz{\'{a}}lez, Javier and Osborne, Michael and Lawrence, Neil D.},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016},
eprint = {1510.06299},
title = {{GLASSES: Relieving the myopia of Bayesian optimisation}},
year = {2016}
}
@article{Cohn2016,
author = {Cohn, Robert W},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cohn - 2016 - Maximizing Expected Value of Information in Decision Problems by Querying on a Wish-to-Know Basis.pdf:pdf},
title = {{Maximizing Expected Value of Information in Decision Problems by Querying on a Wish-to-Know Basis}},
year = {2016}
}
@misc{Lake2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
booktitle = {Behavioral and Brain Sciences},
doi = {10.1017/S0140525X16001837},
eprint = {1604.00289},
isbn = {9781577357384},
issn = {14691825},
pmid = {1000303116},
title = {{Building Machines That Learn and Think Like People}},
year = {2016}
}
@article{Hazan2017,
abstract = {We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.},
archivePrefix = {arXiv},
arxivId = {1711.00946},
author = {Hazan, Elad and Singh, Karan and Zhang, Cyril},
doi = {10.1016/j.bpj.2012.09.011},
eprint = {1711.00946},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hazan, Singh, Zhang - 2017 - Learning Linear Dynamical Systems via Spectral Filtering.pdf:pdf},
issn = {1542-0086},
pages = {1--25},
pmid = {23199917},
title = {{Learning Linear Dynamical Systems via Spectral Filtering}},
url = {http://arxiv.org/abs/1711.00946},
year = {2017}
}
@article{Baltrusaitis2017,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
eprint = {1705.09406},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Baltru{\v{s}}aitis, Ahuja, Morency - 2017 - Multimodal Machine Learning A Survey and Taxonomy.pdf:pdf},
month = {may},
pages = {1--20},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1705.09406},
year = {2017}
}
@article{Soleymani2016,
abstract = {Emotions are time varying affective phenomena that are elicited as a result of stimuli. Videos and movies in particular are made to elicit emotions in their audiences. Detecting the viewers' emotions instantaneously can be used to find the emotional traces of videos. In this paper, we present our approach in instantaneously detecting the emotions of video viewers' emotions from electroencephalogram (EEG) signals and facial expressions. A set of emotion inducing videos were shown to participants while their facial expressions and physiological responses were recorded. The expressed valence (negative to positive emotions) in the videos of participants' faces were annotated by five annotators. The stimuli videos were also continuously annotated on valence and arousal dimensions. Long-short-term-memory recurrent neural networks (LSTM-RNN) and continuous conditional random fields (CCRF) were utilized in detecting emotions automatically and continuously. We found the results from facial expressions to be superior to the results from EEG signals. We analyzed the effect of the contamination of facial muscle activities on EEG signals and found that most of the emotionally valuable content in EEG features are as a result of this contamination. However, our statistical analysis showed that EEG signals still carry complementary information in presence of facial expressions.},
author = {Soleymani, Mohammad and Asghari-Esfeden, Sadjad and Fu, Yun and Pantic, Maja},
doi = {10.1109/TAFFC.2015.2436926},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Soleymani et al. - 2016 - Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection.pdf:pdf},
isbn = {1949-3045},
issn = {1949-3045},
journal = {IEEE Transactions on Affective Computing},
keywords = {Affect,CCRF,Databases,EEG,EEG signal analysis,Feature extraction,LSTM-RNN,Motion pictures,Recurrent neural networks,Tagging,Videos,arousal dimension,continuous conditional random fields,continuous emotion detection,electroencephalogram signals,electroencephalography,elicit emotions,emotion recognition,face recognition,facial expression analysis,facial expressions,facial muscle activities,implicit tagging,long-short-term-memory recurrent neural networks,negative emotions,physiological responses,positive emotions,random processes,recurrent neural nets,statistical analysis,time varying affective phenomena,valence annotation,video emotional traces,video highlight detection,video signal processing,video viewer emotion detection,viewer emotion detection},
month = {jan},
number = {1},
pages = {17--28},
title = {{Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection}},
url = {http://ieeexplore.ieee.org/document/7112127/},
volume = {7},
year = {2016}
}
@article{Yang2017a,
abstract = {Research on using electroencephalographic signals for biometric recognition has made considerable progress and is attracting growing attention in recent years. However, the usability aspects of the proposed biometric systems in the literatures have not received significant attention. In this paper, we present a comprehensive survey to examine the development and current status of various aspects of electroencephalography (EEG)-based biometric recognition. We first compare the characteristics of different stimuli that have been used for evoking biometric information bearing EEG signals. This is followed by a survey of the reported features and classifiers employed for EEG biometric recognition. To highlight the usability challenges of using EEG for biometric recognition in real-life scenarios, we propose a novel usability assessment framework which combines a number of user-related factors to evaluate the reported systems. The evaluation scores indicate a pattern of increasing usability, particularly in recent years, of EEG-based biometric systems as efforts have been made to improve the performance of such systems in realistic application scenarios. We also propose how this framework may be extended to take into account Aging effects as more performance data becomes available.},
author = {Yang, Su and Deravi, Farzin},
doi = {10.1109/THMS.2017.2682115},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Deravi - 2017 - On the Usability of Electroencephalographic Signals for Biometric Recognition A Survey.pdf:pdf},
issn = {21682291},
journal = {IEEE Transactions on Human-Machine Systems},
keywords = {Biometrics,electroencephalography (EEG),feature classification,feature extraction,usability},
number = {6},
pages = {958--969},
title = {{On the Usability of Electroencephalographic Signals for Biometric Recognition: A Survey}},
volume = {47},
year = {2017}
}
@article{Grant2018,
abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.},
archivePrefix = {arXiv},
arxivId = {1801.08930},
author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
eprint = {1801.08930},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Grant et al. - 2018 - Recasting gradient-based meta-learning as hierarchical bayes.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--13},
title = {{Recasting gradient-based meta-learning as hierarchical bayes}},
year = {2018}
}
@article{Lindes2017,
abstract = {This paper describes how language is grounded by a comprehension system called Lucia within a robotic agent called Rosie that can manipulate objects and nav-igate indoors. The whole system is built within the Soar cognitive architecture and uses Embodied Construction Grammar (ECG) as a formalism for describing lin-guistic knowledge. Grounding is per-formed using knowledge from the gram-mar itself, from the linguistic context, from the agent's perception, and from an ontology of long-term knowledge about object categories and properties and ac-tions the agent can perform. The paper al-so describes a benchmark corpus of 200 sentences in this domain, along with test versions of the world model and ontology, and gold-standard meanings for each of the sentences. The benchmark is contained in the supplemental materials.},
author = {Lindes, Peter and Mininger, Aaron and Kirk, James R. and Laird, John E.},
doi = {10.18653/v1/W17-2801},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lindes et al. - 2017 - Grounding Language for Interactive Task Learning.pdf:pdf},
journal = {Proceedings of the First Workshop on Language Grounding for Robotics},
pages = {1--9},
title = {{Grounding Language for Interactive Task Learning}},
url = {http://aclweb.org/anthology/W17-2801},
year = {2017}
}
@article{Campbell2018,
abstract = {Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.},
archivePrefix = {arXiv},
arxivId = {1802.01737},
author = {Campbell, Trevor and Broderick, Tamara},
eprint = {1802.01737},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Campbell, Broderick - 2018 - Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent.pdf:pdf},
title = {{Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent}},
url = {http://arxiv.org/abs/1802.01737},
year = {2018}
}
@article{Li2018c,
abstract = {Gaussian Process Latent Variable Model (GPLVM) is a powerful nonlinear dimension reduction model and has been widely used in many machine learning scenarios. However, the original GPLVM and its variants do not explicitly model the correlations among the original features, leading to the underutilization of underlying information involved in the data. To compensate for this shortcoming, we propose a feature-correlation-aware GPLVM (fcaGPLVM) to simultaneously learn the latent variables and the feature correlations. The main contributions of this paper are 1) introducing a set of extra latent variables into the original GPLVM and proposing a feature-correlation-aware kernel function to explicitly model the feature-description information and infer the feature correlations; 2) defining a joint objective function and developing a stochastic optimization algorithm based on the stochastic variational inference (SVI) to learn all the latent variables. To the best of our knowledge, this is the first work that explicitly considers the feature correlations in the GPLVM and makes many existing GPLVMs become its special cases. Furthermore, it can be applied to both unsupervised and supervised learnings to improve the performance of dimension reduction. Experimental results show that in these two learning scenarios the proposed models outperform their state-of-the-art counterparts.},
author = {Li, Ping and Chen, Songcan},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Li, Chen - 2018 - Feature-correlation-aware Gaussian Process Latent Variable Model.pdf:pdf},
journal = {Proceedings of The 10th Asian Conference on Machine Learning},
keywords = {feature correlations,gaussian process,gplvm,stochastic optimization},
number = {61732006},
pages = {33--48},
title = {{Feature-correlation-aware Gaussian Process Latent Variable Model}},
url = {http://proceedings.mlr.press/v95/li18a.html},
volume = {95},
year = {2018}
}
@article{Wang2014,
abstract = {Logistic regression is one core predictive modeling technique that has been used extensively in health and biomedical problems. Recently a lot of research has been focusing on enforcing sparsity on the learned model to enhance its effectiveness and interpretability, which results in sparse logistic regression model. However, no matter the original or sparse logistic regression, they require the inputs to be in vector form. This limits the applicability of logistic regression in the problems when the data cannot be naturally represented vectors (e.g., functional magnetic resonance imaging and electroencephalography signals). To handle the cases when the data are in the form of multi-dimensional arrays, we propose MulSLR: Multilinear Sparse Logistic Regression. MulSLR can be viewed as a high order extension of sparse logistic regression. Instead of solving one classification vector as in conventional logistic regression, we solve for K classification vectors in MulSLR (K is the number of modes in the data). We propose a block proximal descent approach to solve the problem and prove its convergence. The convergence rate of the proposed algorithm is also analyzed. Finally we validate the efficiency and effectiveness of MulSLR on predicting the onset risk of patients with Alzheimer's disease and heart failure.},
author = {Wang, Xiang and Wang, Fei and Qian, Buyue and Davidson, Ian and Zhang, Ping},
doi = {10.1145/2623330.2623755},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2014 - Clinical risk prediction with multilinear sparse logistic regression.pdf:pdf},
isbn = {9781450329569},
keywords = {health-,logistic regression,multilinear,proximal gradient},
pages = {145--154},
title = {{Clinical risk prediction with multilinear sparse logistic regression}},
year = {2014}
}
@article{Turner2009,
abstract = {State-space inference and learning with Gaussian processes (GPs) is an unsolved problem. We propose a new, general methodology for inference and learning in nonlinear state-space models that are described probabilistically by non-parametric GP models. We apply the expectation maximization algorithm to iterate between inference in the latent state-space and learning the parameters of the underlying GP dynamics model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.08475v2},
author = {Turner, Ryan and Deisenroth, Marc Peter and Rasmussen, Carl Edward},
doi = {10.1.1.207.2093},
eprint = {arXiv:1803.08475v2},
issn = {15324435},
journal = {Journal of Machine Learning Research},
title = {{State-Space Inference and Learning with Gaussian Processes}},
year = {2009}
}
@article{Spivak2012,
abstract = {In this paper we introduce the olog, or ontology log, a category-theoretic model for knowledge representation (KR). Grounded in formal mathematics, ologs can be rigorously formulated and cross-compared in ways that other KR models (such as semantic networks) cannot. An olog is similar to a relational database schema; in fact an olog can serve as a data repository if desired. Unlike database schemas, which are generally difficult to create or modify, ologs are designed to be user-friendly enough that authoring or reconfiguring an olog is a matter of course rather than a difficult chore. It is hoped that learning to author ologs is much simpler than learning a database definition language, despite their similarity. We describe ologs carefully and illustrate with many examples. As an application we show that any primitive recursive function can be described by an olog. We also show that ologs can be aligned or connected together into a larger network using functors. The various methods of information flow and institutions can then be used to integrate local and global world-views. We finish by providing several different avenues for future research.},
archivePrefix = {arXiv},
arxivId = {1102.1889},
author = {Spivak, David I. and Kent, Robert E.},
doi = {10.1371/journal.pone.0024274},
eprint = {1102.1889},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Spivak, Kent - 2012 - Ologs A categorical framework for knowledge representation.pdf:pdf},
isbn = {1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pages = {1--52},
pmid = {22303434},
title = {{Ologs: A categorical framework for knowledge representation}},
volume = {7},
year = {2012}
}
@article{Malach2020,
abstract = {The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training.},
archivePrefix = {arXiv},
arxivId = {2002.00585},
author = {Malach, Eran and Yehudai, Gilad and Shalev-Shwartz, Shai and Shamir, Ohad},
eprint = {2002.00585},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Malach et al. - 2020 - Proving the Lottery Ticket Hypothesis Pruning is All You Need.pdf:pdf},
title = {{Proving the Lottery Ticket Hypothesis: Pruning is All You Need}},
url = {http://arxiv.org/abs/2002.00585},
year = {2020}
}
@article{DeLathauwer2000,
abstract = {We discuss a multilinear generalization of the singular value decomposition. There is a strong analogy between several properties of the matrix and the higher-order tensor decomposition; uniqueness, link with the matrix eigenvalue decomposition, first-order perturbation effects, etc., are analyzed. We investigate how tensor symmetries affect the decomposition and propose a multilinear generalization of the symmetric eigenvalue decomposition for pair-wise symmetric tensors},
author = {{De Lathauwer}, L and {De Moor}, B and Vandewalle, J},
doi = {10.1137/S0895479896305696},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/De Lathauwer, De Moor, Vandewalle - 2000 - A multilinear singular value decomposition.pdf:pdf},
isbn = {0895479810957162},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {1,15a18,15a69,ams subject classifications,an increasing number of,higher-order tensor,introduction,multilinear algebra,pii,s0895479896305696,signal processing problems involve,singular value decomposition},
number = {4},
pages = {1253--1278},
pmid = {7519155},
title = {{A multilinear singular value decomposition}},
url = {papers2://publication/uuid/1F9C4A1E-A3EC-4F48-BB7A-5C2EA0D5E82F},
volume = {21},
year = {2000}
}
@article{Watson2017,
abstract = {QUEST is a Bayesian adaptive psychometric testing method that allows an arbitrary number of stimulus dimensions, psychometric function parameters, and trial outcomes. It is a generalization and extension of the original QUEST procedure and incorporates many subsequent developments in the area of parametric adaptive testing. With a single procedure, it is possible to implement a wide variety of experimental designs, including conventional threshold measurement; measurement of psychometric function parameters, such as slope and lapse; estimation of the contrast sensitivity function; measurement of increment threshold functions; measurement of noise-masking functions; Thurstone scale estimation using pair comparisons; and categorical ratings on linear and circular stimulus dimensions. QUEST provides a general method to accelerate data collection in many areas of cognitive and perceptual science.},
author = {Watson, Andrew B.},
doi = {10.1167/17.3.10},
file = {:Users/mshvarts/Downloads/i1534-7362-17-3-10.pdf:pdf},
issn = {15347362},
journal = {Journal of Vision},
number = {3},
pages = {1--27},
pmid = {28355623},
title = {{QUEST+: A general multidimensional bayesian adaptive psychometric method}},
volume = {17},
year = {2017}
}
@article{Todorov2005,
abstract = {Coupling of actuators into motor synergies has been observed repeatedly, and is traditionally interpreted as a strategy for simplifying complex coordination problems. This view implies a small number of task-independent synergies. We have shown that optimal feedback control also gives rise to synergies in the absence of any simplification; the structure and number of such optimal synergies depends on the task. To compare these hypotheses, we recorded hand postures from a range of complex manipulation task. The structure of the synergies we extracted (via PCA) was task-dependent, and their number significantly exceeded previous observations in a simpler grasping task. Our results lend support to an optimal control explanation rather than a "simplicity" explanation.},
author = {Todorov, E. and Ghahramani, Z.},
doi = {10.1109/iembs.2004.1404285},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Todorov, Ghahramani - 2005 - Analysis of the synergies underlying complex hand manipulation.pdf:pdf},
isbn = {0-7803-8439-3},
journal = {The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
keywords = {hand manipulation,pca,synergy},
pages = {4637--4640},
publisher = {IEEE},
title = {{Analysis of the synergies underlying complex hand manipulation}},
volume = {2},
year = {2005}
}
@article{Cheung2001,
abstract = {We combine two near-infrared diffuse optical techniques to study variations of blood flow, haemoglobin concentration, and blood oxygen saturation in the functioning rat brain. Diffuse correlation spectroscopy (or flowmetry) monitors changes in the cerebral blood flow, without the use of the principles of tracer clearance, by measuring the optical phase-shifts caused by moving blood cells. Near-infrared absorption spectroscopy concurrently measures tissue absorption at two wavelengths to determine haemoglobin concentration and blood oxygen saturation in this same tissue volume. This optical probe is non-invasive and was employed through the intact skull. The utility of the technique is demonstrated in vivo by measuring the temporal changes in the regional vascular dynamics of rat brain during hypercapnia. Temporal and spatial variations of cerebral blood flow, haemoglobin concentration and blood oxygen saturation during hypercapnia are compared with other measurements in the literature, and a quantitative analysis demonstrating the self-consistency of our combined observations of vascular response is presented.},
author = {Cheung, Cecil and Culver, Joseph P. and Takahashi, Kasushi and Greenberg, Joel H. and Yodh, A. G.},
doi = {10.1088/0031-9155/46/8/302},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cheung et al. - 2001 - In vivo cerebrovascular measurement combining diffuse near-infrared absorption and correlation spectroscopies.pdf:pdf},
issn = {00319155},
journal = {Physics in Medicine and Biology},
number = {8},
pages = {2053--2065},
title = {{In vivo cerebrovascular measurement combining diffuse near-infrared absorption and correlation spectroscopies}},
volume = {46},
year = {2001}
}
@article{Leimkuhler2018,
abstract = {We describe parallel Markov chain Monte Carlo methods that propagate a collective ensemble of paths, with local covariance information calculated from neighboring replicas. The use of collective dynamics eliminates multiplicative noise and stabilizes the dynamics thus providing a practical approach to difficult anisotropic sampling problems in high dimensions. Numerical experiments with model problems demonstrate that dramatic potential speedups, compared to various alternative schemes, are attainable.},
author = {Leimkuhler, Benedict and Matthews, Charles and Weare, Jonathan},
doi = {10.1007/s11222-017-9730-1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Leimkuhler, Matthews, Weare - 2018 - Ensemble preconditioning for Markov chain Monte Carlo simulation.pdf:pdf},
isbn = {1122201797301},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {BFGS,Brownian dynamics,Computational statistics,Langevin methods,MCMC,Machine learning,Markov chain Monte Carlo,Stochastic sampling},
number = {2},
pages = {277--290},
publisher = {Springer US},
title = {{Ensemble preconditioning for Markov chain Monte Carlo simulation}},
volume = {28},
year = {2018}
}
@article{Ranganath2013,
abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
archivePrefix = {arXiv},
arxivId = {1401.0118},
author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
eprint = {1401.0118},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ranganath, Gerrish, Blei - 2013 - Black Box Variational Inference.pdf:pdf},
title = {{Black Box Variational Inference}},
url = {http://arxiv.org/abs/1401.0118},
year = {2013}
}
@article{Minervini2018,
abstract = {Neural models combining representation learning and reasoning in an end-to-end trainable manner are receiving increasing interest. However, their use is severely limited by their computational complexity, which renders them unusable on real world datasets. We focus on the Neural Theorem Prover (NTP) model proposed by Rockt{\{}$\backslash$"{\{}a{\}}{\}}schel and Riedel (2017), a continuous relaxation of the Prolog backward chaining algorithm where unification between terms is replaced by the similarity between their embedding representations. For answering a given query, this model needs to consider all possible proof paths, and then aggregate results - this quickly becomes infeasible even for small Knowledge Bases (KBs). We observe that we can accurately approximate the inference process in this model by considering only proof paths associated with the highest proof scores. This enables inference and learning on previously impracticable KBs.},
archivePrefix = {arXiv},
arxivId = {1807.08204},
author = {Minervini, Pasquale and Bosnjak, Matko and Rockt{\"{a}}schel, Tim and Riedel, Sebastian},
eprint = {1807.08204},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Minervini et al. - 2018 - Towards Neural Theorem Proving at Scale.pdf:pdf},
journal = {ICML},
month = {jul},
title = {{Towards Neural Theorem Proving at Scale}},
url = {http://arxiv.org/abs/1807.08204},
volume = {2},
year = {2018}
}
@article{Baskiotis2007,
abstract = {Motivated by realtime website optimization, this paper is about on-line learning in abruptly changing environments. Two extensions of the UCBT algorithm are combined in order to handle dynamic multi-armed bandits, and specifically to cope with fast variations in the rewards. Firstly, a change point detection test based on Page-Hinkley statistics is used to overcome the limitations due to the UCBT inertia. Secondly, a controlled forgetting strategy dubbed Meta-Bandit is proposed to take care of the Exploration vs Exploitation trade-off when the PH test is triggered. Extensive empirical validation shows significant improvements compared to the baseline algorithms. The paper also investigates the sensitivity of the proposed algorithm with respect to the number of available options.},
author = {Baskiotis, Nicolas and Gelly, Sylvain and Teytaud, Olivier},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Baskiotis, Gelly, Teytaud - 2007 - Change Point Detection and Meta-Bandits for Online Learning in Dynamic Environments.pdf:pdf},
journal = {CAp},
keywords = {dynamic environments,meta bandits,online learning,ucb},
pages = {237--250},
title = {{Change Point Detection and Meta-Bandits for Online Learning in Dynamic Environments}},
year = {2007}
}
@article{Meeds2014,
abstract = {Scientists often express their understanding of the world through a computationally demanding simulation program. Analyzing the posterior distribution of the parameters given observations (the inverse problem) can be extremely challenging. The Approximate Bayesian Computation (ABC) framework is the standard statistical tool to handle these likelihood free problems, but they require a very large number of simulations. In this work we develop two new ABC sampling algorithms that significantly reduce the number of simulations necessary for posterior inference. Both algorithms use confidence estimates for the accept probability in the Metropolis Hastings step to adaptively choose the number of necessary simulations. Our GPS-ABC algorithm stores the information obtained from every simulation in a Gaussian process which acts as a surrogate function for the simulated statistics. Experiments on a challenging realistic biological problem illustrate the potential of these algorithms.},
archivePrefix = {arXiv},
arxivId = {1401.2838},
author = {Meeds, Edward and Welling, Max},
eprint = {1401.2838},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Meeds, Welling - 2014 - GPS-ABC Gaussian Process Surrogate Approximate Bayesian Computation.pdf:pdf},
isbn = {9780974903910},
issn = {{\textless}null{\textgreater}},
pages = {1--17},
title = {{GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation}},
url = {http://arxiv.org/abs/1401.2838},
year = {2014}
}
@article{Whitehill2013,
abstract = {The ACT-R theory of cognition developed by John Anderson and colleagues endeavors to explain how humans recall chunks of information and how they solve problems. ACT-R also serves as a theoretical basis for "cognitive tutors", i.e., automatic tutoring systems that help students learn mathematics, computer programming, and other subjects. The official ACT-R definition is distributed across a large body of literature spanning many articles and monographs, and hence it is difficult for an "outsider" to learn the most important aspects of the theory. This paper aims to provide a tutorial to the core components of the ACT-R theory.},
archivePrefix = {arXiv},
arxivId = {1306.0125},
author = {Whitehill, Jacob},
eprint = {1306.0125},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Whitehill - 2013 - Understanding ACT-R - an Outsider's Perspective.pdf:pdf},
month = {jun},
title = {{Understanding ACT-R - an Outsider's Perspective}},
url = {http://arxiv.org/abs/1306.0125},
year = {2013}
}
@article{Roediger2017,
author = {Roediger, Henry L. and Tekin, Eylul and Uner, Oyku},
doi = {10.1016/b978-0-12-809324-5.21036-5},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Roediger, Tekin, Uner - 2017 - Encoding–Retrieval Interactions.pdf:pdf},
isbn = {9780128093245},
journal = {Learning and Memory: A Comprehensive Reference},
pages = {5--26},
title = {{Encoding–Retrieval Interactions}},
year = {2017}
}
@article{Cremer2018,
abstract = {Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.},
archivePrefix = {arXiv},
arxivId = {1801.03558},
author = {Cremer, Chris and Li, Xuechen and Duvenaud, David},
doi = {10.1016/j.accinf.2011.11.002},
eprint = {1801.03558},
isbn = {9781510867963},
issn = {1467-0895},
title = {{Inference Suboptimality in Variational Autoencoders}},
year = {2018}
}
@article{Graves2006,
abstract = {Many real-world sequence learning tasks re- quire the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their out- puts into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label un- segmented sequences directly, thereby solv- ing both problems. An experiment on the TIMIT speech corpus demonstrates its ad- vantages over both a baseline HMM and a hybrid HMM-RNN.},
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Graves, Alex and Fernandez, Santiago and Gomez, Faustino and Schmidhuber, Jurgen},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Graves et al. - 2006 - Connectionist Temporal Classification Labelling Unsegmented Sequence Data with Recurrent Neural Networks.pdf:pdf},
isbn = {1595933832},
issn = {10987576},
journal = {Proceedings of the 23rd international conference on Machine Learning},
pages = {369--376},
pmid = {1000285842},
title = {{Connectionist Temporal Classification : Labelling Unsegmented Sequence Data with Recurrent Neural Networks}},
year = {2006}
}
@inproceedings{Balandat2020,
abstract = {Bayesian optimization provides sample-efficient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BoTorch, a modern programming framework for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BoTorch's modular design facilitates flexible specification and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel "one-shot" formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efficiency of BoTorch relative to other popular libraries.},
archivePrefix = {arXiv},
arxivId = {1910.06403},
author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
booktitle = {Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
eprint = {1910.06403},
file = {:Users/mshvarts/Downloads/1910.06403.pdf:pdf},
title = {{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},
url = {http://arxiv.org/abs/1910.06403},
year = {2020}
}
@article{Wilk2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.01894v1},
author = {Wilk, Mark Van Der and Rasmussen, Carl Edward and Hensman, James},
eprint = {arXiv:1709.01894v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilk, Rasmussen, Hensman - 2017 - Convolutional Gaussian Processes.pdf:pdf},
number = {Nips},
title = {{Convolutional Gaussian Processes}},
year = {2017}
}
@article{Pakman2018,
abstract = {Many data generating processes involve latent random variables over discrete combinatorial spaces whose size grows factorially with the dataset. In these settings, existing posterior inference methods can be inaccurate and/or very slow. In this work we develop methods for efficient amortized approximate Bayesian inference over discrete combinatorial spaces, with applications to random permutations, probabilistic clustering (such as Dirichlet process mixture models) and random communities (such as stochastic block models). The approach is based on mapping distributed, symmetry-invariant representations of discrete arrangements into conditional probabilities. The resulting algorithms parallelize easily, yield iid samples from the approximate posteriors, and can easily be applied to both conjugate and non-conjugate models, as training only requires samples from the generative model.},
archivePrefix = {arXiv},
arxivId = {1901.00409},
author = {Pakman, Ari and Paninski, Liam},
eprint = {1901.00409},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pakman, Paninski - 2018 - Discrete Neural Processes.pdf:pdf},
title = {{Discrete Neural Processes}},
url = {http://arxiv.org/abs/1901.00409},
year = {2018}
}
@article{Korshunova2018,
abstract = {We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.},
archivePrefix = {arXiv},
arxivId = {1802.07535},
author = {Korshunova, Iryna and Degrave, Jonas and Husz{\'{a}}r, Ferenc and Gal, Yarin and Gretton, Arthur and Dambre, Joni},
eprint = {1802.07535},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Korshunova et al. - 2018 - BRUNO A Deep Recurrent Model for Exchangeable Data.pdf:pdf},
isbn = {085825204X},
issn = {03136922},
journal = {National Conference Publication - Institution of Engineers, Australia},
month = {feb},
number = {84-6},
pages = {78--82},
title = {{BRUNO: A Deep Recurrent Model for Exchangeable Data}},
url = {http://arxiv.org/abs/1802.07535},
year = {2018}
}
@article{Mandt2016,
abstract = {Stochastic Gradient Descent (SGD) is an important algorithm in machine learning. With constant learning rates, it is a stochastic process that, after an initial phase of convergence, generates samples from a stationary distribution. We show that SGD with constant rates can be effectively used as an approximate posterior inference algorithm for probabilistic modeling. Specifically, we show how to adjust the tuning parameters of SGD such as to match the resulting stationary distribution to the posterior. This analysis rests on interpreting SGD as a continuous-time stochastic process and then minimizing the Kullback-Leibler divergence between its stationary distribution and the target posterior. (This is in the spirit of variational inference.) In more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then use properties of this process to derive the optimal parameters. This theoretical framework also connects SGD to modern scalable inference algorithms; we analyze the recently proposed stochastic gradient Fisher scoring under this perspective. We demonstrate that SGD with properly chosen constant rates gives a new way to optimize hyperparameters in probabilistic models.},
archivePrefix = {arXiv},
arxivId = {1602.02666},
author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
eprint = {1602.02666},
isbn = {1602.02666v1},
title = {{A Variational Analysis of Stochastic Gradient Algorithms}},
year = {2016}
}
@article{Bonini,
abstract = {Reinforcement Learning (RL) is a successful technique to train autonomous agents. However, the classical RL methods take a long time to learn how to solve tasks. Option-based solutions can be used to accelerate learning and transfer learned behaviors across tasks by encapsulating a partial policy into an action. However, the literature report only single-agent and single-objective option-based methods, but many RL tasks, especially real-world problems, are better described through multiple objectives. We here propose a method to learn options in Multiobjective Reinforcement Learning domains in order to accelerate learning and reuse knowledge across tasks. Our initial experiments in the Goldmine Domain shows that our proposal learn useful options that accelerate learning in multiobjective domains. Our next step is to use the learned options to transfer knowledge across tasks.},
author = {Bonini, Rodrigo Cesar and Leno, Felipe and Helena, Anna and Costa, Reali and Prof, Av and Gualberto, Luciano and El, Engenharia},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bonini et al. - 2017 - Learning Options in Multiobjective Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 31th Conference on Artificial Intelligence (AAAI 2017)},
keywords = {Student Abstracts},
number = {January},
pages = {4907--4908},
title = {{Learning Options in Multiobjective Reinforcement Learning}},
year = {2017}
}
@article{Cohen2017,
abstract = {The driving force behind convolutional networks - the most successful deep learning architecture to date, is their expressive power. Despite its wide acceptance and vast empirical evidence, formal analyses supporting this belief are scarce. The primary notions for formally reasoning about expressiveness are efficiency and inductive bias. Expressive efficiency refers to the ability of a network architecture to realize functions that require an alternative architecture to be much larger. Inductive bias refers to the prioritization of some functions over others given prior knowledge regarding a task at hand. In this paper we overview a series of works written by the authors, that through an equivalence to hierarchical tensor decompositions, analyze the expressive efficiency and inductive bias of various convolutional network architectural features (depth, width, strides and more). The results presented shed light on the demonstrated effectiveness of convolutional networks, and in addition, provide new tools for network design.},
archivePrefix = {arXiv},
arxivId = {1705.02302},
author = {Cohen, Nadav and Sharir, Or and Levine, Yoav and Tamari, Ronen and Yakira, David and Shashua, Amnon},
eprint = {1705.02302},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cohen et al. - 2017 - Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions.pdf:pdf},
keywords = {convolutional networks,expressiveness,hierarchical tensor decompositions},
number = {2015},
pages = {1--20},
title = {{Analysis and Design of Convolutional Networks via Hierarchical Tensor Decompositions}},
url = {http://arxiv.org/abs/1705.02302},
year = {2017}
}
@article{Eliashberg2018,
author = {Eliashberg, Yakov},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Eliashberg - 2018 - Multilinear algebra, differential forms and Stokes' theorem.pdf:pdf},
number = {April},
title = {{Multilinear algebra, differential forms and Stokes' theorem}},
year = {2018}
}
@article{Salvucci2000,
abstract = {The process of fixation identification-separating and labeling fixations and saccades in eye-tracking protocols-is an essential part of eye-movement data analysis and can have a dramatic impact on higher-level analyses. However, algorithms for performing fixation identification are often described informally and rarely compared in a meaningful way. In this paper we propose a taxonomy of fixation identification algorithms that classifies algorithms in terms of how they utilize spatial and temporal information in eye-tracking protocols. Using this taxonomy, we describe five algorithms that are representative of different classes in the taxonomy and are based on commonly employed techniques. We then evaluate and compare these algorithms with respect to a number of qualitative characteristics. The results of these comparisons offer interesting implications for the use of the various algorithms in future work.},
author = {Salvucci, Dario D. and Goldberg, Joseph H.},
doi = {10.1145/355017.355028},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salvucci, Goldberg - 2000 - Identifying fixations and saccades in eye-tracking protocols.pdf:pdf},
isbn = {1581132808},
journal = {Proceedings of the Eye Tracking Research and Applications Symposium 2000},
keywords = {Data analysis algorithms,Eye tracking,Fixation identification},
pages = {71--78},
title = {{Identifying fixations and saccades in eye-tracking protocols}},
year = {2000}
}
@unpublished{Settles2009,
author = {Settles, Burr},
file = {:Users/mshvarts/Downloads/TR1648.pdf:pdf},
institution = {University of Wisconsin–Madison},
keywords = {active learning,literature surv,machine learning},
number = {Technical Report {\#}1648},
title = {{Active Learning Literature Survey}},
year = {2009}
}
@article{Lin2017,
abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that {\$}n{\$} variables cannot be multiplied using fewer than 2{\^{}}n neurons in a single hidden layer.},
archivePrefix = {arXiv},
arxivId = {1608.08225},
author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
doi = {10.1007/s10955-017-1836-5},
eprint = {1608.08225},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Tegmark, Rolnick - 2017 - Why Does Deep and Cheap Learning Work So Well.pdf:pdf},
isbn = {9781627480031},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Artificial neural networks,Deep learning,Statistical physics},
number = {6},
pages = {1223--1247},
pmid = {25246403},
title = {{Why Does Deep and Cheap Learning Work So Well?}},
volume = {168},
year = {2017}
}
@article{Laird2009,
abstract = {Dotar um agente inteligente com uma mem{\'{o}}ria epis{\'{o}}dica proporciona conhecimento com um valor inestim{\'{a}}vel para agir no presente, bem como o apoio a uma vasta gama de capacidades cognitivas. No entanto, apresenta importantes desafios te{\'{o}}ricos e pr{\'{a}}ticos ao proporcionar recupera{\c{c}}{\~{o}}es de epis{\'{o}}dios mais r{\'{a}}pidas ao longo do ciclo de vida do agente. Neste trabalho, propomos o desenvolvimento de sistemas de mem{\'{o}}ria epis{\'{o}}dica que pode armazenar e recuperar eficientemente experi{\^{e}}ncias ao longo de um ano.},
author = {Laird, John E. and Derbinsky, Nate},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Laird, Derbinsky - 2009 - A Year of Episodic Memory.pdf:pdf},
journal = {Proceedings of the Workshop on Grand Challenges for Reasoning from Experiences, IJCAI},
pages = {7--10},
title = {{A Year of Episodic Memory}},
year = {2009}
}
@article{Heinonen,
abstract = {We propose non-stationary spectral kernels for Gaussian process regression. We propose to model the spectral density of a non-stationary kernel function as a mixture of input-dependent Gaussian process frequency density surfaces. We solve the generalised Fourier transform with such a model, and present a family of non-stationary and non-monotonic kernels that can learn input-dependent and potentially long-range, non-monotonic covariances between inputs. We derive efficient inference using model whitening and marginalized posterior, and show with case studies that these kernels are necessary when modelling even rather simple time series, image or geospatial data with non-stationary characteristics.},
archivePrefix = {arXiv},
arxivId = {1705.08736},
author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},
eprint = {1705.08736},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Remes, Heinonen, Kaski - 2017 - Non-Stationary Spectral Kernels.pdf:pdf},
issn = {10495258},
title = {{Non-Stationary Spectral Kernels}},
url = {http://arxiv.org/abs/1705.08736},
year = {2017}
}
@article{Chow2018,
abstract = {In many real-world reinforcement learning (RL) problems, besides optimizing the main objective function, an agent must concurrently avoid violating a number of constraints. In particular, besides optimizing performance it is crucial to guarantee the safety of an agent during training as well as deployment (e.g. a robot should avoid taking actions - exploratory or not - which irrevocably harm its hardware). To incorporate safety in RL, we derive algorithms under the framework of constrained Markov decision problems (CMDPs), an extension of the standard Markov decision problems (MDPs) augmented with constraints on expected cumulative costs. Our approach hinges on a novel $\backslash$emph{\{}Lyapunov{\}} method. We define and present a method for constructing Lyapunov functions, which provide an effective way to guarantee the global safety of a behavior policy during training via a set of local, linear constraints. Leveraging these theoretical underpinnings, we show how to use the Lyapunov approach to systematically transform dynamic programming (DP) and RL algorithms into their safe counterparts. To illustrate their effectiveness, we evaluate these algorithms in several CMDP planning and decision-making tasks on a safety benchmark domain. Our results show that our proposed method significantly outperforms existing baselines in balancing constraint satisfaction and performance.},
archivePrefix = {arXiv},
arxivId = {1805.07708},
author = {Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
eprint = {1805.07708},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chow et al. - 2018 - A Lyapunov-based Approach to Safe Reinforcement Learning.pdf:pdf},
number = {NeurIPS},
pages = {1--10},
title = {{A Lyapunov-based Approach to Safe Reinforcement Learning}},
url = {http://arxiv.org/abs/1805.07708},
year = {2018}
}
@article{Franklin2019,
abstract = {Humans spontaneously organize a continuous experience into discrete events and use the learned structure of these events to generalize and organize memory. We introduce the Structured Event Memory (SEM) model of event cognition, which accounts for human abilities in event segmentation, memory, and generalization. SEM is derived from a probabilistic generative model of event dynamics defined over structured symbolic scenes. By embedding symbolic scene representations in a vector space and parametrizing the scene dynamics in this continuous space, SEM combines the advantages of structured and neural network approaches to high-level cognition. Using probabilistic reasoning over this generative model, SEM can infer event boundaries, learn event schemata, and use event knowledge to reconstruct past experience. We show that SEM can scale up to high-dimensional input spaces, producing human-like event segmentation for naturalistic video data, and accounts for a wide array of memory phenomena.},
author = {Franklin, Nicholas and Norman, Kenneth A. and Ranganath, Charan and Zacks, Jeffrey M. and Gershman, Samuel J.},
doi = {10.1101/541607},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Franklin et al. - 2019 - Structured event memory a neuro-symbolic model of event cognition.pdf:pdf},
journal = {bioRxiv},
pages = {541607},
title = {{Structured event memory: a neuro-symbolic model of event cognition}},
url = {https://www.biorxiv.org/content/10.1101/541607v1.abstract},
year = {2019}
}
@article{Rakitsch2013,
abstract = {Multi-task prediction methods are widely used to couple regressors or classifica- tion models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efficient parameter inference and out of sample prediction are feasible. On both synthetic examples and applica- tions to phenotype prediction in genetics, we find substantial benefits of modeling structured noise compared to established alternatives. 1},
author = {Rakitsch, Barbara and Lippert, Christoph and Borgwardt, Karsten and Stegle, Oliver},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rakitsch et al. - 2013 - It is all in the noise Efficient multi-task Gaussian process inference with structured residuals.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1466--1474},
title = {{It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals}},
url = {http://papers.nips.cc/paper/5089-it-is-all-in-the-noise-efficient-multi-task-gaussian-process-inference-with-structured-residuals},
year = {2013}
}
@article{Lee2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.00165v3},
author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-dickstein, Jascha},
eprint = {arXiv:1711.00165v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2018 - D n n g p.pdf:pdf},
title = {{D n n g p}},
year = {2018}
}
@article{Kandasamy2018,
abstract = {We design and analyse variations of the classical Thompson sampling (TS) procedure for Bayesian optimisation (BO) in settings where function evaluations are expensive, but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making {\$}n{\$} evaluations distributed among {\$}M{\$} workers is essentially equivalent to performing {\$}n{\$} evaluations in sequence. Further, by modeling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronously parallel TS achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous TS outperforms a suite of existing parallel BO algorithms in simulations and in a hyper-parameter tuning application in convolutional neural networks. In addition to these, the proposed procedure is conceptually and computationally much simpler than existing work for parallel BO.},
author = {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Schneider, Jeff and P{\'{o}}czos, Barnab{\'{a}}s},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kandasamy et al. - 2018 - Parallelised bayesian optimisation via thompson sampling.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics, AISTATS 2018},
pages = {133--142},
title = {{Parallelised bayesian optimisation via thompson sampling}},
volume = {84},
year = {2018}
}
@article{Guo2017,
abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
archivePrefix = {arXiv},
arxivId = {1706.04599},
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
doi = {10.1007/s001320050185},
eprint = {1706.04599},
isbn = {9781510855144},
issn = {01956574},
pmid = {124274},
title = {{On Calibration of Modern Neural Networks}},
year = {2017}
}
@article{Bauer2016,
abstract = {Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.},
archivePrefix = {arXiv},
arxivId = {1606.04820},
author = {Bauer, Matthias and van der Wilk, Mark and Rasmussen, Carl Edward},
eprint = {1606.04820},
issn = {10495258},
title = {{Understanding Probabilistic Sparse Gaussian Process Approximations}},
year = {2016}
}
@article{Wu2018,
abstract = {Time series data analytics has been a problem of substantial interests for decades, and Dynamic Time Warping (DTW) has been the most widely adopted technique to measure dissimilarity between time series. A number of global-alignment kernels have since been proposed in the spirit of DTW to extend its use to kernel-based estimation method such as support vector machine. However, those kernels suffer from diagonal dominance of the Gram matrix and a quadratic complexity w.r.t. the sample size. In this work, we study a family of alignment-aware positive definite (p.d.) kernels, with its feature embedding given by a distribution of $\backslash$emph{\{}Random Warping Series (RWS){\}}. The proposed kernel does not suffer from the issue of diagonal dominance while naturally enjoys a $\backslash$emph{\{}Random Features{\}} (RF) approximation, which reduces the computational complexity of existing DTW-based techniques from quadratic to linear in terms of both the number and the length of time-series. We also study the convergence of the RF approximation for the domain of time series of unbounded length. Our extensive experiments on 16 benchmark datasets demonstrate that RWS outperforms or matches state-of-the-art classification and clustering methods in both accuracy and computational time. Our code and data is available at {\{} $\backslash$url{\{}https://github.com/IBM/RandomWarpingSeries{\}}{\}}.},
archivePrefix = {arXiv},
arxivId = {1809.05259},
author = {Wu, Lingfei and Yen, Ian En-Hsu and Yi, Jinfeng and Xu, Fangli and Lei, Qi and Witbrock, Michael},
eprint = {1809.05259},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2018 - Random Warping Series A Random Features Method for Time-Series Embedding.pdf:pdf},
title = {{Random Warping Series: A Random Features Method for Time-Series Embedding}},
url = {http://arxiv.org/abs/1809.05259},
volume = {84},
year = {2018}
}
@inproceedings{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, I and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
eprint = {1406.2661},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - 2014 - Generative adversarial nets.pdf:pdf},
pages = {1--9},
title = {{Generative adversarial nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets},
year = {2014}
}
@article{Virta2017,
abstract = {The blind source separation model for multivariate time series generally assumes that the observed series is a linear transformation of an unobserved series with temporally uncorrelated or independent components. Given the observations, the objective is to find a linear transformation that recovers the latent series. Several methods for accomplishing this exist and three particular ones are the classic SOBI and the recently proposed generalized FOBI (gFOBI) and generalized JADE (gJADE), each based on the use of joint lagged moments. In this paper we generalize the methodologies behind these algorithms for tensor-valued time series. We assume that our data consists of a tensor observed at each time point and that the observations are linear transformations of latent tensors we wish to estimate. The tensorial generalizations are shown to have particularly elegant forms and we show that each of them is Fisher consistent and orthogonal equivariant. Comparing the new methods with the original ones in various settings shows that the tensorial extensions are superior to both their vector-valued counterparts and to two existing tensorial dimension reduction methods for i.i.d. data. Finally, applications to fMRI-data and video processing show that the methods are capable of extracting relevant information from noisy high-dimensional data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1703.10381v1},
author = {Virta, J. and Nordhausen, K.},
doi = {10.1109/MLSP.2017.8168122},
eprint = {arXiv:1703.10381v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Virta, Nordhausen - 2017 - Blind source separation for nonstationary tensor-valued time series.pdf:pdf},
isbn = {9781509063413},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Block stationary,Joint diagonalization,Smooth variance functions},
pages = {1--6},
title = {{Blind source separation for nonstationary tensor-valued time series}},
volume = {2017-Septe},
year = {2017}
}
@article{Shvartsman2014,
abstract = {This dissertation pursues a computationally rational analysis of eye move- ments in a simple list-reading task. The strength of the computationally rational approach is in the ability to explain why certain phenomena may emerge under the assumption that behavior is an approximately optimal adaptation to the joint constraints of an organism's intrinsic computational constraints and task demands. The provided theory and model integrates a framework of lexical processing as active perception (Norris, 2006) with ocu- lomotor constraints derived from a broad-coverage model of eye movement control in reading (Reichle, Warren {\&} McConnell 2009). The ?rst portion of the thesis provides experimental evidence of adaptation of ?xation durations to quantitatively-expressed payo?s in a simple reading task, and adaptation in the model on the same dimension. The second portion explores spillover lexical frequency e?ects in the same framework and how they may emerge from a model that can adaptively allocate processing resources to informa- tion drawn from perception (foveal or parafoveal), or memory. In addition to implications for eye movement control in reading, these ?ndings can be interpreted to bear on task adaptation in reading, as well as the adaptive use of perception and memory in a sequential sampling framework},
author = {Shvartsman, Michael},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shvartsman - 2014 - Adaptive Eye Movement Control in a Simple Linguistic Task.pdf:pdf},
pages = {151},
title = {{Adaptive Eye Movement Control in a Simple Linguistic Task}},
year = {2014}
}
@article{Havasi2018,
abstract = {Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method, Variational Inference (VI), employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work, we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters, we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.},
archivePrefix = {arXiv},
arxivId = {1806.05490},
author = {Havasi, Marton and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Murillo-Fuentes, Juan Jos{\'{e}}},
eprint = {1806.05490},
title = {{Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo}},
year = {2018}
}
@article{Swersky2013,
abstract = {Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter settings more efficiently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method significantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.},
author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Swersky, Snoek, Adams - 2013 - Multi-task Bayesian optimization.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Multi-task Bayesian optimization}},
year = {2013}
}
@article{Morey2008,
author = {Morey, Richard D.},
doi = {10.20982/tqmp.04.2.p061},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Morey - 2008 - Confidence Intervals from Normalized Data A correction to Cousineau (2005).pdf:pdf},
issn = {1913-4126},
journal = {Tutorials in Quantitative Methods for Psychology},
keywords = {Alopecia,Diet,Micro-nutriments,Vitamins},
month = {sep},
number = {2},
pages = {61--64},
title = {{Confidence Intervals from Normalized Data: A correction to Cousineau (2005)}},
url = {http://www.tqmp.org/RegularArticles/vol04-2/p061},
volume = {4},
year = {2008}
}
@article{Choudhuri2007,
abstract = {The article describes a nonparametric Bayesian approach to estimating the regression function for binary response data measured with multiple covariates. A multiparameter Gaussian process, after some transformation, is used as a prior on the regression function. Such a prior does not require any assumptions like monotonicity or additivity of the covariate effects. However, additivity, if desired, may be imposed through the selection of appropriate parameters of the prior. By introducing some latent variables, the conditional distributions in the posterior may be shown to be conjugate, and thus an efficient Gibbs sampler to compute the posterior distribution may be developed. A hierarchical scheme to construct a prior around a parametric family is described. A robustification technique to protect the resulting Bayes estimator against miscoded observations is also designed. A detailed simulation study is conducted to investigate the performance of the proposed methods. We also analyze some real data using the methods developed in this article. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {math/0702686},
author = {Choudhuri, Nidhan and Ghosal, Subhashis and Roy, Anindya},
doi = {10.1016/j.stamet.2006.07.003},
eprint = {0702686},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Choudhuri, Ghosal, Roy - 2007 - Nonparametric binary regression using a Gaussian process prior.pdf:pdf},
issn = {15723127},
journal = {Statistical Methodology},
keywords = {Gibbs sampler,Latent variable,Link function,Response probability,Robustification},
number = {2},
pages = {227--243},
primaryClass = {math},
title = {{Nonparametric binary regression using a Gaussian process prior}},
volume = {4},
year = {2007}
}
@article{Huang2018,
abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time, via Inverse Autoregressive Flows (IAF). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.},
archivePrefix = {arXiv},
arxivId = {1804.00779},
author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
doi = {10.1017/S0014479713000288},
eprint = {1804.00779},
isbn = {1520-5126 (Electronic)$\backslash$r0002-7863 (Linking)},
issn = {0014-4797},
pmid = {20845070},
title = {{Neural Autoregressive Flows}},
year = {2018}
}
@article{Rezende2014,
abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
archivePrefix = {arXiv},
arxivId = {1401.4082},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
doi = {10.1051/0004-6361/201527329},
eprint = {1401.4082},
isbn = {9781634393973},
issn = {10495258},
pmid = {23459267},
title = {{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},
year = {2014}
}
@article{Tran2017a,
abstract = {Variational Bayes (VB) is rapidly becoming a popular tool for Bayesian inference in statistical modeling. However, the existing VB algorithms are restricted to cases where the likelihood is tractable, which precludes the use of VB in many interesting situations such as in state space models and in approximate Bayesian computation (ABC), where application of VB methods was previously impossible. This paper extends the scope of application of VB to cases where the likelihood is intractable, but can be estimated unbiasedly. The proposed VB method therefore makes it possible to carry out Bayesian inference in many statistical applications, including state space models and ABC. The method is generic in the sense that it can be applied to almost all statistical models without requiring too much model-based derivation, which is a drawback of many existing VB algorithms. We also show how the proposed method can be used to obtain highly accurate VB approximations of marginal posterior distributions.},
archivePrefix = {arXiv},
arxivId = {1503.08621},
author = {Tran, Minh Ngoc and Nott, David J. and Kohn, Robert},
doi = {10.1080/10618600.2017.1330205},
eprint = {1503.08621},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
number = {4},
title = {{Variational Bayes With Intractable Likelihood}},
volume = {26},
year = {2017}
}
@article{Ros2015,
abstract = {In this paper we introduce a covariance framework for the analysis of single subject EEG and MEG data that takes into account observed temporal stationarity on small time scales and trial-to-trial variations. We formulate a model for the covariance matrix, which is a Kronecker product of three components that correspond to space, time and epochs/trials, and consider maximum likelihood estimation of the unknown parameter values. An iterative algorithm that finds approximations of the maximum likelihood estimates is proposed. Our covariance model is applicable in a variety of cases where spontaneous EEG or MEG acts as source of noise and realistic noise covariance estimates are needed, such as in evoked activity studies, or where the properties of spontaneous EEG or MEG are themselves the topic of interest, like in combined EEG-fMRI experiments in which the correlation between EEG and fMRI signals is investigated. We use a simulation study to assess the performance of the estimator and investigate the influence of different assumptions about the covariance factors on the estimated covariance matrix and on its components. We apply our method to real EEG and MEG data sets.},
archivePrefix = {arXiv},
arxivId = {1410.2522},
author = {Ro{\'{s}}, Beata P. and Bijma, Fetsje and de Gunst, Mathisca C M and de Munck, Jan C.},
doi = {10.1016/j.neuroimage.2015.06.020},
eprint = {1410.2522},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ro{\'{s}} et al. - 2015 - A three domain covariance framework for EEGMEG data.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Covariance structure,EEG,Kronecker product structure,MEG,Maximum likelihood,fMRI},
pages = {305--315},
pmid = {26072253},
title = {{A three domain covariance framework for EEG/MEG data}},
volume = {119},
year = {2015}
}
@article{Chen2016,
abstract = {Identification of regions of interest (ROI) associated with certain disease has a great impact on public health. Imposing sparsity of pixel values and extracting active regions simultaneously greatly complicate the image analysis. We address these challenges by introducing a novel region-selection penalty in the framework of image-on-scalar regression. Our penalty combines the Smoothly Clipped Absolute Deviation (SCAD) regularization, enforcing sparsity, and the SCAD of total variation (TV) regularization, enforcing spatial contiguity, into one group, which segments contiguous spatial regions against zero-valued background. Efficient algorithm is based on the alternative direction method of multipliers (ADMM) which decomposes the non-convex problem into two iterative optimization problems with explicit solutions. Another virtue of the proposed method is that a divide and conquer learning algorithm is developed, thereby allowing scaling to large images. Several examples are presented and the experimental results are compared with other state-of-the-art approaches.},
archivePrefix = {arXiv},
arxivId = {1605.08501},
author = {Chen, Yao and Wang, Xiao and Kong, Linglong and Zhu, Hongtu},
eprint = {1605.08501},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2016 - Local Region Sparse Learning for Image-on-Scalar Regression.pdf:pdf},
pages = {1--10},
title = {{Local Region Sparse Learning for Image-on-Scalar Regression}},
url = {http://arxiv.org/abs/1605.08501},
year = {2016}
}
@article{Jacobsen2018,
abstract = {It is widely believed that the success of deep convolutional networks is based on progressively discarding uninformative variability about the input with respect to the problem at hand. This is supported empirically by the difficulty of recovering images from their hidden representations, in most commonly used network architectures. In this paper we show via a one-to-one mapping that this loss of information is not a necessary condition to learn representations that generalize well on complicated problems, such as ImageNet. Via a cascade of homeomorphic layers, we build the i-RevNet, a network that can be fully inverted up to the final projection onto the classes, i.e. no information is discarded. Building an invertible architecture is difficult, for one, because the local inversion is ill-conditioned, we overcome this by providing an explicit inverse. An analysis of i-RevNets learned representations suggests an alternative explanation for the success of deep networks by a progressive contraction and linear separation with depth. To shed light on the nature of the model learned by the i-RevNet we reconstruct linear interpolations between natural image representations.},
archivePrefix = {arXiv},
arxivId = {1802.07088},
author = {Jacobsen, J{\"{o}}rn-Henrik and Smeulders, Arnold and Oyallon, Edouard},
eprint = {1802.07088},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jacobsen, Smeulders, Oyallon - 2018 - i-RevNet Deep Invertible Networks.pdf:pdf},
pages = {1--11},
title = {{i-RevNet: Deep Invertible Networks}},
url = {http://arxiv.org/abs/1802.07088},
year = {2018}
}
@article{Blomqvist2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1810.03052v1},
author = {Blomqvist, Kenneth},
eprint = {arXiv:1810.03052v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Blomqvist - 2018 - Deep convolutional Gaussian processes.pdf:pdf},
title = {{Deep convolutional Gaussian processes}},
year = {2018}
}
@article{Gershman2014,
abstract = {{\textcopyright} 2014 Gershman et al. Psychophysical and neurophysiological studies have suggested that memory is not simply a carbon copy of our experience: Memories are modified or new memories are formed depending on the dynamic structure of our experience, and specifically, on how gradually or abruptly the world changes. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can qualitatively account for several psychophysical and neural phenomena, and present results of a new visual memory experiment aimed at testing the theory directly. Our experimental findings suggest that humans can use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective we offer provides a coherent account of the conditions under which new experience is integrated into an old memory versus forming a new memory, and shows that memory formation depends on inferences about the underlying structure of our experience.},
author = {Gershman, Samuel J. and Radulescu, Angela and Norman, Kenneth A. and Niv, Yael},
doi = {10.1371/journal.pcbi.1003939},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gershman et al. - 2014 - Statistical Computations Underlying the Dynamics of Memory Updating.pdf:pdf},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {11},
title = {{Statistical Computations Underlying the Dynamics of Memory Updating}},
volume = {10},
year = {2014}
}
@article{Louizos2017,
abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.},
archivePrefix = {arXiv},
arxivId = {1703.01961},
author = {Louizos, Christos and Welling, Max},
eprint = {1703.01961},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Multiplicative Normalizing Flows for Variational Bayesian Neural Networks}},
year = {2017}
}
@article{Savov2018,
author = {Savov, Vlad},
journal = {The Verge},
month = {may},
title = {{Google's Selfish Ledger is an unsettling vision of Silicon Valley social engineering}},
url = {https://www.theverge.com/2018/5/17/17344250/google-x-selfish-ledger-video-data-privacy},
year = {2018}
}
@article{Anandkumar2014,
abstract = {This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.},
archivePrefix = {arXiv},
arxivId = {1210.7559},
author = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
doi = {10.1007/978-3-319-24486-0_2},
eprint = {1210.7559},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Anandkumar et al. - 2014 - Tensor Decompositions for Learning Latent Variable Models.pdf:pdf},
isbn = {9783319244853},
issn = {16113349},
journal = {Journal of Machine Learning Research},
pages = {19--38},
title = {{Tensor Decompositions for Learning Latent Variable Models}},
volume = {9355},
year = {2014}
}
@misc{Meir-Hasson2014,
abstract = {This work introduces a general framework for producing an EEG Finger-Print (EFP) which can be used to predict specific brain activity as measured by fMRI at a given deep region. This new approach allows for improved EEG spatial resolution based on simultaneous fMRI activity measurements. Advanced signal processing and machine learning methods were applied on EEG data acquired simultaneously with fMRI during relaxation training guided by on-line continuous feedback on changing alpha/theta EEG measure. We focused on demonstrating improved EEG prediction of activation in sub-cortical regions such as the amygdala. Our analysis shows that a ridge regression model that is based on time/frequency representation of EEG data from a single electrode, can predict the amygdala related activity significantly better than a traditional theta/alpha activity sampled from the best electrode and about 1/3 of the times, significantly better than a linear combination of frequencies with a pre-defined delay. The far-reaching goal of our approach is to be able to reduce the need for fMRI scanning for probing specific sub-cortical regions such as the amygdala as the basis for brain-training procedures. On the other hand, activity in those regions can be characterized with higher temporal resolution than is obtained by fMRI alone thus revealing additional information about their processing mode.},
author = {Meir-Hasson, Yehudit and Kinreich, Sivan and Podlipsky, Ilana and Hendler, Talma and Intrator, Nathan},
booktitle = {NeuroImage},
doi = {10.1016/j.neuroimage.2013.11.004},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Meir-Hasson et al. - 2014 - An EEG Finger-Print of fMRI deep regional activation.pdf:pdf},
isbn = {1053-8119},
issn = {10959572},
keywords = {EEG Finger-Print,Ridge-regression,Simultaneous fMRI/EEG,Time/frequency},
number = {P1},
pages = {128--141},
pmid = {24246494},
title = {{An EEG Finger-Print of fMRI deep regional activation}},
volume = {102},
year = {2014}
}
@article{Gregor2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.0030v2},
author = {Gregor, Karol and Deepmind, Google and Com, Karolg Google},
eprint = {arXiv:1402.0030v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gregor, Deepmind, Com - 2014 - Neural Variational Inference and Learning in Belief Networks.pdf:pdf},
keywords = {variational inference, belief networks, deep learn},
title = {{Neural Variational Inference and Learning in Belief Networks}},
volume = {32},
year = {2014}
}
@article{Afshar2004,
abstract = { The neural-based control of a robotic hand has many clinical and engineering applications. Current approaches to this problem have been limited due to a lack of understanding of the relationship between neural signals and dynamic finger movements. Here, we present a technique to predict index finger joint angles from neural signals recorded from the associated muscles. The neural signals are converted to a torque estimate (EBTE) and then input to artificial neural networks. The networks predict the finger position more closely when the input to the networks are torque estimates rather than neural signals. Furthermore, the networks trained with the EBTE signals could predict the joint angles for different phases of finger movements (i.e. dynamic reaching and positioning task) while networks trained with the neural signals could not. Our results indicate that (1) similar finger movements are executed with different synergistic strategies and (2) different phases of finger movements employ different neural strategies. Through these results, we have demonstrated the first concrete technique to control a hand prosthetic device or dexterous tele-manipulator using natural neural control signals.},
author = {Afshar, P. and Matsuoka, Y.},
doi = {10.1109/robot.2004.1302448},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Afshar, Matsuoka - 2004 - Neural-based control of a robotic hand evidence for distinct muscle strategies.pdf:pdf},
isbn = {0780382323},
keywords = {-motor control,a group of motor,electromyography,joint angles,neurons,these movement vectors over},
number = {April},
pages = {4633--4638 Vol.5},
title = {{Neural-based control of a robotic hand: evidence for distinct muscle strategies}},
year = {2004}
}
@article{Kleine2007,
abstract = {Recently, high-density surface EMG electrode grids and multi-channel amplifiers became available for non-invasive recording of human motor units (MUs). We present a way to decompose surface EMG signals into MU firing patterns, whereby we concentrate on the importance of two-dimensional spatial differences between the MU action potentials (MUAPs). Our method is exemplified with high-density EMG data from the vastus lateralis muscle of a single subject. Bipolar and Laplacian spatial filtering was applied to the monopolar raw signals. From the single recording in this subject six different simultaneously active MUs could be distinguished using the spatial differences between MUAPs in the direction perpendicular to the muscle fiber direction. After spike-triggered averaging, 125-channel two-dimensional MUAP templates were obtained. Template-matching allowed tracking of all MU firings. The impact of spatial information was measured by using subsets of the MUAP templates, either in parallel or perpendicular to the muscle fiber direction. The use of one-dimensional spatial information perpendicular to the muscle fiber direction was superior to the use of a linear array electrode in the longitudinal direction. However, to detect the firing events of the MUs with a high accuracy, as needed for instance for estimation of firing synchrony, two-dimensional information from the complete grid electrode appears essential. {\textcopyright} 2006.},
author = {Kleine, Bert U. and van Dijk, Johannes P. and Lapatki, Bernd G. and Zwarts, Machiel J. and Stegeman, Dick F.},
doi = {10.1016/j.jelekin.2006.05.003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kleine et al. - 2007 - Using two-dimensional spatial information in decomposition of surface EMG signals.pdf:pdf},
issn = {10506411},
journal = {Journal of Electromyography and Kinesiology},
keywords = {Decomposition,High-density surface EMG,Linear array electrode,Motor neuron firing pattern,Motor unit action potential,Spatial filter},
number = {5},
pages = {535--548},
title = {{Using two-dimensional spatial information in decomposition of surface EMG signals}},
volume = {17},
year = {2007}
}
@article{Mei2017,
abstract = {Many events occur in the world. Some event types are stochastically excited or inhibited - in the sense of having their probabilities elevated or decreased - by patterns in the sequence of previous events. Discovering such patterns can help us predict which type of event will happen next and when. We model streams of discrete events in continuous time, by constructing a neurally self-modulating multivariate point process in which the intensities of multiple event types evolve according to a novel continuous-time LSTM. This generative model allows past events to influence the future in complex and realistic ways, by conditioning future event intensities on the hidden state of a recurrent neural network that has consumed the stream of past events. Our model has desirable qualitative properties. It achieves competitive likelihood and predictive accuracy on real and synthetic datasets, including under missing-data conditions.},
archivePrefix = {arXiv},
arxivId = {1612.09328},
author = {Mei, Hongyuan and Eisner, Jason},
eprint = {1612.09328},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mei, Eisner - 2017 - The neural Hawkes process A neurally self-modulating multivariate point process.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {6755--6765},
title = {{The neural Hawkes process: A neurally self-modulating multivariate point process}},
volume = {2017-Decem},
year = {2017}
}
@article{Radvansky2017,
abstract = {Research on event cognition is rapidly developing and is revealing fundamental aspects of human cognition. In this paper, we review recent and current work that is driving this field forward. We first outline the Event Horizon Model, which broadly describes the impact of event boundaries on cognition and memory. Then, we address recent work on event segmentation, the role of event cognition in working memory and long-term memory, including event model updating, and long term retention. Throughout we also consider how event cognition varies across individuals and groups of people and consider the neural mechanisms involved. Events are at the center of human experience, and event cognition is the study of how people perceive, conceive, talk about, and remember them [1]. A current focus of interest is how cognitive systems form and update representations of events, namely event models. The Event Horizon Model [1, 2, 3] provides a framework for how such representations are created, structured, and remembered. It consists of five principles, illustrated in Figure 1: (1) people segment an ongoing stream of activity into a succession of event models; (2) only the current event model is in working memory; (3) the dominant dimension organizing relations between event models in long term memory is the causal connectivity among elements; (4) people better remember information stored across multiple events in noncompetitive attribute retrieval; and (5) people have retrieval interference for information stored across multiple events. Here, we will highlight recent developments in event cognition in light of this framework. (For systematic reviews, see [1, 4].) The first principle states that people parse action into events. An account of how this is done is given by Event Segmentation Theory [5, 6, 7] which proposes that people's event comprehension systems form predictions about upcoming happenings based on the current event model. When important situation features change, such as new movements, spatial location, characters, objects, causes, and goals, then prediction error spikes. As a result, the},
author = {Radvansky, Gabriel A and Zacks, Jeffrey M and Opin, Curr and Author, Behav Sci},
doi = {10.1016/j.cobeha.2017.08.006},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Radvansky et al. - 2017 - Event Boundaries in Memory and Cognition HHS Public Access Author manuscript.pdf:pdf},
journal = {Curr Opin Behav Sci},
pages = {133--140},
title = {{Event Boundaries in Memory and Cognition HHS Public Access Author manuscript}},
volume = {17},
year = {2017}
}
@article{Gelman2014,
abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
archivePrefix = {arXiv},
arxivId = {1307.5928},
author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
doi = {10.1007/s11222-013-9416-2},
eprint = {1307.5928},
isbn = {0960-3174},
issn = {15731375},
journal = {Statistics and Computing},
number = {6},
title = {{Understanding predictive information criteria for Bayesian models}},
volume = {24},
year = {2014}
}
@article{Frigola2014,
abstract = {State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.},
archivePrefix = {arXiv},
arxivId = {1406.4905},
author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl E.},
eprint = {1406.4905},
issn = {10495258},
title = {{Variational Gaussian Process State-Space Models}},
year = {2014}
}
@article{Sirignano2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.01053v1},
author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
eprint = {arXiv:1805.01053v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sirignano, Spiliopoulos - 2018 - Mean Field Analysis of Neural Networks.pdf:pdf},
number = {Dms 1550918},
title = {{Mean Field Analysis of Neural Networks}},
year = {2018}
}
@article{Feurer2018,
abstract = {Bayesian optimization has become a standard technique for hyperparameter optimization, including data-intensive models such as deep neural networks that may take days or weeks to train. We consider the setting where previous optimization runs are available, and we wish to use their results to warm-start a new optimization run. We develop an ensemble model that can incorporate the results of past optimization runs, while avoiding the poor scaling that comes with putting all results into a single Gaussian process model. The ensemble combines models from past runs according to estimates of their generalization performance on the current optimization. Results from a large collection of hyperparameter optimization benchmark problems and from optimization of a production computer vision platform at Facebook show that the ensemble can substantially reduce the time it takes to obtain near-optimal configurations, and is useful for warm-starting expensive searches or running quick re-optimizations.},
archivePrefix = {arXiv},
arxivId = {1802.02219},
author = {Feurer, Matthias and Letham, Benjamin and Bakshy, Eytan},
eprint = {1802.02219},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Feurer, Letham, Bakshy - 2018 - Scalable Meta-Learning for Bayesian Optimization.pdf:pdf},
title = {{Scalable Meta-Learning for Bayesian Optimization}},
url = {http://arxiv.org/abs/1802.02219},
year = {2018}
}
@article{Yehudai2019,
abstract = {Recently, a spate of papers have provided positive theoretical results for training over-parameterized neural networks (where the network size is larger than what is needed to achieve low error). The key insight is that with sufficient over-parameterization, gradient-based methods will implicitly leave some components of the network relatively unchanged, so the optimization dynamics will behave as if those components are essentially fixed at their initial random values. In fact, fixing these explicitly leads to the well-known approach of learning with random features. In other words, these techniques imply that we can successfully learn with neural networks, whenever we can successfully learn with random features. In this paper, we first review these techniques, providing a simple and self-contained analysis for one-hidden-layer networks. We then argue that despite the impressive positive results, random feature approaches are also inherently limited in what they can explain. In particular, we rigorously show that random features cannot be used to learn even a single ReLU neuron with standard Gaussian inputs, unless the network size (or magnitude of the weights) is exponentially large. Since a single neuron is learnable with gradient-based methods, we conclude that we are still far from a satisfying general explanation for the empirical success of neural networks.},
archivePrefix = {arXiv},
arxivId = {1904.00687},
author = {Yehudai, Gilad and Shamir, Ohad},
eprint = {1904.00687},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yehudai, Shamir - 2019 - On the Power and Limitations of Random Features for Understanding Neural Networks.pdf:pdf},
month = {apr},
pages = {1--29},
title = {{On the Power and Limitations of Random Features for Understanding Neural Networks}},
url = {http://arxiv.org/abs/1904.00687},
year = {2019}
}
@misc{Bronstein2017a,
abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
author = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
booktitle = {IEEE Signal Processing Magazine},
doi = {10.1109/MSP.2017.2693418},
issn = {10535888},
number = {4},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
volume = {34},
year = {2017}
}
@misc{Jeong2015,
abstract = {Understanding human episodic memory in aspects of large-scale brain networks has become one of the central themes in neuroscience over the last decade. Traditionally, episodic memory was regarded as mostly relying on medial temporal lobe (MTL) structures. However, recent studies have suggested involvement of more widely distributed cortical network and the importance of its interactive roles in the memory process. Both direct and indirect neuro-modulations of the memory network have been tried in experimental treatments of memory disorders. In this review, we focus on the functional organization of the MTL and other neocortical areas in episodic memory. Task-related neuroimaging studies together with lesion studies suggested that specific sub-regions of the MTL are responsible for specific components of memory. However, recent studies have emphasized that connectivity within MTL structures and even their network dynamics with other cortical areas are essential in the memory process. Resting-state functional network studies also have revealed that memory function is subserved by not only the MTL system but also a distributed network, particularly the default-mode network (DMN). Furthermore, researchers have begun to investigate memory networks throughout the entire brain not restricted to the specific resting-state network (RSN). Altered patterns of functional connectivity (FC) among distributed brain regions were observed in patients with memory impairments. Recently, studies have shown that brain stimulation may impact memory through modulating functional networks, carrying future implications of a novel interventional therapy for memory impairment.},
author = {Jeong, Woorim and Chung, Chun Kee and Kim, June Sic},
booktitle = {Frontiers in Human Neuroscience},
doi = {10.3389/fnhum.2015.00454},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jeong, Chung, Kim - 2015 - Episodic memory in aspects of large-scale brain networks.pdf:pdf},
issn = {16625161},
keywords = {Brain stimulation,Episodic memory,Functional connectivity,Large-scale networks,Medial temporal lobe,Resting-state networks},
month = {aug},
number = {AUGUST},
publisher = {Frontiers Media S. A},
title = {{Episodic memory in aspects of large-scale brain networks}},
volume = {9},
year = {2015}
}
@article{DeCarlo2012,
abstract = {The standard signal detection theory (SDT) approach to m-alternative forced choice uses the proportion correct as the outcome variable and assumes that there is no response bias. The assumption of no bias is not made for theoretical reasons, but rather because it simplifies the model and estimation of its parameters. The SDT model for mAFC with bias is presented, with the cases of two, three, and four alternatives considered in detail. Two approaches to fitting the model are noted: maximum likelihood estimation with Gaussian quadrature and Bayesian estimation with Markov chain Monte Carlo. Both approaches are examined in simulations. SAS and OpenBUGS programs to fit the models are provided, and an application to real-world data is presented. {\textcopyright} 2012 Elsevier Inc..},
author = {DeCarlo, Lawrence T.},
doi = {10.1016/j.jmp.2012.02.004},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/DeCarlo - 2012 - On a signal detection approach to m-alternative forced choice with bias, with maximum likelihood and Bayesian approache.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Bayesian estimation,Bias,Forced choice,Gaussian quadrature,Markov chain Monte Carlo,Signal detection theory},
number = {3},
pages = {196--207},
publisher = {Elsevier Inc.},
title = {{On a signal detection approach to m-alternative forced choice with bias, with maximum likelihood and Bayesian approaches to estimation}},
url = {http://dx.doi.org/10.1016/j.jmp.2012.02.004},
volume = {56},
year = {2012}
}
@article{Shafto2014,
abstract = {Much of learning and reasoning occurs in pedagogical situations-situations in which a person who knows a concept chooses examples for the purpose of helping a learner acquire the concept. We introduce a model of teaching and learning in pedagogical settings that predicts which examples teachers should choose and what learners should infer given a teacher's examples. We present three experiments testing the model predictions for rule-based, prototype, and causally structured concepts. The model shows good quantitative and qualitative fits to the data across all three experiments, predicting novel qualitative phenomena in each case. We conclude by discussing implications for understanding concept learning and implications for theoretical claims about the role of pedagogy in human learning. {\textcopyright} 2014 Elsevier Inc.},
author = {Shafto, Patrick and Goodman, Noah D. and Griffiths, Thomas L.},
doi = {10.1016/j.cogpsych.2013.12.004},
isbn = {1095-5623},
issn = {00100285},
journal = {Cognitive Psychology},
pmid = {24607849},
title = {{A rational account of pedagogical reasoning: Teaching by, and learning from, examples}},
volume = {71},
year = {2014}
}
@article{Hsu2018,
author = {Hsu, Kuang-jui},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hsu - 2018 - Co-attention CNNs for Unsupervised Object Co-segmentation.pdf:pdf},
keywords = {Computer Vision: Computer Vision,Machine Learning: Deep Learning,Machine Learning: Unsupervised Learning},
pages = {748--756},
title = {{Co-attention CNNs for Unsupervised Object Co-segmentation}},
year = {2018}
}
@article{Souza2016,
abstract = {The concept of attention has a prominent place in cognitive psychology. Attention can be directed not only to perceptual information, but also to information in working memory (WM). Evidence for an internal focus of attention has come from the retro-cue effect: Performance in tests of visual WM is improved when attention is guided to the test-relevant contents of WM ahead of testing them. The retro-cue paradigm has served as a test bed to empirically investigate the functions and limits of the focus of attention in WM. In this article, we review the growing body of (behavioral) studies on the retro-cue effect. We evaluate the degrees of experimental support for six hypotheses about what causes the retro-cue effect: (1) Attention protects representations from decay, (2) attention prioritizes the selected WM contents for comparison with a probe display, (3) attended representations are strengthened in WM, (4) not-attended representations are removed from WM, (5) a retro-cue to the retrieval target provides a head start for its retrieval before decision making, and (6) attention protects the selected representation from perceptual interference. The extant evidence provides support for the last four of these hypotheses.},
author = {Souza, Alessandra S. and Oberauer, Klaus},
doi = {10.3758/s13414-016-1108-5},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Souza, Oberauer - 2016 - In search of the focus of attention in working memory 13 years of the retro-cue effect.pdf:pdf},
issn = {1943393X},
journal = {Attention, Perception, and Psychophysics},
keywords = {Attention,Retro-cue,Working memory},
number = {7},
pages = {1839--1860},
publisher = {Attention, Perception, {\&} Psychophysics},
title = {{In search of the focus of attention in working memory: 13 years of the retro-cue effect}},
url = {http://dx.doi.org/10.3758/s13414-016-1108-5},
volume = {78},
year = {2016}
}
@article{Greenwald2015,
abstract = {Greenwald, Poehlman, Uhlmann, and Banaji (2009; GPUB hereafter) reported an average predictive validity correlation of r .236 for Implicit Association Test (IAT) measures involving Black–White racial attitudes and stereotypes. Oswald, Mitchell, Blanton, Jaccard, and Tetlock (2013; OMBJT) reported a lower aggregate figure for correlations involving IAT measures (r .148). The difference between the estimates of the 2 reviews was due mostly to their use of different policies for including effect sizes. GPUB limited their study to findings that assessed theoretically expected attitude–behavior and stereotype–judgment correlations along with others that the authors expected to show positive correlations. OMBJT included a substantial minority of correlations for which there was no theoretical expectation of a predictive relationship. Regardless of inclusion policy, both meta-analyses estimated aggregate correlational effect sizes that were large enough to explain discriminatory impacts that are societally significant either because they can affect many people simultaneously or because they can repeatedly affect single persons. Keywords:},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Greenwald, Anthony G. and Banaji, Mahzarin R. and Nosek, Brian A.},
doi = {10.1037/pspa0000016},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Greenwald, Banaji, Nosek - 2015 - Statistically small effects of the Implicit Association Test can have societally large effects.pdf:pdf},
isbn = {0022-3514},
issn = {00223514},
journal = {Journal of Personality and Social Psychology},
keywords = {Effect size,Implicit Association Test,Meta-analysis,Predictive validity,Race discrimination},
number = {4},
pages = {553--561},
pmid = {25402677},
title = {{Statistically small effects of the Implicit Association Test can have societally large effects}},
volume = {108},
year = {2015}
}
@article{Fort2019,
abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable approximate Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. We demonstrate that while low-loss connectors between modes exist, they are not connected in the space of predictions. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods.},
archivePrefix = {arXiv},
arxivId = {1912.02757},
author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
eprint = {1912.02757},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Fort, Hu, Lakshminarayanan - 2019 - Deep Ensembles A Loss Landscape Perspective.pdf:pdf},
number = {2},
pages = {1--14},
title = {{Deep Ensembles: A Loss Landscape Perspective}},
url = {http://arxiv.org/abs/1912.02757},
year = {2019}
}
@article{Riihimaki2010,
abstract = {A method for using monotonicity information in multivariate Gaussian process regression and classification is proposed. Monotonicity information is introduced with virtual derivative observations, and the resulting posterior is approximated with expectation propagation. Behaviour of the method is illustrated with artificial regression examples, and the method is used in a real world health care classification problem to include monotonicity information with respect to one of the covariates. Copyright 2010 by the authors.},
author = {Riihim{\"{a}}ki, Jaakko and Vehtari, Aki},
file = {:Users/mshvarts/Downloads/riihimaki10a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {645--652},
title = {{Gaussian processes with monotonicity information}},
volume = {9},
year = {2010}
}
@inproceedings{Salem2018,
author = {Salem, Nermin M and Mahdi, Hani M. K. and Abbas, Hazem},
booktitle = {2018 13th International Conference on Computer Engineering and Systems (ICCES)},
doi = {10.1109/ICCES.2018.8639258},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salem, Mahdi, Abbas - 2018 - Semantic Image Inpainting Vsing Self-Learning Encoder-Decoder and Adversarial Loss.pdf:pdf},
isbn = {978-1-5386-5111-7},
keywords = {5,a restoration model for,context encoder,deblurring,denoising and pixel interpolation,for asymmetric images,gan,image inpainting,in,inpainting,l2 loss,symmetric images but failed,the inpainting,the proposed technique is},
month = {dec},
pages = {103--108},
publisher = {IEEE},
title = {{Semantic Image Inpainting Vsing Self-Learning Encoder-Decoder and Adversarial Loss}},
url = {https://ieeexplore.ieee.org/document/8639258/},
year = {2018}
}
@article{Robinson2011,
abstract = {The Electroencephalogram (EEG) based Brain Computer Interface (BCI) is a non-invasive system to acquire, decode and convert brain signals into control signals for an external device. The Motor-Imagery based BCI (MI-BCI) efficiently decodes the brain signals from the imagination of movement but the performance is limited by the number of commands such as right and left hand motor imageries. However, other parameters of an actual voluntary movement, such as the direction of movement, speed and extent, are encoded in the brain signals. This paper investigates the EEG brain signals from directional changes in actual hand movement. The Wavelet-Common Spatial Pattern algorithm is proposed to extract discriminative features of the brain signals that carries the direction-related information. The experiment performed on two subjects yielded a mean classification accuracy of 87.85{\%} in decoding two classes of the direction-related information.},
author = {Robinson, Neethu and Vinod, A. P. and Guan, Cuntai and Ang, Kai Keng and Peng, Tee Keng},
doi = {10.1109/ICICS.2011.6174210},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Robinson et al. - 2011 - A Wavelet-CSP method to classify hand movement directions in EEG based BCI system.pdf:pdf},
isbn = {9781457700309},
journal = {ICICS 2011 - 8th International Conference on Information, Communications and Signal Processing},
keywords = {Brain Computer Interface,Common Spatial Pattern,Discrete Wavelet Transform,Electroencephalogram},
title = {{A Wavelet-CSP method to classify hand movement directions in EEG based BCI system}},
year = {2011}
}
@article{Kim2018,
abstract = {Automating statistical modelling is a challenging problem in artificial intelligence. The Automatic Statistician takes a first step in this direction, by employing a kernel search algorithm with Gaussian Processes (GP) to provide interpretable statistical models for regression problems. However this does not scale due to its O(N3) running time for the model selection. We propose Scalable Kernel Composition (SKC), a scalable kernel search algorithm that extends the Automatic Statistician to bigger data sets. In doing so, we derive a cheap upper bound on the GP marginal likelihood that sandwiches the marginal likelihood with the variational lower bound . We show that the upper bound is significantly tighter than the lower bound and thus useful for model selection.},
archivePrefix = {arXiv},
arxivId = {1706.02524},
author = {Kim, Hyunjik and Teh, Yee Whye},
eprint = {1706.02524},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Teh - 2018 - Scaling up the automatic statistician Scalable structure discovery using gaussian processes.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics, AISTATS 2018},
pages = {575--584},
title = {{Scaling up the automatic statistician: Scalable structure discovery using gaussian processes}},
volume = {7},
year = {2018}
}
@article{Zhang2018,
abstract = {Deep neural networks (DNNs) have achieved tremendous success in a variety of applications across many disciplines. Yet, their superior performance comes with the expensive cost of requiring correctly annotated large-scale datasets. Moreover, due to DNNs' rich capacity, errors in training labels can hamper performance. To combat this problem, mean absolute error (MAE) has recently been proposed as a noise-robust alternative to the commonly-used categorical cross entropy (CCE) loss. However, as we show in this paper, MAE can perform poorly with DNNs and challenging datasets. Here, we present a theoretically grounded set of noise-robust loss functions that can be seen as a generalization of MAE and CCE. Proposed loss functions can be readily applied with any existing DNN architecture and algorithm, while yielding good performance in a wide range of noisy label scenarios. We report results from experiments conducted with CIFAR-10, CIFAR-100 and FASHION-MNIST datasets and synthetically generated noisy labels.},
archivePrefix = {arXiv},
arxivId = {1805.07836},
author = {Zhang, Zhilu and Sabuncu, Mert R.},
eprint = {1805.07836},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Sabuncu - 2018 - Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.pdf:pdf},
number = {NeurIPS},
title = {{Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels}},
url = {http://arxiv.org/abs/1805.07836},
year = {2018}
}
@article{Evans2018,
abstract = {We introduce a kernel approximation strategy that enables computation of the Gaussian process log marginal likelihood and all hyperparameter derivatives in {\$}\backslashmathcal{\{}O{\}}(p){\$} time. Our GRIEF kernel consists of {\$}p{\$} eigenfunctions found using a Nystrom approximation from a dense Cartesian product grid of inducing points. By exploiting algebraic properties of Kronecker and Khatri-Rao tensor products, computational complexity of the training procedure can be practically independent of the number of inducing points. This allows us to use arbitrarily many inducing points to achieve a globally accurate kernel approximation, even in high-dimensional problems. The fast likelihood evaluation enables type-I or II Bayesian inference on large-scale datasets. We benchmark our algorithms on real-world problems with up to two-million training points and {\$}10{\^{}}{\{}33{\}}{\$} inducing points.},
archivePrefix = {arXiv},
arxivId = {1807.02125},
author = {Evans, Trefor W. and Nair, Prasanth B.},
eprint = {1807.02125},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Evans, Nair - 2018 - Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF).pdf:pdf},
title = {{Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF)}},
url = {http://arxiv.org/abs/1807.02125},
year = {2018}
}
@article{Steyvers2006,
abstract = {The idea of viewing human cognition as a rational solution to computational problems posed by the environment has influenced several recent theories of human memory. The first rational models of memory demonstrated that human memory seems to be remarkably well adapted to environmental statistics but made only minimal assumptions about the form of the environmental information represented in memory. Recently, several probabilistic methods for representing the latent semantic structure of language have been developed, drawing on research in computer science, statistics and computational linguistics. These methods provide a means of extending rational models of memory retrieval to linguistic stimuli, and a way to explore the influence of the statistics of language on human memory. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Steyvers, Mark and Griffiths, Thomas L. and Dennis, Simon},
doi = {10.1016/j.tics.2006.05.005},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Steyvers, Griffiths, Dennis - 2006 - Probabilistic inference in human semantic memory.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {327--334},
title = {{Probabilistic inference in human semantic memory}},
volume = {10},
year = {2006}
}
@article{Li2018b,
abstract = {In this work, we study the unsupervised video object segmentation problem where moving objects are segmented without prior knowledge of these objects. First, we propose a motion-based bilateral network to estimate the background based on the motion pattern of non-object regions. The bilateral network reduces false positive regions by accurately identifying background objects. Then, we integrate the background estimate from the bilateral network with instance embeddings into a graph, which allows multiple frame reasoning with graph edges linking pixels from different frames. We classify graph nodes by defining and minimizing a cost function, and segment the video frames based on the node labels. The proposed method outperforms previous state-of-the-art unsupervised video object segmentation methods against the DAVIS 2016 and the FBMS-59 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Li, Siyang and Seybold, Bryan and Vorobyov, Alexey and Lei, Xuejing and Kuo, C. C.Jay},
doi = {10.1007/978-3-030-01219-9_13},
eprint = {1311.2901},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - Unsupervised Video Object Segmentation with Motion-Based Bilateral Networks.pdf:pdf},
isbn = {9783030012182},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Bilateral networks,Instance embeddings,Video object segmentation},
pages = {215--231},
pmid = {15515431},
title = {{Unsupervised Video Object Segmentation with Motion-Based Bilateral Networks}},
volume = {11207 LNCS},
year = {2018}
}
@article{MacKay1998,
author = {MacKay, David},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/MacKay - 1998 - Introduction to Gaussian Processes.pdf:pdf},
journal = {NATO ASI Series F Computer and Systems Sciences},
pages = {133--166},
title = {{Introduction to Gaussian Processes}},
volume = {168},
year = {1998}
}
@article{Russell1995,
abstract = {Since its inception? arti?cial in telligence has relied upon a theoretical foundation cen? tred around p erfe ct r ationality as the desired propert yof in telligen t systems? W e argue? as others ha e done? that this foundation is inadequate because it imposes fundamen v tally unsatis?able requiremen ts? As a result? there has arisen a wide gap bet een theory and w practice in AI? hindering progress in the ?eld? W e propose instead a propert y called b ounde d optimality? Roughly speaking? an agen t is bounded?optimal if its program is a solution to the constrained optimization problem presen ted b y its arc hitecture and the task en viron? men t? W e sho who w to construct agen ts with this propert y for a simple class of mac hine arc hitectures in a broad class of real?time en vironmen ts? W e illustrate these results using a simple model of an automated mail sorting facilit ? W y e also de?ne a w eak er propert ? y asymptotic b ounde d optimality ?ABO?? that generalizes the notion of optimalit y in classical complexit y theory ? W e then construct universal ABO programs? i?e?? programs that are ABO no matter what real?time constrain ts are applied? Univ ersal ABO programs can be used as building bloc ks for more complex systems? W e conclude with a discussion of the prospects for bounded optimalit y as a theoretical basis for AI? and relate it to similar trends in philosoph ? economics? and game theory y ? ??},
archivePrefix = {arXiv},
arxivId = {cs/9505103},
author = {Russell, Stuart J and Subramanian, Devika},
doi = {10.1.1.54.9473},
eprint = {9505103},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Russell, Subramanian - 1995 - Provably Bounded-Optimal Agents.pdf:pdf},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
pages = {575--609},
primaryClass = {cs},
title = {{Provably Bounded-Optimal Agents}},
volume = {2},
year = {1995}
}
@article{Siddharth2017,
abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
archivePrefix = {arXiv},
arxivId = {1706.00400},
author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H. S.},
eprint = {1706.00400},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Siddharth et al. - 2017 - Learning Disentangled Representations with Semi-Supervised Deep Generative Models.pdf:pdf},
number = {Nips},
title = {{Learning Disentangled Representations with Semi-Supervised Deep Generative Models}},
url = {http://arxiv.org/abs/1706.00400},
year = {2017}
}
@incollection{Frazier2018,
abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
archivePrefix = {arXiv},
arxivId = {1807.02811},
author = {Frazier, Peter I.},
booktitle = {Recent Advances in Optimization and Modeling of Contemporary Problems},
doi = {10.1287/educ.2018.0188},
eprint = {1807.02811},
file = {:Users/mshvarts/Downloads/1807.02811.pdf:pdf},
issn = {23318422},
month = {oct},
number = {Section 5},
pages = {255--278},
publisher = {INFORMS},
title = {{Bayesian Optimization}},
url = {http://pubsonline.informs.org/doi/10.1287/educ.2018.0188},
year = {2018}
}
@article{Yue2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1903.03867v1},
author = {Yue, Xubo and Kontar, Raed},
eprint = {arXiv:1903.03867v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yue, Kontar - 2018 - Variational Inference of Joint Models using Multivariate Gaussian Convolution Processes.pdf:pdf},
title = {{Variational Inference of Joint Models using Multivariate Gaussian Convolution Processes}},
year = {2018}
}
@article{Miller2017,
abstract = {Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the "reparameterization trick," represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to use more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on non-conjugate multi-level hierarchical models and a Bayesian neural net where we observed gradient variance reductions of multiple orders of magnitude (20-2,000x).},
archivePrefix = {arXiv},
arxivId = {1705.07880},
author = {Miller, Andrew C. and Foti, Nicholas J. and D'Amour, Alexander and Adams, Ryan P.},
eprint = {1705.07880},
issn = {10495258},
title = {{Reducing Reparameterization Gradient Variance}},
year = {2017}
}
@article{Hernandez-Lobato2015a,
abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
archivePrefix = {arXiv},
arxivId = {1502.05336},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Adams, Ryan P.},
eprint = {1502.05336},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hern{\'{a}}ndez-Lobato, Adams - 2015 - Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks.pdf:pdf},
title = {{Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks}},
url = {http://arxiv.org/abs/1502.05336},
year = {2015}
}
@article{Sloman2002,
abstract = {Research on algorithms and representations once dominated AI . Recently the importance of architectures has been acknowledged , but researchers have different objectives , presup - positions and conceptual frameworks , and this can lead to confused terminology , argumenta - tion at cross purposes , re - invention of wheels and fragmentation of the research . We propose a methodological framework : develop a gen - eral representation of a wide class of architec - tures within which different architectures can be compared and contrasted . This should fa - cilitate communication and integration across sub - fields of and approaches to AI , as well as providing a framework for evaluating alterna - tive architectures . As a first - draft example we present the CogAff architecture schema , and show how it provides a draft framework . But there is much still to be done .},
author = {Sloman, Aaron and Scheutz, Matthias},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sloman, Scheutz - 2002 - A Framework for Comparing Agent Architectures.pdf:pdf},
journal = {UK Workshop on Computational Intelligence},
pages = {169--176},
title = {{A Framework for Comparing Agent Architectures}},
year = {2002}
}
@article{Kotseruba2016,
abstract = {In this paper we present a broad overview of the last 40 years of research on cognitive architectures. Although the number of existing architectures is nearing several hundred, most of the existing surveys do not reflect this growth and focus on a handful of well-established architectures. Thus, in this survey we wanted to shift the focus towards a more inclusive and high-level overview of the research on cognitive architectures. Our final set of 84 architectures includes 49 that are still actively developed, and borrow from a diverse set of disciplines, spanning areas from psychoanalysis to neuroscience. To keep the length of this paper within reasonable limits we discuss only the core cognitive abilities, such as perception, attention mechanisms, action selection, memory, learning and reasoning. In order to assess the breadth of practical applications of cognitive architectures we gathered information on over 900 practical projects implemented using the cognitive architectures in our list. We use various visualization techniques to highlight overall trends in the development of the field. In addition to summarizing the current state-of-the-art in the cognitive architecture research, this survey describes a variety of methods and ideas that have been tried and their relative success in modeling human cognitive abilities, as well as which aspects of cognitive behavior need more research with respect to their mechanistic counterparts and thus can further inform how cognitive science might progress.},
archivePrefix = {arXiv},
arxivId = {1610.08602},
author = {Kotseruba, Iuliia and Tsotsos, John K.},
eprint = {1610.08602},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kotseruba, Tsotsos - 2016 - A Review of 40 Years of Cognitive Architecture Research Core Cognitive Abilities and Practical Applications.pdf:pdf},
keywords = {attention,cognitive abilities,cognitive architectures,perception,practical,survey},
title = {{A Review of 40 Years of Cognitive Architecture Research: Core Cognitive Abilities and Practical Applications}},
url = {http://arxiv.org/abs/1610.08602},
year = {2016}
}
@article{BrianA.Nosek2007,
author = {{Brian A. Nosek} and {Anothony G. Greenwald} and {Mahzarin R. Banaji}},
doi = {10.1016/j.mrfmmm.2009.01.007},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Brian A. Nosek, Anothony G. Greenwald, Mahzarin R. Banaji - 2007 - The Implicit Association Test at Age 7 A Methodological and Conceptua.pdf:pdf},
isbn = {1134954107},
issn = {00275107},
journal = {Social Psychology and the Unconscious: The Automaticity of Higher Mental  Processes},
pages = {265--292},
pmid = {12500823},
title = {{The Implicit Association Test at Age 7: A Methodological and Conceptual and Conceptual Review.}},
url = {http://books.google.ca/books?hl=en{\&}lr={\&}id=fOuDJNtiCCUC{\&}oi=fnd{\&}pg=PA265{\&}dq=The+Implicit+Association+Test+at+Age+7:+A+Methodological+and+Conceptual+and+Conceptual+Review.{\&}ots=zrU{\_}Kvw36-{\&}sig=xFFJJ74qHGAqa3{\_}vfS4aGWkVwhs{\#}v=onepage{\&}q=The Implicit Association Te},
year = {2007}
}
@article{Ge2016,
abstract = {This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-{\$}k{\$} generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of {\$}O(\backslashfrac{\{}z k \backslashsqrt{\{}\backslashkappa{\}}{\}}{\{}\backslashrho{\}} \backslashlog(1/\backslashepsilon) \backslashlog \backslashleft(k\backslashkappa/\backslashrho\backslashright)){\$} where {\$}z{\$} is the total number of nonzero entries, {\$}\backslashkappa{\$} is the condition number and {\$}\backslashrho{\$} is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components {\$}k{\$} up to a {\$}\backslashlog(k){\$} factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets.},
archivePrefix = {arXiv},
arxivId = {1604.03930},
author = {Ge, Rong and Jin, Chi and Kakade, Sham M. and Netrapalli, Praneeth and Sidford, Aaron},
eprint = {1604.03930},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ge et al. - 2016 - Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis.pdf:pdf},
pages = {1--26},
title = {{Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis}},
url = {http://arxiv.org/abs/1604.03930},
year = {2016}
}
@article{Schulman2015,
abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
archivePrefix = {arXiv},
arxivId = {1506.05254},
author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
eprint = {1506.05254},
isbn = {9780999241103},
issn = {10495258},
title = {{Gradient Estimation Using Stochastic Computation Graphs}},
year = {2015}
}
@article{Lu2009,
abstract = {The common spatial patterns (CSP) algorithm is commonly used to extract discriminative spatial filters for the classification of electroencephalogram (EEG) signals in the context of brain-computer interfaces (BCIs). However, CSP is based on a sample-based covariance matrix estimation. Therefore, its performance is limited when the number of available training samples is small. In this paper, the CSP method is considered in such a small-sample setting. We propose a regularized common spatial patterns (R-CSP) algorithm by incorporating the principle of generic learning. The covariance matrix estimation in R-CSP is regularized through two regularization parameters to increase the estimation stability while reducing the estimation bias due to limited number of training samples. The proposed method is tested on data set IVa of the third BCI competition and the results show that R-CSP can outperform the classical CSP algorithm by 8.5{\%} on average. Moreover, the regularization introduced is particularly effective in the small-sample setting.},
author = {Lu, Haiping and Plataniotis, Konstantinos N. and Venetsanopoulos, Anastasios N.},
doi = {10.1109/IEMBS.2009.5332554},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lu, Plataniotis, Venetsanopoulos - 2009 - Regularized common spatial patterns with generic learning for EEG signal classification.pdf:pdf},
isbn = {9781424432967},
issn = {1557-170X},
journal = {Proceedings of the 31st Annual International Conference of the IEEE Engineering in Medicine and Biology Society: Engineering the Future of Biomedicine, EMBC 2009},
pages = {6599--6602},
pmid = {19963675},
title = {{Regularized common spatial patterns with generic learning for EEG signal classification}},
year = {2009}
}
@article{Hu2017a,
abstract = {Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.},
archivePrefix = {arXiv},
arxivId = {1706.00550},
author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1706.00550},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2017 - On Unifying Deep Generative Models(2).pdf:pdf},
pages = {1--22},
title = {{On Unifying Deep Generative Models}},
url = {http://arxiv.org/abs/1706.00550},
year = {2017}
}
@article{Liu2015,
abstract = {Reinforcement learning (RL) is a powerful paradigm for sequential decision-making under uncertainties, and most RL algorithms aim to maximize some numerical value which represents only one long-term objective. However, multiple longterm objectives are exhibited in many real-world decision and control systems, so recently there has been growing interest in solving multiobjective reinforcement learning (MORL) problems where there are multiple conflicting objectives. The aim of this paper is to present a comprehensive overview of MORL. The basic architecture, research topics, and na{\"{i}}ve solutions of MORL are introduced at first. Then, several representative MORL approaches and some important directions of recent research are comprehensively reviewed. The relationships between MORL and other related research are also discussed, which include multiobjective optimization, hierarchical RL, and multiagent RL. Moreover, research challenges and open problems of MORL techniques are suggested.},
author = {Liu, Chunming and Xu, Xin and Hu, Dewen},
doi = {10.1109/TSMC.2014.2358639},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Xu, Hu - 2015 - Multiobjective reinforcement learning A comprehensive overview.pdf:pdf},
isbn = {10946977 (ISSN)},
issn = {10834427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
keywords = {Markov decision process (MDP),Pareto front,multiobjective reinforcement learning (MORL),reinforcement learning (RL),sequential decision-making},
number = {3},
pages = {385--398},
title = {{Multiobjective reinforcement learning: A comprehensive overview}},
volume = {45},
year = {2015}
}
@article{Lee2017a,
abstract = {{\textcopyright} 2017 Neural information processing systems foundation. All rights reserved. Spike sorting is a critical first step in extracting neural signals from large-scale electrophysiological data. This manuscript describes an efficient, reliable pipeline for spike sorting on dense multi-electrode arrays (MEAs), where neural signals appear across many electrodes and spike sorting currently represents a major computational bottleneck. We present several new techniques that make dense MEA spike sorting more robust and scalable. Our pipeline is based on an efficient multistage "triage-then-cluster-then-pursuit" approach that initially extracts only clean, high-quality waveforms from the electrophysiological time series by temporarily skipping noisy or "collided" events (representing two neurons firing synchronously). This is accomplished by developing a neural network detection method followed by efficient outlier triaging. The clean waveforms are then used to infer the set of neural spike waveform templates through nonparametric Bayesian clustering. Our clustering approach adapts a "coreset" approach for data reduction and uses efficient inference methods in a Dirichlet process mixture model framework to dramatically improve the scalability and reliability of the entire pipeline. The "triaged" waveforms are then finally recovered with matching-pursuit deconvolution techniques. The proposed methods improve on the state-of-the-art in terms of accuracy and stability on both real and biophysically-realistic simulated MEA data. Furthermore, the proposed pipeline is efficient, learning templates and clustering faster than real-time for a ≈ 500-electrode dataset, largely on a single CPU core.},
author = {Lee, JinHyung and Carlson, David and Shokri, Hooshmand and Yao, Weichi and Goetz, Georges and Hagen, Espen and Batty, Eleanor and Chichilnisky, EJ and Einevoll, Gaute and Paninski, Liam},
doi = {10.1101/151928},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2017 - YASS Yet Another Spike Sorter.pdf:pdf},
keywords = {artificial intelligence,artificial neural network,cluster analysis,computer science,coreset,data reduction,dirichlet process,machine learning,mixture model,pattern recognition,scalability,spike sorting},
number = {Nips},
pages = {4002--4012},
title = {{YASS: Yet Another Spike Sorter}},
year = {2017}
}
@article{Mnih2015,
abstract = {Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.},
archivePrefix = {arXiv},
arxivId = {1604.03986},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1604.03986},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236 http://www.nature.com/articles/nature14236},
volume = {518},
year = {2015}
}
@article{Tran2015,
abstract = {We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.},
archivePrefix = {arXiv},
arxivId = {1506.03159},
author = {Tran, Dustin and Blei, David M. and Airoldi, Edoardo M.},
eprint = {1506.03159},
issn = {10495258},
title = {{Copula variational inference}},
year = {2015}
}
@article{Berthelot2019,
abstract = {Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38{\%} to 11{\%}) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.},
archivePrefix = {arXiv},
arxivId = {1905.02249},
author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
eprint = {1905.02249},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Berthelot et al. - 2019 - MixMatch A Holistic Approach to Semi-Supervised Learning.pdf:pdf},
title = {{MixMatch: A Holistic Approach to Semi-Supervised Learning}},
url = {http://arxiv.org/abs/1905.02249},
year = {2019}
}
@article{Al-Shedivat2016a,
abstract = {Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.},
archivePrefix = {arXiv},
arxivId = {1610.08936},
author = {Al-Shedivat, Maruan and Wilson, Andrew Gordon and Saatchi, Yunus and Hu, Zhiting and Xing, Eric P.},
eprint = {1610.08936},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Al-Shedivat et al. - 2016 - Learning Scalable Deep Kernels with Recurrent Structure.pdf:pdf},
pages = {1--37},
title = {{Learning Scalable Deep Kernels with Recurrent Structure}},
url = {http://arxiv.org/abs/1610.08936},
volume = {18},
year = {2016}
}
@article{Srivastava2016,
abstract = {We derive expressions for the first three moments of the decision time (DT) distribution produced via first threshold crossings by sample paths of a drift-diffusion equation. The “pure” and “extended” diffusion processes are widely used to model two-alternative forced choice decisions, and, while simple formulae for accuracy, mean DT and coefficient of variation are readily available, third and higher moments and conditioned moments are not generally available. We provide explicit formulae for these, describe their behaviors as drift rates and starting points approach interesting limits, and, with the support of numerical simulations, discuss how trial-to-trial variability of drift rates, starting points, and non-decision times affect these behaviors in the extended diffusion model. Both unconditioned moments and those conditioned on correct and erroneous responses are treated. We argue that the results will assist in exploring mechanisms of evidence accumulation and in fitting parameters to experimental data.},
archivePrefix = {arXiv},
arxivId = {1601.06420},
author = {Srivastava, V. and Holmes, P. and Simen, P.},
doi = {10.1016/j.jmp.2016.03.005},
eprint = {1601.06420},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava, Holmes, Simen - 2016 - Explicit moments of decision times for single- and double-threshold drift-diffusion processes.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Conditioned and unconditioned moments,Decision time,Diffusion model},
pages = {96--109},
title = {{Explicit moments of decision times for single- and double-threshold drift-diffusion processes}},
volume = {75},
year = {2016}
}
@article{Atkinson2018,
abstract = {We introduce a Bayesian Gaussian process latent variable model that explicitly captures spatial correlations in data using a parameterized spatial kernel and leveraging structure-exploiting algebra on the model covariance matrices for computational tractability. Inference is made tractable through a collapsed variational bound with similar computational complexity to that of the traditional Bayesian GP-LVM. Inference over partially-observed test cases is achieved by optimizing a "partially-collapsed" bound. Modeling high-dimensional time series systems is enabled through use of a dynamical GP latent variable prior. Examples imputing missing data on images and super-resolution imputation of missing video frames demonstrate the model.},
archivePrefix = {arXiv},
arxivId = {1805.08665},
author = {Atkinson, Steven and Zabaras, Nicholas},
doi = {10.1162/089976699300016331},
eprint = {1805.08665},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Atkinson, Zabaras - 2018 - Structured Bayesian Gaussian process latent variable model.pdf:pdf},
isbn = {978-1-4503-1285-1},
issn = {0899-7667},
pmid = {80990000001},
title = {{Structured Bayesian Gaussian process latent variable model}},
url = {http://arxiv.org/abs/1805.08665},
year = {2018}
}
@article{Sun2009,
abstract = {Canonical correlation analysis (CCA) and partial least squares (PLS) are well-known techniques for feature extraction from two sets of multi-dimensional variables. The fundamental difference between CCA and PLS is that CCA maximizes the correlation while PLS maximizes the covariance. Although both CCA and PLS have been applied successfully in various applications, the intrinsic relationship between them remains unclear. In this paper, we attempt to address this issue by showing the equivalence relationship between CCA and orthonormalized partial least squares (OPLS), a variant of PLS. We further extend the equivalence relationship to the case when regularization is employed for both sets of variables. In addition, we show that the CCA projection for one set of variables is independent of the regularization on the other set of variables. We have performed experimental studies using both synthetic and real data sets and our results confirm the established equivalence relationship. The presented analysis provides novel insights into the connection between these two existing algorithms as well as the effect of the regularization.},
author = {Sun, Liang and Ji, Shuiwang and Yu, Shipeng and Ye, Jieping},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2009 - On the equivalence between canonical correlation analysis and orthonormalized partial least squares.pdf:pdf},
isbn = {9781577354260},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning, Canonical correlation analysis,},
pages = {1230--1235},
title = {{On the equivalence between canonical correlation analysis and orthonormalized partial least squares}},
year = {2009}
}
@article{Ratcliff2016,
author = {Ratcliff, Roger and Smith, Philip L and Brown, Scott D. and McKoon, Gail},
doi = {10.1016/j.tics.2016.01.007},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ratcliff et al. - 2016 - Diffusion Decision Model Current Issues and History.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {apr},
number = {4},
pages = {260--281},
title = {{Diffusion Decision Model: Current Issues and History}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661316000255},
volume = {20},
year = {2016}
}
@article{Stegle2011,
abstract = {Inference in matrix-variate Gaussian models has major applications for multi-output prediction and joint learning of row and column covariances from matrix-variate data. Here, we discuss an approach for efficient inference in such models that explicitly account for iid observation noise. Computational tractability can be retained by exploiting the Kronecker product between row and column covariance matrices. Using this framework, we show how to generalize the Graphical Lasso in order to learn a sparse inverse covariance between features while accounting for a low-rank confounding covariance between samples. We show practical utility on applications to biology, where we model covariances with more than 100,000 dimensions. We find greater accuracy in recovering biological network structures and are able to better reconstruct the confounders.},
author = {Stegle, Oliver and Lippert, Christoph and Mooij, Joris and Lawrence, Neil and Borgwardt, Karsten},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Stegle et al. - 2011 - Efficient inference in matrix-variate Gaussian models with iid observation noise.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24 (NIPS 2011)},
keywords = {Stegle:2011},
pages = {630--638},
title = {{Efficient inference in matrix-variate Gaussian models with iid observation noise}},
url = {http://papers.nips.cc/paper/4281-efficient-inference-in-matrix-variate-gaussian-models-with-iid-observation-noise},
year = {2011}
}
@article{Bottou2017,
abstract = {Learning algorithms for implicit generative models can optimize a variety of criteria that measure how the data distribution differs from the implicit model distribution, including the Wasserstein distance, the Energy distance, and the Maximum Mean Discrepancy criterion. A careful look at the geometries induced by these distances on the space of probability measures reveals interesting differences. In particular, we can establish surprising approximate global convergence guarantees for the {\$}1{\$}-Wasserstein distance,even when the parametric generator has a nonconvex parametrization.},
author = {Bottou, Leon and Arjovsky, Martin and Lopez-paz, David and Oquab, Maxime},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bottou et al. - 2017 - Geometrical Insights for Implicit Generative Modeling.pdf:pdf},
journal = {arXiv:1712.07822 [cs, stat]},
pages = {1--40},
title = {{Geometrical Insights for Implicit Generative Modeling}},
url = {http://arxiv.org/abs/1712.07822},
year = {2017}
}
@article{Zhe2019,
abstract = {While most Gaussian processes (GP) work focus on learning single-output functions, many applications, such as physical simulations and gene expressions prediction, require estimations of functions with many outputs. The number of outputs can be much larger than or comparable to the size of training samples. Existing multi-output GP models either are limited to low-dimensional outputs and restricted kernel choices, or assume oversimplified low-rank structures within the outputs. To address these issues, we propose HOGPR, a High-Order Gaussian Process Regression model, which can flexibly capture complex correlations among the outputs and scale up to a large number of outputs. Specifically, we tensorize the high-dimensional outputs, introducing latent coordinate features to index each tensor element (i.e., output) and to capture their correlations. We then generalize a multilinear model to a hybrid of a GP and latent GP model. The model is endowed with a Kronecker product structure over the inputs and the latent features. Using the Kronecker product properties and tensor algebra, we are able to perform exact inference over millions of outputs. We show the advantage of the proposed model on several real-world applications.},
author = {Zhe, Shandian and Xing, Wei and Kirby, Robert M},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhe, Xing, Kirby - 2019 - Scalable High-Order Gaussian Process Regression.pdf:pdf},
journal = {Proceedings of Machine Learning Research},
pages = {2611--2620},
title = {{Scalable High-Order Gaussian Process Regression}},
url = {http://proceedings.mlr.press/v89/zhe19a.html},
volume = {89},
year = {2019}
}
@article{Zhang2009,
author = {Zhang, Kun and Sch, Bernhard and Janzing, Dominik},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Sch, Janzing - 2009 - Invariant Gaussian Process Latent Variable Models and Application in Causal Discovery.pdf:pdf},
title = {{Invariant Gaussian Process Latent Variable Models and Application in Causal Discovery}},
year = {2009}
}
@article{Jones2014,
abstract = {Much current research on speeded choice utilizes models in which the response is triggered by a stochastic process crossing a deterministic threshold. This article focuses on 2 such model classes, 1 based on continuous-time diffusion and the other on linear ballistic accumulation (LBA). Both models assume random variability in growth rates and in other model components across trials. We show that if the form of this variability is unconstrained, the models can exactly match any possible pattern of response probabilities and response time distributions. Thus, the explanatory or predictive content of these models is determined not by their structural assumptions but, rather, by distributional assumptions (e.g., Gaussian distributions) that are traditionally regarded as implementation details. Selective influence assumptions (i.e., which experimental manipulations affect which model parameters) are shown to have no restrictive effect, except for the theoretically questionable assumption that speed-accuracy instructions do not affect growth rates. The 2nd contribution of this article concerns translation of falsifiable models between universal modeling languages. Specifically, we translate the predictions of the diffusion and LBA models (with their parametric and selective influence assumptions intact) into the Grice modeling framework, in which accumulation processes are deterministic and thresholds are random variables. The Grice framework is also known to reproduce any possible pattern of response probabilities and times, and hence it can be used as a common language for comparing models. It is found that only a few simple properties of empirical data are necessary predictions of the diffusion and LBA models. {\textcopyright} 2013 American Psychological Association.},
author = {Jones, Matt and Dzhafarov, Ehtibar N.},
doi = {10.1037/a0034190},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jones, Dzhafarov - 2014 - Unfalsifiability and mutual translatability of major modeling schemes for choice reaction time.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
keywords = {Choice reaction time,Diffusion model,Grice framework,Linear ballistic accumulator,Model falsifiability},
number = {1},
pages = {1--32},
title = {{Unfalsifiability and mutual translatability of major modeling schemes for choice reaction time}},
volume = {121},
year = {2014}
}
@article{Houlsby2012,
abstract = {We present a new model based on Gaussian processes (GPs) for learning pairwise preferences expressed by multiple users. Inference is simplified by using a preference kernel for GPs which allows us to combine supervised GP learning of user preferences with unsupervised dimensionality reduction for multi-user systems. The model not only exploits collaborative information from the shared structure in user behavior, but may also incorporate user features if they are available. Approximate inference is implemented using a combination of expectation propagation and variational Bayes. Finally, we present an efficient active learning strategy for querying preferences. The proposed technique performs favorably on real-world data against state-of-the-art multi-user preference learning algorithms.},
author = {Houlsby, Neil and Hern{\'{a}}ndez-Lobato, Jose Miguel and Husz{\'{a}}r, Ferenc and Ghahramani, Zoubin},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Houlsby et al. - 2012 - Collaborative Gaussian processes for preference learning.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2096--2104},
title = {{Collaborative Gaussian processes for preference learning}},
volume = {3},
year = {2012}
}
@article{Rosenbloom2016,
abstract = {{\textless}p{\textgreater}Sigma ($\Sigma$) is a cognitive architecture and system whose development is driven by a combination of four desiderata:{\textless}/p{\textgreater}},
author = {Rosenbloom, Paul S. and Demski, Abram and Ustun, Volkan},
doi = {10.1515/jagi-2016-0001},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rosenbloom, Demski, Ustun - 2016 - The Sigma Cognitive Architecture and System Towards Functionally Elegant Grand Unification.pdf:pdf},
issn = {1946-0163},
journal = {Journal of Artificial General Intelligence},
keywords = {cognitive architecture,cognitive system,graphical models,sigma},
number = {1},
pages = {1--103},
title = {{The Sigma Cognitive Architecture and System: Towards Functionally Elegant Grand Unification}},
url = {https://www.degruyter.com/view/j/jagi.2016.7.issue-1/jagi-2016-0001/jagi-2016-0001.xml},
volume = {7},
year = {2016}
}
@article{Hu2017,
abstract = {Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as powerful frameworks for deep generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transfered techniques.},
archivePrefix = {arXiv},
arxivId = {1706.00550},
author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1706.00550},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2017 - On Unifying Deep Generative Models.pdf:pdf},
pages = {1--19},
title = {{On Unifying Deep Generative Models}},
url = {http://arxiv.org/abs/1706.00550},
year = {2017}
}
@article{Ma2015,
abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
archivePrefix = {arXiv},
arxivId = {1506.04696},
author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
eprint = {1506.04696},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ma, Chen, Fox - 2015 - A Complete Recipe for Stochastic Gradient MCMC.pdf:pdf},
issn = {10495258},
title = {{A Complete Recipe for Stochastic Gradient MCMC}},
year = {2015}
}
@inproceedings{Zhou2017,
abstract = {Copyright {\textcopyright} 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Canonical Correlation Analysis (CCA) is a classical technique for two-view correlation analysis, while Probabilistic CCA (PCCA) provides a generative and more general viewpoint for this task. Recently, PCCA has been extended to bilinear cases for dealing with two-view matrices in order to preserve and exploit the matrix structures in PCCA. However, existing bilinear PCCAs impose restrictive model assumptions for matrix structure preservation, sacrificing generative correctness or model flexibility. To overcome these drawbacks, we propose BPCCA, a new bilinear extension of PCCA, by introducing a hybrid joint model. Our new model preserves matrix structures indirectly via hybrid vector-based and matrix-based concatenations. This enables BPCCA to gain more model flexibility in capturing two-view correlations and obtain close-form solutions in parameter estimation. Experimental results on two real-world applications demonstrate the superior performance of BPCCA over competing methods.},
author = {Zhou, Yang and Lu, Haiping and Cheung, Yiu-ming},
booktitle = {AAAI},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Lu, Cheung - 2017 - Bilinear Probabilistic Canonical Correlation Analysis via Hybrid Concatenations.pdf:pdf},
pages = {2949--2955},
title = {{Bilinear Probabilistic Canonical Correlation Analysis via Hybrid Concatenations}},
year = {2017}
}
@article{Siegel1999,
abstract = {Diffuse optical tomography (DOT) can image spatial variations in highly scattering optical media. We have built an inexpensive and portable continuous-wave DOT system containing 18 laser diode sources (9 at 780nm and 9 at 830nm) and 16 silicon detectors, which can acquire 288 independent measurements in less than 4 seconds. These data can then be processed using a variety of imaging algorithms. We first discuss the design of diffuse imaging equipment in general, and then describe our instrument, along with the technical issues that influenced its design. The technical challenges involved in performing DOT over large optode areas are discussed. We also present rat brain measurements following electrical forepaw stimulation using DOT. These results clearly demonstrate the capabilities of DOT and set the stage for advancement to quantitative functional brain imaging.},
author = {Siegel, A and Marota, J J and Boas, D},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Siegel, Marota, Boas - 1999 - Design and evaluation of a continuous-wave diffuse optical tomography system.pdf:pdf},
issn = {1094-4087},
journal = {Optics express},
number = {8},
pages = {287--98},
pmid = {19396285},
title = {{Design and evaluation of a continuous-wave diffuse optical tomography system.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19396285},
volume = {4},
year = {1999}
}
@article{Jang2018,
abstract = {This paper proposes a novel graph signal-based deep learning method for electroencephalography (EEG) and its application to EEG-based video identification. We present new methods to effectively represent EEG data as signals on graphs, and learn them using graph convolutional neural networks. Experimental results for video identification using EEG responses obtained while watching videos show the effectiveness of the proposed approach in comparison to existing methods. Effective schemes for graph signal representation of EEG are also discussed.},
archivePrefix = {arXiv},
arxivId = {1809.04229},
author = {Jang, Soobeom and Moon, Seong-Eun and Lee, Jong-Seok},
eprint = {1809.04229},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jang, Moon, Lee - 2018 - EEG-based video identification using graph signal modeling and graph convolutional neural network.pdf:pdf},
month = {sep},
title = {{EEG-based video identification using graph signal modeling and graph convolutional neural network}},
url = {http://arxiv.org/abs/1809.04229},
year = {2018}
}
@article{Bui2016a,
abstract = {Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.},
archivePrefix = {arXiv},
arxivId = {1602.04133},
author = {Bui, Thang D. and Hern{\'{a}}ndez-Lobato, Daniel and Li, Yingzhen and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Turner, Richard E.},
doi = {10.1162/NECO_a_00104},
eprint = {1602.04133},
isbn = {978-0-387-30768-8, 978-0-387-30164-8},
issn = {1530-888X},
pmid = {21222527},
title = {{Deep Gaussian Processes for Regression using Approximate Expectation Propagation}},
year = {2016}
}
@article{Hensman2018,
author = {Hensman, James},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hensman - 2018 - Variational Fourier Features for Gaussian Processes.pdf:pdf},
keywords = {fourier features,gaussian processes,variational inference},
pages = {1--52},
title = {{Variational Fourier Features for Gaussian Processes}},
volume = {18},
year = {2018}
}
@article{Pennington2018,
abstract = {An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods.},
author = {Pennington, Jeffrey and Brain, Google and Worah, Pratik and Research, Google},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pennington et al. - 2018 - The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network.pdf:pdf},
journal = {Neural Information Processing Systems (NIPS)},
number = {Nips},
title = {{The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network}},
year = {2018}
}
@inproceedings{Dutordoir2018,
abstract = {Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GPs) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.},
archivePrefix = {arXiv},
arxivId = {1810.12750},
author = {Dutordoir, Vincent and Salimbeni, Hugh and Deisenroth, Marc Peter and Hensman, James},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1810.12750},
issn = {10495258},
title = {{Gaussian process conditional density estimation}},
volume = {2018-Decem},
year = {2018}
}
@article{Fisac2017,
abstract = {As intelligent systems gain autonomy and capability, it becomes vital to ensure that their objectives match those of their human users; this is known as the value-alignment problem. In robotics, value alignment is key to the design of collaborative robots that can integrate into human workflows, successfully inferring and adapting to their users' objectives as they go. We argue that a meaningful solution to value alignment must combine multi-agent decision theory with rich mathematical models of human cognition, enabling robots to tap into people's natural collaborative capabilities. We present a solution to the cooperative inverse reinforcement learning (CIRL) dynamic game based on well-established cognitive models of decision making and theory of mind. The solution captures a key reciprocity relation: the human will not plan her actions in isolation, but rather reason pedagogically about how the robot might learn from them; the robot, in turn, can anticipate this and interpret the human's actions pragmatically. To our knowledge, this work constitutes the first formal analysis of value alignment grounded in empirically validated cognitive models.},
archivePrefix = {arXiv},
arxivId = {1707.06354},
author = {Fisac, Jaime F. and Gates, Monica A. and Hamrick, Jessica B. and Liu, Chang and Hadfield-Menell, Dylan and Palaniappan, Malayandi and Malik, Dhruv and Sastry, S. Shankar and Griffiths, Thomas L. and Dragan, Anca D.},
eprint = {1707.06354},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Fisac et al. - 2017 - Pragmatic-Pedagogic Value Alignment.pdf:pdf},
keywords = {dynamic game theory,human-robot interaction,value alignment},
title = {{Pragmatic-Pedagogic Value Alignment}},
url = {http://arxiv.org/abs/1707.06354},
year = {2017}
}
@inproceedings{Nikolaidis2016,
abstract = {—Mutual adaptation is critical for effective team collaboration. This paper presents a formalism for human-robot mutual adaptation in collaborative tasks. We propose the bounded-memory adaptation model (BAM), which captures human adaptive behaviors based on a bounded memory assumption. We integrate BAM into a partially observable stochastic model, which enables robot adaptation to the human. When the human is adaptive, the robot will guide the human towards a new, optimal collaborative strategy unknown to the human in advance. When the human is not willing to change their strategy, the robot adapts to the human in order to retain human trust. Human subject experiments indicate that the proposed formalism can significantly improve the effectiveness of human-robot teams, while human subject ratings on the robot performance and trust are comparable to those achieved by cross training, a state-of-the-art human-robot team training practice.},
author = {Nikolaidis, Stefanos and Kuznetsov, Anton and Hsu, David and Srinivasa, Siddhartha},
booktitle = {ACM/IEEE International Conference on Human-Robot Interaction},
doi = {10.1109/HRI.2016.7451736},
isbn = {9781467383707},
issn = {21672148},
title = {{Formalizing human-robot mutual adaptation: A bounded memory model}},
volume = {2016-April},
year = {2016}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7587},
pmid = {26819042},
title = {{Mastering the game of Go with deep neural networks and tree search}},
volume = {529},
year = {2016}
}
@article{Gilboa2015,
abstract = {Exact Gaussian Process (GP) regression has O(N{\^{}}3) runtime for data size N, making it intractable for large N. Many algorithms for improving GP scaling approximate the covariance with lower rank matrices. Other work has exploited structure inherent in particular covariance functions, including GPs with implied Markov structure, and equispaced inputs (both enable O(N) runtime). However, these GP advances have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications. This paper introduces and tests novel extensions of structured GPs to multidimensional inputs. We present new methods for additive GPs, showing a novel connection between the classic backfitting method and the Bayesian framework. To achieve optimal accuracy-complexity tradeoff, we extend this model with a novel variant of projection pursuit regression. Our primary result -- projection pursuit Gaussian Process Regression -- shows orders of magnitude speedup while preserving high accuracy. The natural second and third steps include non-Gaussian observations and higher dimensional equispaced grid methods. We introduce novel techniques to address both of these necessary directions. We thoroughly illustrate the power of these three advances on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost.},
author = {Gilboa, Elad and Saatci, Yunus and Cunningham, John P.},
doi = {10.1109/TPAMI.2013.192},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gilboa, Saatci, Cunningham - 2015 - Scaling multidimensional inference for structured gaussian processes.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
title = {{Scaling multidimensional inference for structured gaussian processes}},
volume = {37},
year = {2015}
}
@article{Gorski2009,
author = {Gorski, Nicholas},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gorski - 2009 - Learning to Use Episodic Memory.pdf:pdf},
keywords = {Artificial Intelligence,Cognitive Architecture,Episodic Memory,Intelligent Agents,Reinforcement Learning,artificial intelligence,cognitive architecture,episodic memory,intelligent agents,reinforcement},
title = {{Learning to Use Episodic Memory}},
year = {2009}
}
@article{Betancourt2014,
abstract = {Although Hamiltonian Monte Carlo has proven an empirical success, the lack of a rigorous theoretical understanding of the algorithm has in many ways impeded both principled developments of the method and use of the algorithm in practice. In this paper we develop the formal foundations of the algorithm through the construction of measures on smooth manifolds, and demonstrate how the theory naturally identifies efficient implementations and motivates promising generalizations.},
archivePrefix = {arXiv},
arxivId = {1410.5110},
author = {Betancourt, M. J. and Byrne, Simon and Livingstone, Samuel and Girolami, Mark},
doi = {10.3150/16-BEJ810},
eprint = {1410.5110},
issn = {1350-7265},
title = {{The Geometric Foundations of Hamiltonian Monte Carlo}},
year = {2014}
}
@article{Soares2016,
abstract = {A superintelligent machine would not automatically act as intended: it will act as programmed, but the fit between human intentions and formal specification could be poor. We discuss methods by which a system could be constructed to learn what to value. We highlight open problems specific to inductive value learning (from labeled training data), and raise a number of questions about the construction of systems which model the preferences of their operators and act accordingly.},
author = {Soares, Nate},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Soares - 2016 - The value learning problem.pdf:pdf},
journal = {Association for the Advancement of Artificial Intelligence},
number = {July},
pages = {1--8},
title = {{The value learning problem}},
url = {http://intelligence.org/files/ValueLearningProblem.pdf},
year = {2016}
}
@article{Gal2017,
abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
archivePrefix = {arXiv},
arxivId = {1703.02910},
author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
doi = {10.17863/CAM.11070},
eprint = {1703.02910},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Deep Bayesian Active Learning with Image Data}},
year = {2017}
}
@article{Holmes2014,
abstract = {We review how leaky competing accumulators (LCAs) can be used to model decision making in two-alternative, forced-choice tasks, and we show how they reduce to drift diffusion (DD) processes in special cases. As continuum limits of the sequential probability ratio test, DD processes are optimal in producing decisions of specified accuracy in the shortest possible time. Furthermore, the DD model can be used to derive a speed-accuracy trade-off that optimizes reward rate for a restricted class of two alternative forced-choice decision tasks. We review findings that compare human performance with this benchmark, and we reveal both approximations to and deviations from optimality. We then discuss three potential sources of deviations from optimality at the psychological level--avoidance of errors, poor time estimation, and minimization of the cost of control--and review recent theoretical and empirical findings that address these possibilities. We also discuss the role of cognitive control in changing environments and in modulating exploitation and exploration. Finally, we consider physiological factors in which nonlinear dynamics may also contribute to deviations from optimality.},
author = {Holmes, Philip and Cohen, Jonathan D.},
doi = {10.1111/tops.12084},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Holmes, Cohen - 2014 - Optimality and some of its discontents Successes and shortcomings of existing models for binary decisions.pdf:pdf},
isbn = {1756-8765 (Electronic)$\backslash$r1756-8757 (Linking)},
issn = {17568765},
journal = {Topics in Cognitive Science},
keywords = {Accumulator,Cognitive control,Costs,Decision making,Drift-diffusion process,Exploitation,Exploration,Optimality,Robustness,Speed-accuracy trade-off},
number = {2},
pages = {258--278},
pmid = {24648411},
title = {{Optimality and some of its discontents: Successes and shortcomings of existing models for binary decisions}},
volume = {6},
year = {2014}
}
@article{Damianou2016,
abstract = {The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projec-tion variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the non-linear latent space. The result-ing framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.},
author = {Damianou, Andreas C and Titsias, Michalis K and Lawrence, Neil D},
issn = {15337928},
journal = {Journal of Machine Learning Research},
title = {{Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes}},
volume = {17},
year = {2016}
}
@article{Louizos2016,
abstract = {We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian $\backslash$cite{\{}gupta1999matrix{\}} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the "local reprarametrization trick" $\backslash$cite{\{}kingma2015variational{\}} on this posterior distribution we arrive at a Gaussian Process $\backslash$cite{\{}rasmussen2006gaussian{\}} interpretation of the hidden units in each layer and we, similarly with $\backslash$cite{\{}gal2015dropout{\}}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate "pseudo-data" $\backslash$cite{\{}snelson2005sparse{\}} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments.},
archivePrefix = {arXiv},
arxivId = {1603.04733},
author = {Louizos, Christos and Welling, Max},
eprint = {1603.04733},
isbn = {1603.04733},
issn = {1938-7228},
title = {{Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors}},
year = {2016}
}
@article{Pillonetto2009,
abstract = {Population models are widely applied in biomedical data analysis since they characterize both the average and individual responses of a population of subjects. In the absence of a reliable mechanistic model, one can resort to the Bayesian nonparametric approach that models the individual curves as Gaussian processes. This paper develops an efficient computational scheme for estimating the average and individual curves from large data sets collected in standardized experiments, i.e. with a fixed sampling schedule. It is shown that the overall scheme exhibits a "client-server" architecture. The server is in charge of handling and processing the collective data base of past experiments. The clients ask the server for the information needed to reconstruct the individual curve in a single new experiment. This architecture allows the clients to take advantage of the overall data set without violating possible privacy and confidentiality constraints and with negligible computational effort. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Pillonetto, Gianluigi and {De Nicolao}, Giuseppe and Chierici, Marco and Cobelli, Claudio},
doi = {10.1016/j.automatica.2008.06.003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pillonetto et al. - 2009 - Fast algorithms for nonparametric population modeling of large data sets.pdf:pdf},
issn = {00051098},
journal = {Automatica},
keywords = {Bayesian estimation,Estimation theory,Gaussian processes,Glucose metabolism,Nonparametric identification},
number = {1},
pages = {173--179},
title = {{Fast algorithms for nonparametric population modeling of large data sets}},
volume = {45},
year = {2009}
}
@article{Nickson2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1510.07965v2},
author = {Nickson, Thomas and Gunter, Tom and Lloyd, Chris and Osborne, Michael A and Roberts, Stephen},
eprint = {arXiv:1510.07965v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nickson et al. - 2015 - Blitzkriging Kronecker-structured Stochastic Gaussian Processes.pdf:pdf},
pages = {1--13},
title = {{Blitzkriging : Kronecker-structured Stochastic Gaussian Processes}},
year = {2015}
}
@article{Ebden2015,
abstract = {A gentle introduction to Gaussian processes (GPs). The three parts of the document consider GPs for regression, classification, and dimensionality reduction.},
archivePrefix = {arXiv},
arxivId = {1505.02965},
author = {Ebden, Mark},
eprint = {1505.02965},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ebden - 2015 - Gaussian Processes A Quick Introduction.pdf:pdf},
isbn = {9781467367592},
issn = {{\textless}null{\textgreater}},
number = {August},
title = {{Gaussian Processes: A Quick Introduction}},
url = {http://arxiv.org/abs/1505.02965},
year = {2015}
}
@article{Kia2018,
abstract = {Most brain disorders are very heterogeneous in terms of their underlying biology and developing analysis methods to model such heterogeneity is a major challenge. A promising approach is to use probabilistic regression methods to estimate normative models of brain function using (f)MRI data then use these to map variation across individuals in clinical populations (e.g., via anomaly detection). To fully capture individual differences, it is crucial to statistically model the patterns of correlation across different brain regions and individuals. However, this is very challenging for neuroimaging data because of high-dimensionality and highly structured patterns of correlation across multiple axes. Here, we propose a general and flexible multi-task learning framework to address this problem. Our model uses a tensor-variate Gaussian process in a Bayesian mixed-effects model and makes use of Kronecker algebra and a low-rank approximation to scale efficiently to multi-way neuroimaging data at the whole brain level. On a publicly available clinical fMRI dataset, we show that our computationally affordable approach substantially improves detection sensitivity over both a mass-univariate normative model and a classifier that --unlike our approach-- has full access to the clinical labels.},
archivePrefix = {arXiv},
arxivId = {1808.00036},
author = {Kia, Seyed Mostafa and Beckmann, Christian F and Marquand, Andre F},
eprint = {1808.00036},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kia, Beckmann, Marquand - 2018 - Scalable Multi-Task Gaussian Process Tensor Regression for Normative Modeling of Structured Variation i.pdf:pdf},
month = {jul},
title = {{Scalable Multi-Task Gaussian Process Tensor Regression for Normative Modeling of Structured Variation in Neuroimaging Data}},
url = {http://arxiv.org/abs/1808.00036},
year = {2018}
}
@article{Duncker2019,
abstract = {We develop an approach to learn an interpretable semi-parametric model of a latent continuous-time stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function drawn from a Gaussian process (GP) conditioned on a set of learnt fixed points and corresponding local Jacobian matrices. This form yields a flexible nonparametric model of the dynamics, with a representation corresponding directly to the interpretable portraits routinely employed in the study of nonlinear dynamical systems. The learning algorithm combines inference of continuous latent paths underlying observed data with a sparse variational description of the dynamical process. We demonstrate our approach on simulated data from different nonlinear dynamical systems.},
archivePrefix = {arXiv},
arxivId = {1902.04420},
author = {Duncker, Lea and Bohner, Gergo and Boussard, Julien and Sahani, Maneesh},
doi = {10.1111/cogs.12047},
eprint = {1902.04420},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Duncker et al. - 2019 - Learning interpretable continuous-time models of latent stochastic dynamical systems.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {a research pro-,combinatorial structure,distributed representation,gram that was already,harmonic grammar,optimization,path for carrying out,selection,sketched by 1986 1,speech errors,the study discussed here,was developed as one},
month = {feb},
number = {6},
pages = {1102--1138},
title = {{Learning interpretable continuous-time models of latent stochastic dynamical systems}},
url = {http://doi.wiley.com/10.1111/cogs.12047 http://arxiv.org/abs/1902.04420},
volume = {38},
year = {2019}
}
@article{Bijma2003,
abstract = {The general spatiotemporal covariance matrix of the background noise in MEG/EEG signals is huge. To reduce the dimensionality of this matrix it is modeled as a Kronecker product of a spatial and a temporal covariance matrix. When the number of time samples is larger than, say, J = 500, the iterative Maximum Likelihood estimation of these two matrices is still too time-consuming to be useful on a routine basis. In this study we looked for methods to circumvent this computationally expensive procedure by using a parametric model with subject-dependent parameters. Such a model would additionally help with interpreting MEG/EEG signals. For the spatial covariance, models have been derived already and it has been shown that measured MEG/EEG signals can be understood spatially as random processes, generated by random dipoles. The temporal covariance, however, has not been modeled yet, therefore we studied the temporal covariance matrix in several subjects. For all subjects the temporal covariance shows an alpha oscillation and vanishes for large time lag. This gives rise to a temporal noise model consisting of two components: alpha activity and additional random noise. The alpha activity is modeled as randomly occurring waves with random phase and the covariance of the additional noise decreases exponentially with lag. This model requires only six parameters instead of 1/2J(J + 1). Theoretically, this model is stationary but in practice the stationarity of the matrix is highly influenced by the baseline correction. It appears that very good agreement between the data and the parametric model can be obtained when the baseline correction window is taken into account properly. This finding implies that the background noise is in principle a stationary process and that nonstationarities are mainly caused by the nature of the preprocessing method. When analyzing events at a fixed sample after the stimulus (e.g., the SEF N20 response) one can take advantage of this nonstationarity by optimizing the baseline window to obtain a low noise variance at this particular sample. {\textcopyright} 2003 Elsevier Science (USA). All rights reserved.},
author = {Bijma, Fetsje and {De Munck}, Jan C. and Huizenga, Hilde M. and Heethaar, Rob M.},
doi = {10.1016/S1053-8119(03)00215-5},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bijma et al. - 2003 - A mathematical approach to the temporal stationarity of background noise in MEGEEG measurements.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Alpha activity,Magnetoencephalography,Spatiotemporal covariance,Temporal stationarity},
number = {1},
pages = {233--243},
pmid = {14527584},
title = {{A mathematical approach to the temporal stationarity of background noise in MEG/EEG measurements}},
volume = {20},
year = {2003}
}
@techreport{Parra2003a,
abstract = {In this short note we highlight the fact that linear blind source separation can be formulated as a generalized eigenvalue decomposition under the assumptions of non-Gaussian, non-stationary, or non-white independent sources. The solution for the unmixing matrix is given by the generalized eigenvectors that simultaneously diagonalize the covariance matrix of the observations and an additional symmetric matrix whose form depends upon the particular assumptions. The method critically determines the mixture coefficients and is therefore not robust to estimation errors. However it provides a rather general and unified solution that summarizes the conditions for successful blind source separation. To demonstrate the method, which can be implemented in two lines of matlab code, we present results for artificial mixtures of speech and real mixtures of electroencephalogra-phy (EEG) data, showing that the same sources are recovered under the various assumptions.},
author = {Parra, Lucas and Sajda, Paul},
booktitle = {Journal of Machine Learning Research},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Parra, Sajda - 2003 - Blind Source Separation via Generalized Eigenvalue Decomposition.pdf:pdf},
keywords = {blind source separation,generalized eigenvalue decomposition,non-Gaussian,non-stationary,non-white},
pages = {1261--1269},
title = {{Blind Source Separation via Generalized Eigenvalue Decomposition}},
volume = {4},
year = {2003}
}
@article{Naudts2020,
abstract = {A generalised notion of exponential families is introduced. It is based on the variational principle, borrowed from statistical physics.
It is shown that inequivalent generalised entropy functions lead to distinct generalised exponential families. The well-known result that the inequality of Cram$\backslash$'er and Rao becomes an equality in the case of an exponential family can be generalised. However, this requires the introduction of escort probabilities.},
author = {Naudts, Jan},
doi = {10.3390/entropy-e10030131},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Naudts - 2020 - Generalised Exponential Families and Associated Entropy Functions.pdf:pdf},
keywords = {Bregman divergence,alised exponential family,bregman divergence,escort proba-,escort probability,gener-,generalised Fisher information,generalised entropy,generalised exponential family,generalised fisher information,maximum entropy principle,variational principle},
number = {2},
pages = {131--149},
title = {{Generalised Exponential Families and Associated Entropy Functions}},
year = {2020}
}
@inproceedings{Houlsby2011,
abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
archivePrefix = {arXiv},
arxivId = {1112.5745},
author = {Houlsby, Neil and Husz{\'{a}}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'{a}}t{\'{e}}},
booktitle = {Neural Information Processing Systems},
eprint = {1112.5745},
file = {:Users/mshvarts/Downloads/1112.5745.pdf:pdf},
month = {dec},
pages = {1--17},
title = {{Bayesian Active Learning for Classification and Preference Learning}},
url = {http://arxiv.org/abs/1112.5745},
year = {2011}
}
@article{Shrivastava2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1612.07828v2},
author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
eprint = {arXiv:1612.07828v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shrivastava et al. - 2016 - Training.pdf:pdf},
title = {{Training}},
year = {2016}
}
@article{Tiganj2019,
abstract = {{\textless}p{\textgreater}Natural learners must compute an estimate of future outcomes that follow from a stimulus in continuous time. Widely used reinforcement learning algorithms discretize continuous time and estimate either transition functions from one step to the next (model-based algorithms) or a scalar value of exponentially discounted future reward using the Bellman equation (model-free algorithms). An important drawback of model-based algorithms is that computational cost grows linearly with the amount of time to be simulated. An important drawback of model-free algorithms is the need to select a timescale required for exponential discounting. We present a computational mechanism, developed based on work in psychology and neuroscience, for computing a scale-invariant timeline of future outcomes. This mechanism efficiently computes an estimate of inputs as a function of future time on a logarithmically compressed scale and can be used to generate a scale-invariant power-law-discounted estimate of expected future reward. The representation of future time retains information about what will happen when. The entire timeline can be constructed in a single parallel operation that generates concrete behavioral and neural predictions. This computational mechanism could be incorporated into future reinforcement learning algorithms.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.06426v3},
author = {Tiganj, Zoran and Gershman, Samuel J. and Sederberg, Per B. and Howard, Marc W.},
doi = {10.1162/neco_a_01171},
eprint = {arXiv:1802.06426v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tiganj et al. - 2019 - Estimating Scale-Invariant Future in Continuous Time.pdf:pdf},
issn = {1530888X},
journal = {Neural Computation},
number = {4},
pages = {681--709},
title = {{Estimating Scale-Invariant Future in Continuous Time}},
volume = {31},
year = {2019}
}
@article{Li2014,
abstract = {The mean field algorithm is a widely used approximate inference algorithm for graphical models whose exact inference is intractable. In each iteration of mean field, the approximate marginals for each variable are updated by getting information from the neighbors. This process can be equivalently converted into a feedforward network, with each layer representing one iteration of mean field and with tied weights on all layers. This conversion enables a few natural extensions, e.g. untying the weights in the network. In this paper, we study these mean field networks (MFNs), and use them as inference tools as well as discriminative models. Preliminary experiment results show that MFNs can learn to do inference very efficiently and perform significantly better than mean field as discriminative models.},
archivePrefix = {arXiv},
arxivId = {1410.5884},
author = {Li, Yujia and Zemel, Richard},
eprint = {1410.5884},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Li, Zemel - 2014 - Mean-Field Networks.pdf:pdf},
month = {oct},
number = {2},
pages = {1--5},
title = {{Mean-Field Networks}},
url = {http://arxiv.org/abs/1410.5884},
volume = {32},
year = {2014}
}
@article{Gorski2011,
abstract = {This paper brings together work in modeling episodic memory and reinforcement learning (RL). We demonstrate that is possible to learn to use episodic memory retrievals while simultaneously learning to act in an external environment. In a series of three experiments, we investigate using RL to learn what to retrieve from episodic memory and when to retrieve it, how to use temporal episodic memory retrievals, and how to build cues that are the conjunctions of multiple features. In these experiments, our empirical results demonstrate that it is computationally feasible to learn to use episodic memory; furthermore, learning to use internal episodic memory accomplishes tasks that reinforcement learning alone cannot. These experiments also expose some important interactions that arise between reinforcement learning and episodic memory. In a fourth experiment, we demonstrate that an agent endowed with a simple bit memory cannot learn to use it effectively. This indicates that mechanistic characteristics of episodic memory may be essential to learning to use it, and that these characteristics are not shared by simpler memory mechanisms. {\textcopyright} 2010 Elsevier B.V.},
author = {Gorski, Nicholas A. and Laird, John E.},
doi = {10.1016/j.cogsys.2010.08.001},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gorski, Laird - 2011 - Learning to use episodic memory.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Artificial intelligence,Cognitive architecture,Episodic memory,Intelligent agents,Reinforcement learning},
number = {2},
pages = {144--153},
publisher = {Elsevier B.V.},
title = {{Learning to use episodic memory}},
url = {http://dx.doi.org/10.1016/j.cogsys.2010.08.001},
volume = {12},
year = {2011}
}
@article{Lewis2010,
author = {Lewis, Richard L},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lewis - 2010 - Search , Mapping , Function , State An Attempt at Clarifying De nitions 1 Background.pdf:pdf},
journal = {Psychological Science},
pages = {1--6},
title = {{Search , Mapping , Function , State An Attempt at Clarifying De nitions 1 Background}},
year = {2010}
}
@article{Wang2010,
author = {Wang, Yuyang and Khardon, Roni and Protopapas, Pavlos},
doi = {10.1007/978-3-642-15939-8_27},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Khardon, Protopapas - 2010 - Shift-invariant grouped multi-task learning for gaussian processes.pdf:pdf},
isbn = {3642159389},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {418--434},
title = {{Shift-invariant grouped multi-task learning for gaussian processes}},
volume = {6323 LNAI},
year = {2010}
}
@article{Gonzalez-Navarro2017,
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Gonzalez-Navarro, P. and Moghadamfalahi, M. and Akcakaya, M. and Erdogmus, D.},
doi = {10.1016/j.sigpro.2016.08.001},
eprint = {15334406},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gonzalez-Navarro et al. - 2017 - Spatio-temporal EEG models for brain interfaces.pdf:pdf},
isbn = {3037242094},
issn = {01651684},
journal = {Signal Processing},
keywords = {ar,auto-regressive,brain computer interface,eeg,electroencephalogram,kronecker product,linear mixture,model,multichannel,structured covariance matrices},
month = {feb},
number = {14},
pages = {333--343},
pmid = {26928661},
title = {{Spatio-temporal EEG models for brain interfaces}},
url = {http://journals.sagepub.com/doi/10.1177/0333102415576222 https://linkinghub.elsevier.com/retrieve/pii/S0165168416301876},
volume = {131},
year = {2017}
}
@article{Li2017,
abstract = {The discovery of processes for the synthesis of new materials involves many decisions about process design, operation, and material properties. Experimentation is crucial but as complexity increases, exploration of variables can become impractical using traditional combinatorial approaches. We describe an iterative method which uses machine learning to optimise process development, incorporating multiple qualitative and quantitative objectives. We demonstrate the method with a novel fluid processing platform for synthesis of short polymer fibers, and show how the synthesis process can be efficiently directed to achieve material and process objectives.},
author = {Li, Cheng and {Rub{\'{i}}n De Celis Leal}, David and Rana, Santu and Gupta, Sunil and Sutti, Alessandra and Greenhill, Stewart and Slezak, Teo and Height, Murray and Venkatesh, Svetha},
doi = {10.1038/s41598-017-05723-0},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2017 - Rapid Bayesian optimisation for synthesis of short polymer fiber materials.pdf:pdf},
isbn = {4159801705723},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--10},
title = {{Rapid Bayesian optimisation for synthesis of short polymer fiber materials}},
volume = {7},
year = {2017}
}
@article{Turner2012,
abstract = {Approximate Bayesian computation (ABC) is a simulation-based method for estimating the posterior distribution of the parameters of a model. The ABC approach is instrumental when a likelihood function for a model cannot be mathematically specified, or has a complicated form. Although difficulty in calculating a model's likelihood is extremely common, current ABC methods suffer from two problems that have largely prevented their mainstream adoption: long computation time and an inability to scale beyond a few parameters. We introduce differential evolution as a computationally efficient genetic algorithm for proposal generation in our ABC sampler. We show how using this method allows our new ABC algorithm, called ABCDE, to obtain accurate posterior estimates in fewer iterations than kernel-based ABC algorithms and to scale to high-dimensional parameter spaces that have proven difficult for current ABC methods. {\textcopyright} 2012 Elsevier Inc..},
author = {Turner, Brandon M. and Sederberg, Per B.},
doi = {10.1016/j.jmp.2012.06.004},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Turner, Sederberg - 2012 - Approximate Bayesian computation with differential evolution.pdf:pdf},
isbn = {0022-2496},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Approximate Bayesian computation,Computational modeling,Differential evolution,Likelihood-free inference},
number = {5},
pages = {375--385},
publisher = {Elsevier Inc.},
title = {{Approximate Bayesian computation with differential evolution}},
url = {http://dx.doi.org/10.1016/j.jmp.2012.06.004},
volume = {56},
year = {2012}
}
@misc{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
booktitle = {Nature},
doi = {10.1038/nature14541},
isbn = {0028-0836},
issn = {14764687},
number = {7553},
pmid = {26017444},
title = {{Probabilistic machine learning and artificial intelligence}},
volume = {521},
year = {2015}
}
@article{Klami2013,
abstract = {Canonical correlation analysis (CCA) is a classical method for seeking correlations between two multivariate data sets. During the last ten years, it has received more and more attention in the machine learning community in the form of novel computational formulations and a plethora of applications. We review recent developments in Bayesian models and inference methods for CCA which are attractive for their potential in hierarchical extensions and for coping with the combination of large dimensionalities and small sample sizes. The existing methods have not been particularly successful in fulfilling the promise yet; we introduce a novel efficient solution that imposes group-wise sparsity to estimate the posterior of an extended model which not only extracts the statistical dependencies (correlations) between data sets but also decomposes the data into shared and data set-specific components. In statistics literature the model is known as inter-battery factor analysis (IBFA), for which we now provide a Bayesian treatment. Copyright {\textcopyright} 2013 Arto Klami, Seppo Virtanen and Samuel Kaski.},
author = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Klami, Virtanen, Kaski - 2013 - Bayesian canonical correlation analysis.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian modeling,Canonical correlation analysis,Group-wise sparsity,Inter-battery factor analysis,Variational Bayesian approximation},
number = {1},
pages = {965--1003},
title = {{Bayesian canonical correlation analysis}},
volume = {14},
year = {2013}
}
@article{Kingma2014,
abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
archivePrefix = {arXiv},
arxivId = {1406.5298},
author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
eprint = {1406.5298},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Models.pdf:pdf},
pages = {1--9},
title = {{Semi-Supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
year = {2014}
}
@inproceedings{Isola2017,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
author = {Isola, Phillip and Zhu, Jun Yan and Zhou, Tinghui and Efros, Alexei A.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.632},
isbn = {9781538604571},
title = {{Image-to-image translation with conditional adversarial networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Wilson2015,
abstract = {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.},
archivePrefix = {arXiv},
arxivId = {1503.01057},
author = {Wilson, Andrew Gordon and Nickisch, Hannes},
doi = {10.1007/978-0-8176-8172-2_11},
eprint = {1503.01057},
isbn = {9781510810587},
title = {{Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)}},
year = {2015}
}
@article{Miller2016,
abstract = {We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, termed variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing the practitioner to trade computation time for accuracy. We show how to expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that resulting posterior inferences compare favorably to existing posterior approximation algorithms in both accuracy and efficiency.},
archivePrefix = {arXiv},
arxivId = {1611.06585},
author = {Miller, Andrew C. and Foti, Nicholas and Adams, Ryan P.},
eprint = {1611.06585},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Variational Boosting: Iteratively Refining Posterior Approximations}},
year = {2016}
}
@inproceedings{Mattos2016,
abstract = {We define Recurrent Gaussian Processes (RGP) models, a general family of Bayesian nonparametric models with recurrent GP priors which are able to learn dynamical patterns from sequential data. Similar to Recurrent Neural Networks (RNNs), RGPs can have different formulations for their internal states, distinct inference methods and be extended with deep structures. In such context, we propose a novel deep RGP model whose autoregressive states are latent, thereby performing representation and dynamical learning simultaneously. To fully exploit the Bayesian nature of the RGP model we develop the Recurrent Variational Bayes (REVARB) framework, which enables efficient inference and strong regularization through coherent propagation of uncertainty across the RGP layers and states. We also introduce a RGP extension where variational parameters are greatly reduced by being reparametrized through RNN-based sequential recognition models. We apply our model to the tasks of nonlinear system identification and human motion modeling. The promising obtained results indicate that our RGP model maintains its highly flexibility while being able to avoid overfitting and being applicable even when larger datasets are not available.},
archivePrefix = {arXiv},
arxivId = {1511.06644},
author = {Mattos, C{\'{e}}sar Lincoln C. and Dai, Zhenwen and Damianou, Andreas and Forth, Jeremy and Barreto, Guilherme A. and Lawrence, Neil D.},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.06644},
title = {{Recurrent Gaussian processes}},
year = {2016}
}
@article{Strathmann2015,
abstract = {We propose Kernel Hamiltonian Monte Carlo (KMC), a gradient-free adaptive MCMC algorithm based on Hamiltonian Monte Carlo (HMC). On target densities where classical HMC is not an option due to intractable gradients, KMC adaptively learns the target's gradient structure by fitting an exponential family model in a Reproducing Kernel Hilbert Space. Computational costs are reduced by two novel efficient approximations to this gradient. While being asymptotically exact, KMC mimics HMC in terms of sampling efficiency, and offers substantial mixing improvements over state-of-the-art gradient free samplers. We support our claims with experimental studies on both toy and real-world applications, including Approximate Bayesian Computation and exact-approximate MCMC.},
archivePrefix = {arXiv},
arxivId = {1506.02564},
author = {Strathmann, Heiko and Sejdinovic, Dino and Livingstone, Samuel and Szabo, Zoltan and Gretton, Arthur},
eprint = {1506.02564},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Strathmann et al. - 2015 - Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families.pdf:pdf},
issn = {10495258},
pages = {1--20},
title = {{Gradient-free Hamiltonian Monte Carlo with Efficient Kernel Exponential Families}},
url = {http://arxiv.org/abs/1506.02564},
year = {2015}
}
@book{Kleyko2016,
abstract = {Pattern recognition is an area constantly enlarging its theoretical and practical horizons. Applications of pattern recognition and machine learning can be found in many areas of the present day world including health-care, robotics, manufacturing, economics, automation, transportation, etc. Despite some success in many domains pattern recognition algorithms are still far from being close to their biological vis-a-vis – human brain. New possibilities in the area of pattern recognition may be achieved by application of biologically inspired approaches. This thesis presents the usage of a bio-inspired method of representing concepts and their meaning – Vector Symbolic Architectures – in the context of pattern recognition with possible applications in intelligent transportation systems, automation systems, and language processing. Vector Symbolic Architectures is an approach for encoding and manipulating distributed representations of information. They have previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information. First, it is shown that Vector Symbolic Architectures are capable of pattern classification of temporal patterns. With this approach, it is possible to represent, learn and subsequently classify vehicles using measurements from vibration sensors. Next, an architecture called Holographic Graph Neuron for one-shot learning of patterns of generic sensor stimuli is proposed. The architecture is based on implementing the Hierarchical Graph Neuron approach using Vector Symbolic Architectures. Holographic Graph Neuron shows the previously reported performance characteristics of Hierarchical Graph Neuron while maintaining the simplicity of its design. The Holographic Graph Neuron architecture is applied in two domains: fault detection and longest common substrings search. In the area of fault detection the architecture showed superior performance compared to classical methods of artificial intelligence while featuring zero configuration and simple operations. The application of the architecture for longest common substrings search showed its ability to robustly solve the task given that the length of a common substring is longer than 4{\%} of the longest pattern. Furthermore, the required number of operations on binary vectors is equal to the suffix trees approach, which is the fastest traditional algorithm for this problem. In summary, the work presented in this thesis extends understanding of the performance proprieties of distributed representations and opens the way for new applications.},
author = {Kleyko, Denis},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kleyko - 2016 - Pattern Recognition with Vector Symbolic Architectures.pdf:pdf},
isbn = {978-91-7583-536-5},
pages = {136},
title = {{Pattern Recognition with Vector Symbolic Architectures}},
year = {2016}
}
@article{Liu2005,
abstract = {Receiver operating characteristic (ROC) curve is an effective and widely used method for evaluating the discriminating power of a diagnostic test or statistical model. As a useful statistical method, a wealth of literature about its theories and computation methods has been established. There search on ROC curves, however, has focused mainly on cross-sectional design.Very little research on estimating ROC curves and their summary statistics, especially signiﬁcance testing, has been conducted for repeated measures design. Due to the complexity of estimating the standard error of a ROC curve, there is no currently established statistical method for testing the signiﬁcance of ROC curves under a repeated measures design. In this paper, we estimate the area of a ROC curve under a repeated measures design through generalized linear mixed model (GLMM) using the predicted probability of a disease or positivity of a condition and propose a bootstrap method to estimate the standard error of the are aunder a ROC curve for such designs. Statistical signiﬁcance testing of the are a under a ROC curve is then conducted using the bootstrapped standard error. The validity of bootstrap approach and the statistical testing of the area under the ROC curve was validated through simulation analyses. A specialstatistical soft- ware written in SAS/IML/MACRO v8 was also created for implementing the bootstrapping algorithm, conducting the calculations and statistical testing.},
author = {Liu, Honghu and Li, Gang},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Li - 2005 - Testing Statistical Significance of the Area under a Receiving Operating Characteristics Curve for Repeated Measures De.pdf:pdf},
journal = {Journal of Data Science},
keywords = {area under roc curve,bootstrapping,generalized linear mixed,glmm,model,simulation,standard error},
pages = {257--278},
title = {{Testing Statistical Significance of the Area under a Receiving Operating Characteristics Curve for Repeated Measures Design with Bootstrapping}},
url = {http://www.jds-online.com/file{\_}download/79/JDS-206.pdf},
volume = {3},
year = {2005}
}
@article{Dai2017b,
abstract = {Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) and that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP, of which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.},
archivePrefix = {arXiv},
arxivId = {1705.09862},
author = {Dai, Zhenwen and {\'{A}}lvarez, Mauricio A. and Lawrence, Neil D.},
eprint = {1705.09862},
month = {may},
title = {{Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes}},
url = {http://arxiv.org/abs/1705.09862},
year = {2017}
}
@article{Carpenter2017,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus A. and Guo, Jiqiang and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
file = {:Users/mshvarts/Downloads/v76i01.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {Algorithmic differentiation,Bayesian inference,Probabilistic program,Stan},
number = {1},
title = {{Stan: A probabilistic programming language}},
volume = {76},
year = {2017}
}
@article{Hyun2016,
author = {Hyun, Jung Won and Li, Yimei and Huang, Chao and Styner, Martin and Lin, Weili and Zhu, Hongtu},
doi = {10.1016/j.neuroimage.2016.04.023},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hyun et al. - 2016 - STGP Spatio-temporal Gaussian process models for longitudinal neuroimaging data.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
month = {jul},
number = {24},
pages = {550--562},
title = {{STGP: Spatio-temporal Gaussian process models for longitudinal neuroimaging data}},
url = {http://doi.wiley.com/10.1002/cncr.27633 https://linkinghub.elsevier.com/retrieve/pii/S105381191630057X},
volume = {134},
year = {2016}
}
@article{Gal2015b,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1506.02142},
isbn = {1506.02142},
issn = {10414347},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
year = {2015}
}
@article{Pan2018,
abstract = {We study the use of feedforward neural networks (FNN) to develop models of nonlinear dynamical systems from data. Emphasis is placed on predictions at long times, with limited data availability. Inspired by global stability analysis, and the observation of the strong correlation between the local error and the maximum singular value of the Jacobian of the ANN, we introduce Jacobian regularization in the loss function. This regularization suppresses the sensitivity of the prediction to the local error and is shown to improve accuracy and robustness. Comparison between the proposed approach and sparse polynomial regression is presented in numerical examples ranging from simple ODE systems to nonlinear PDE systems including vortex shedding behind a cylinder, and instability-driven buoyant mixing flow. Furthermore, limitations of feedforward neural networks are highlighted, especially when the training data does not include a low dimensional attractor. Strategies of data augmentation are presented as remedies to address these issues to a certain extent.},
archivePrefix = {arXiv},
arxivId = {1805.12547},
author = {Pan, Shaowu and Duraisamy, Karthik},
doi = {10.1068/a3496},
eprint = {1805.12547},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pan, Duraisamy - 2018 - Long-time predictive modeling of nonlinear dynamical systems using neural networks.pdf:pdf},
keywords = {artificial neural networks,data-driven modeling,dynamical system modeling},
title = {{Long-time predictive modeling of nonlinear dynamical systems using neural networks}},
url = {http://arxiv.org/abs/1805.12547},
year = {2018}
}
@article{Howard2018a,
abstract = {Evidence accumulation models of simple decision-making have long assumed that the brain estimates a scalar decision variable corresponding to the log-likelihood ratio of the two alternatives. Typical neural implementations of this algorithmic cognitive model assume that large numbers of neurons are each noisy exemplars of the scalar decision variable. Here we propose a neural implementation of the diffusion model in which many neurons construct and maintain the Laplace transform of the distance to each of the decision bounds. As in classic findings from brain regions including LIP, the firing rate of neurons coding for the Laplace transform of net accumulated evidence grows to a bound during random dot motion tasks. However, rather than noisy exemplars of a single mean value, this approach makes the novel prediction that firing rates grow to the bound exponentially, across neurons there should be a distribution of different rates. A second set of neurons records an approximate inversion of the Laplace transform, these neurons directly estimate net accumulated evidence. In analogy to time cells and place cells observed in the hippocampus and other brain regions, the neurons in this second set have receptive fields along a "decision axis." This finding is consistent with recent findings from rodent recordings. This theoretical approach places simple evidence accumulation models in the same mathematical language as recent proposals for representing time and space in cognitive models for memory.},
archivePrefix = {arXiv},
arxivId = {1806.04122},
author = {Howard, Marc W. and Luzardo, Andre and Tiganj, Zoran},
eprint = {1806.04122},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Luzardo, Tiganj - 2018 - Evidence accumulation in a Laplace domain decision space(2).pdf:pdf},
keywords = {Evidence accumulation,Diffusion model,Laplace tran,behavior without regard to,chology to account for,cognition that are most,developed in mathematical psy-,diffusion model,evidence accumulation,influen-,laplace transform,neural,neurophysiological models of cognition,the computational models of,tial on neuroscience were},
pages = {237--251},
publisher = {Computational Brain {\&} Behavior},
title = {{Evidence accumulation in a Laplace domain decision space}},
url = {http://arxiv.org/abs/1806.04122},
year = {2018}
}
@article{Pascanu2012,
archivePrefix = {arXiv},
arxivId = {1301.3584v7},
author = {Pascanu, Razvan and Bengio, Yoshua},
eprint = {1301.3584v7},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pascanu, Bengio - 2012 - Revisiting natural gradient for deep networks.pdf:pdf},
pages = {1--18},
title = {{Revisiting natural gradient for deep networks}},
year = {2012}
}
@article{Zacks2007,
abstract = {One way to understand something is to break it up into parts. New research indicates that segmenting ongoing activity into meaningful events is a core component of perception and that this has consequences for memory and learning. Behavioral and neuroimaging data suggest that event segmentation is automatic and that people spontaneously segment activity into hierarchically organized parts and subparts. This segmentation depends on the bottom-up processing of sensory features such as movement and on the top-down processing of conceptual features such as actors' goals. How people segment activity affects what they remember later; as a result, those who identify appropriate event boundaries during perception tend to remember more and to learn more proficiently. Copyright {\textcopyright} 2007 Association for Psychological Science.},
author = {Zacks, Jeffrey M. and Swallow, Khena M.},
doi = {10.1111/j.1467-8721.2007.00480.x},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zacks, Swallow - 2007 - Event segmentation.pdf:pdf},
issn = {09637214},
journal = {Current Directions in Psychological Science},
keywords = {Event perception,Intentions,Motion,Segmentation},
number = {2},
pages = {80--84},
title = {{Event segmentation}},
volume = {16},
year = {2007}
}
@article{Villacampa-Calvo2017,
abstract = {This paper describes an expectation propagation (EP) method for multi-class classification with Gaussian processes that scales well to very large datasets. In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances. This enables efficient training using stochastic gradients and mini-batches. When this type of training is used, the computational cost does not depend on the number of data instances {\$}N{\$}. Furthermore, extra assumptions in the approximate inference process make the memory cost independent of {\$}N{\$}. The consequence is that the proposed EP method can be used on datasets with millions of instances. We compare empirically this method with alternative approaches that approximate the required computations using variational inference. The results show that it performs similar or even better than these techniques, which sometimes give significantly worse predictive distributions in terms of the test log-likelihood. Besides this, the training process of the proposed approach also seems to converge in a smaller number of iterations.},
archivePrefix = {arXiv},
arxivId = {1706.07258},
author = {Villacampa-Calvo, Carlos and Hern{\'{a}}ndez-Lobato, Daniel},
eprint = {1706.07258},
isbn = {9781510855144},
title = {{Scalable Multi-Class Gaussian Process Classification using Expectation Propagation}},
year = {2017}
}
@article{Daume2004,
abstract = {Unpublished paper},
author = {Daume, Hal},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Daume - 2004 - From Zero to Reproducing Kernel Hilbert Spaces in Twelve Pages or Less.pdf:pdf},
journal = {Most},
number = {February},
pages = {1--12},
title = {{From Zero to Reproducing Kernel Hilbert Spaces in Twelve Pages or Less}},
year = {2004}
}
@article{Martens2018,
abstract = {Gaussian Process Regression (GPR) and Gaussian Process Latent Variable Models (GPLVM) offer a principled way of performing probabilistic non-linear regression and dimensionality reduction. In this paper we propose a hybrid between the two, the covariate-GPLVM (c-GPLVM), to perform dimensionality reduction in the presence of covariate information (e.g. continuous covariates, class labels, or censored survival times). This construction lets us adjust for covariate effects and reveals meaningful latent structure which is not revealed when using GPLVM. Furthermore, we introduce structured decomposable kernels which will let us interpret how the fixed and latent inputs contribute to feature-level variation, e.g. identify the presence of a non-linear interaction. We demonstrate the utility of this model on applications in disease progression modelling from high-dimensional gene expression data in the presence of additional phenotypes.},
archivePrefix = {arXiv},
arxivId = {1810.06983},
author = {M{\"{a}}rtens, Kaspar and Campbell, Kieran R. and Yau, Christopher},
eprint = {1810.06983},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/M{\"{a}}rtens, Campbell, Yau - 2018 - Covariate Gaussian Process Latent Variable Models.pdf:pdf},
pages = {1--15},
title = {{Covariate Gaussian Process Latent Variable Models}},
url = {http://arxiv.org/abs/1810.06983},
year = {2018}
}
@article{Wang2017,
abstract = {A key challenge for modern Bayesian statistics is how to perform scalable inference of posterior distributions. To address this challenge, variational Bayes (VB) methods have emerged as a popular alternative to the classical Markov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while achieving comparable predictive performance. However, there are few theoretical results around VB. In this paper, we establish frequentist consistency and asymptotic normality of VB methods. Specifically, we connect VB methods to point estimates based on variational approximations, called frequentist variational approximations, and we use the connection to prove a variational Bernstein-von Mises theorem. The theorem leverages the theoretical characterizations of frequentist variational approximations to understand asymptotic properties of VB. In summary, we prove that (1) the VB posterior converges to the Kullback-Leibler (KL) minimizer of a normal distribution, centered at the truth and (2) the corresponding variational expectation of the parameter is consistent and asymptotically normal. As applications of the theorem, we derive asymptotic properties of VB posteriors in Bayesian mixture models, Bayesian generalized linear mixed models, and Bayesian stochastic block models. We conduct a simulation study to illustrate these theoretical results.},
archivePrefix = {arXiv},
arxivId = {1705.03439},
author = {Wang, Yixin and Blei, David M.},
doi = {10.1080/01621459.2018.1473776},
eprint = {1705.03439},
issn = {1537274X},
title = {{Frequentist Consistency of Variational Bayes}},
year = {2017}
}
@article{Snelson,
abstract = {We generalise the Gaussian process (GP) framework for regression by learning a nonlinear transformation of the GP outputs. This allows for non-Gaussian processes and non-Gaussian noise. The learning algorithm chooses a nonlinear transformation such that transformed data is well-modelled by a GP. This can be seen as including a preprocessing transformation as an integral part of the probabilistic modelling problem, rather than as an ad-hoc step. We demonstrate on several real regression problems that learning the transformation can lead to significantly better performance than using a regular GP, or a GP with a fixed transformation.},
author = {Snelson, Edward},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Snelson - Unknown - Warped Gaussian Processes.pdf:pdf},
title = {{Warped Gaussian Processes}}
}
@article{Yu2016,
abstract = {Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications.},
archivePrefix = {arXiv},
arxivId = {1607.02535},
author = {Yu, Rose and Liu, Yan},
eprint = {1607.02535},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Liu - 2016 - Learning from Multiway Data Simple and Efficient Tensor Regression.pdf:pdf},
isbn = {9781510829008},
issn = {9781510829008},
month = {jul},
title = {{Learning from Multiway Data: Simple and Efficient Tensor Regression}},
url = {http://arxiv.org/abs/1607.02535},
volume = {48},
year = {2016}
}
@article{Jakel2009,
abstract = {Kernel methods are among the most successful tools in machine learning and are used in challenging data analysis problems in many disciplines. Here we provide examples where kernel methods have proven to be powerful tools for analyzing behavioral data, especially for identifying features in categorization experiments. We also demonstrate that kernel methods relate to perceptrons and exemplar models of categorization. Hence, we argue that kernel methods have neural and psychological plausibility, and theoretical results concerning their behavior are therefore potentially relevant for human category learning. In particular, we believe kernel methods have the potential to provide explanations ranging from the implementational via the algorithmic to the computational level. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {J{\"{a}}kel, Frank and Sch{\"{o}}lkopf, Bernhard and Wichmann, Felix A.},
doi = {10.1016/j.tics.2009.06.002},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/J{\"{a}}kel, Sch{\"{o}}lkopf, Wichmann - 2009 - Does Cognitive Science Need Kernels.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {381--388},
title = {{Does Cognitive Science Need Kernels?}},
volume = {13},
year = {2009}
}
@inproceedings{Chaudhari2018,
abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such "out-of-equilibrium" behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1{\%} of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
archivePrefix = {arXiv},
arxivId = {1710.11029},
author = {Chaudhari, Pratik and Soatto, Stefano},
booktitle = {2018 Information Theory and Applications Workshop, ITA 2018},
doi = {10.1109/ITA.2018.8503224},
eprint = {1710.11029},
isbn = {9781728101248},
title = {{Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks}},
year = {2018}
}
@article{Parra2003,
abstract = {In this short note we highlight the fact that linear blind source separation can be formulated as a generalized eigenvalue decomposition under the assumptions of non-Gaussian, non-stationary, or non-white independent sources. The solution for the unmixing matrix is given by the generalized eigenvectors that simultaneously diagonalize the covariance matrix of the observations and an additional symmetric matrix whose form depends upon the particular assumptions. The method critically determines the mixture coefficients and is therefore not robust to estimation errors. However it provides a rather general and unified solution that summarizes the conditions for successful blind source separation. To demonstrate the method, which can be implemented in two lines of matlab code, we present results for artificial mixtures of speech and real mixtures of electroencephalography (EEG) data, showing that the same sources are recovered under the various assumptions.},
author = {Parra, Lucas and Sajda, Paul},
doi = {10.1162/jmlr.2003.4.7-8.1261},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Parra, Sajda - 2003 - Blind Source Separation via Generalized Eigenvalue Decomposition.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {blind source separation,generalized eigenvalue decomposition,non,non gaussian,non stationary,white},
number = {7-8},
pages = {1261--1269},
title = {{Blind Source Separation via Generalized Eigenvalue Decomposition}},
url = {http://www.crossref.org/jmlr{\_}DOI.html},
volume = {4},
year = {2003}
}
@article{Anand2018,
abstract = {We explore blindfold (question-only) baselines for Embodied Question Answering. The EmbodiedQA task requires an agent to answer a question by intelligently navigating in a simulated environment, gathering necessary visual information only through first-person vision before finally answering. Consequently, a blindfold baseline which ignores the environment and visual information is a degenerate solution, yet we show through our experiments on the EQAv1 dataset that a simple question-only baseline achieves state-of-the-art results on the EmbodiedQA task in all cases except when the agent is spawned extremely close to the object.},
archivePrefix = {arXiv},
arxivId = {1811.05013},
author = {Anand, Ankesh and Belilovsky, Eugene and Kastner, Kyle and Larochelle, Hugo and Courville, Aaron},
eprint = {1811.05013},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Anand et al. - 2018 - Blindfold Baselines for Embodied QA.pdf:pdf},
isbn = {1811.05013v1},
pages = {1--6},
title = {{Blindfold Baselines for Embodied QA}},
url = {http://arxiv.org/abs/1811.05013},
year = {2018}
}
@article{Yeung2017,
abstract = {The Koopman operator has recently garnered much attention for its value in dynamical systems analysis and data-driven model discovery. However, its application has been hindered by the computational complexity of extended dynamic mode decomposition; this requires a combinatorially large basis set to adequately describe many nonlinear systems of interest, e.g. cyber-physical infrastructure systems, biological networks, social systems, and fluid dynamics. Often the dictionaries generated for these problems are manually curated, requiring domain-specific knowledge and painstaking tuning. In this paper we introduce a deep learning framework for learning Koopman operators of nonlinear dynamical systems. We show that this novel method automatically selects efficient deep dictionaries, outperforming state-of-the-art methods. We benchmark this method on partially observed nonlinear systems, including the glycolytic oscillator and show it is able to predict quantitatively 100 steps into the future, using only a single timepoint, and qualitative oscillatory behavior 400 steps into the future.},
archivePrefix = {arXiv},
arxivId = {1708.06850},
author = {Yeung, Enoch and Kundu, Soumya and Hodas, Nathan},
eprint = {1708.06850},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yeung, Kundu, Hodas - 2017 - Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems.pdf:pdf},
title = {{Learning Deep Neural Network Representations for Koopman Operators of Nonlinear Dynamical Systems}},
url = {http://arxiv.org/abs/1708.06850},
year = {2017}
}
@article{Takeishi2017a,
abstract = {Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1710.04340v2},
author = {Takeishi, Naoya and Kawahara, Yoshinobu and Yairi, Takehisa},
eprint = {arXiv:1710.04340v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Takeishi, Kawahara, Yairi - 2017 - Learning Koopman invariant subspaces for dynamic mode decomposition.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1131--1141},
title = {{Learning Koopman invariant subspaces for dynamic mode decomposition}},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco and Singh, Sameer and Guestrin, Carlos},
booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations},
doi = {10.18653/v1/N16-3020},
eprint = {1602.04938},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - “Why Should I Trust You” Explaining the Predictions of Any Classifier.pdf:pdf},
isbn = {9781450321389},
issn = {9781450321389},
pages = {97--101},
pmid = {214160309},
publisher = {Association for Computational Linguistics},
title = {{“Why Should I Trust You?”: Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938 http://aclweb.org/anthology/N16-3020},
year = {2016}
}
@article{Riemer2018,
abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
archivePrefix = {arXiv},
arxivId = {1810.11910},
author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
eprint = {1810.11910},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Riemer et al. - 2018 - Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference.pdf:pdf},
month = {oct},
pages = {1--27},
title = {{Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference}},
url = {http://arxiv.org/abs/1810.11910},
year = {2018}
}
@article{Duncker2019a,
abstract = {We develop an approach to learn an interpretable semi-parametric model of a latent continuous-time stochastic dynamical system, assuming noisy high-dimensional outputs sampled at uneven times. The dynamics are described by a nonlinear stochastic differential equation (SDE) driven by a Wiener process, with a drift evolution function drawn from a Gaussian process (GP) conditioned on a set of learnt fixed points and corresponding local Jacobian matrices. This form yields a flexible nonparametric model of the dynamics, with a representation corresponding directly to the interpretable portraits routinely employed in the study of nonlinear dynamical systems. The learning algorithm combines inference of continuous latent paths underlying observed data with a sparse variational description of the dynamical process. We demonstrate our approach on simulated data from different nonlinear dynamical systems.},
archivePrefix = {arXiv},
arxivId = {1902.04420},
author = {Duncker, Lea and Bohner, Gergo and Boussard, Julien and Sahani, Maneesh},
eprint = {1902.04420},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Duncker et al. - 2019 - Learning interpretable continuous-time models of latent stochastic dynamical systems(2).pdf:pdf},
month = {feb},
title = {{Learning interpretable continuous-time models of latent stochastic dynamical systems}},
url = {http://arxiv.org/abs/1902.04420},
year = {2019}
}
@article{Hannagan2012,
abstract = {It has been recently argued that some machine learning techniques known as Kernel methods could be relevant for capturing cognitive and neural mechanisms (J{\"{a}}kel, Sch{\"{o}}lkopf,  {\&}  Wichmann, 2009). We point out that ''String kernels,'' initially designed for protein function prediction and spam detection, are virtually identical to one contending proposal for how the brain encodes orthographic information during reading. We suggest some reasons for this connection and we derive new ideas for visual word recognition that are successfully put to the test. We argue that the versatility and performance of String kernels makes a compelling case for their implementation in the brain. {\textcopyright} 2012 Cognitive Science Society, Inc.},
author = {Hannagan, Thomas and Grainger, Jonathan},
doi = {10.1111/j.1551-6709.2012.01236.x},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hannagan, Grainger - 2012 - Protein Analysis Meets Visual Word Recognition A Case for String Kernels in the Brain.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Open-bigrams,Orthographic coding,String kernels,Visual Word Form Area,Visual word recognition},
number = {4},
pages = {575--606},
title = {{Protein Analysis Meets Visual Word Recognition: A Case for String Kernels in the Brain}},
volume = {36},
year = {2012}
}
@inproceedings{Chatzis2015,
abstract = {In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks. Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80$\backslash${\%}. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks. We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.},
author = {Chatzis, Sotirios P.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-23525-7_22},
isbn = {9783319235240},
issn = {16113349},
title = {{Sparse bayesian recurrent neural networks}},
volume = {9285},
year = {2015}
}
@article{Bratman2012,
abstract = {Recent work has defined an optimal reward problem (ORP) in which an agent designer, with an objective reward func-tion that evaluates an agent's behavior, has a choice of what reward function to build into a learning or planning agent to guide its behavior. Existing results on ORP show weak miti-gation of limited computational resources, i.e., the existence of reward functions so that agents when guided by them do better than when guided by the objective reward function. These existing results ignore the cost of finding such good reward functions. We define a nested optimal reward and control architecture that achieves strong mitigation of lim-ited computational resources. We show empirically that the designer is better off using the new architecture that spends some of its limited resources learning a good reward function instead of using all of its resources to optimize its behavior with respect to the objective reward function.},
author = {Bratman, Jeshua and Singh, Satinder and Lewis, Richard L. and Sorg, Jonathan},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bratman et al. - 2012 - Strong mitigation nesting search for good policies within search for good reward.pdf:pdf},
isbn = {0-9817381-1-7, 978-0-9817381-1-6},
journal = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
keywords = {planning,reinforcement learning},
pages = {407--414},
title = {{Strong mitigation: nesting search for good policies within search for good reward}},
url = {http://dl.acm.org/citation.cfm?id=2343576.2343634},
year = {2012}
}
@article{Posner2016,
author = {Posner, Michael I and Rothbart, Mary K},
doi = {10.1146/annurev.psych.58.110405.085516},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Posner, Rothbart - 2016 - Research on Attention Networks as a Model for the Integration of Psychological Science.pdf:pdf},
keywords = {attention,candidate genes,neural networks,orienting},
number = {September},
pages = {1--23},
title = {{Research on Attention Networks as a Model for the Integration of Psychological Science}},
year = {2016}
}
@article{Nuxoll2012,
abstract = {For a human, episodic memory is a memory of past experiences that one gains over a lifetime. While episodic memory appears critical to human function, researchers have done little to explore the potential benefits for an artificially intelligent agent. In this research, we have added a task-independent, episodic memory to a cognitive architecture. To frame the research, we propose that episodic memory supports a set of cognitive capabilities that improve an agent's ability to sense its environment, reason, and learn. We demonstrate that episodic memory enables agents created with our architecture to employ these cognitive capabilities. {\textcopyright} 2011 Elsevier B.V.},
author = {Nuxoll, Andrew M. and Laird, John E.},
doi = {10.1016/j.cogsys.2011.10.002},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nuxoll, Laird - 2012 - Enhancing intelligent agents with episodic memory.pdf:pdf},
issn = {13890417},
journal = {Cognitive Systems Research},
keywords = {Cognitive architecture,Episodic memory,Soar},
pages = {34--48},
publisher = {Elsevier B.V.},
title = {{Enhancing intelligent agents with episodic memory}},
url = {http://dx.doi.org/10.1016/j.cogsys.2011.10.002},
volume = {17-18},
year = {2012}
}
@article{Rogers,
author = {Rogers, Mark and Russell, Stuart},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rogers, Russell - Unknown - Multilinear Dynamical Systems for Tensor Time Series.pdf:pdf},
pages = {1--9},
title = {{Multilinear Dynamical Systems for Tensor Time Series}}
}
@article{Kanagawa2018,
abstract = {This paper is an attempt to bridge the conceptual gaps between researchers working on the two widely used approaches based on positive definite kernels: Bayesian learning or inference using Gaussian processes on the one side, and frequentist kernel methods based on reproducing kernel Hilbert spaces on the other. It is widely known in machine learning that these two formalisms are closely related; for instance, the estimator of kernel ridge regression is identical to the posterior mean of Gaussian process regression. However, they have been studied and developed almost independently by two essentially separate communities, and this makes it difficult to seamlessly transfer results between them. Our aim is to overcome this potential difficulty. To this end, we review several old and new results and concepts from either side, and juxtapose algorithmic quantities from each framework to highlight close similarities. We also provide discussions on subtle philosophical and theoretical differences between the two approaches.},
archivePrefix = {arXiv},
arxivId = {1807.02582},
author = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K},
eprint = {1807.02582},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kanagawa et al. - 2018 - Gaussian Processes and Kernel Methods A Review on Connections and Equivalences.pdf:pdf},
pages = {1--64},
title = {{Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences}},
url = {http://arxiv.org/abs/1807.02582},
year = {2018}
}
@article{Gutmann2018,
abstract = {Increasingly complex generative models are being used across disciplines as they allow for realistic characterization of data, but a common difficulty with them is the prohibitively large computational cost to evaluate the likelihood function and thus to perform likelihood-based statistical inference. A likelihood-free inference framework has emerged where the parameters are identified by finding values that yield simulated data resembling the observed data. While widely applicable, a major difficulty in this framework is how to measure the discrepancy between the simulated and observed data. Transforming the original problem into a problem of classifying the data into simulated versus observed, we find that classification accuracy can be used to assess the discrepancy. The complete arsenal of classification methods becomes thereby available for inference of intractable generative models. We validate our approach using theory and simulations for both point estimation and Bayesian inference, and demonstrate its use on real data by inferring an individual-based epidemiological model for bacterial infections in child care centers.},
archivePrefix = {arXiv},
arxivId = {1407.4981},
author = {Gutmann, Michael U. and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
doi = {10.1007/s11222-017-9738-6},
eprint = {1407.4981},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gutmann et al. - 2018 - Likelihood-free inference via classification.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Approximate Bayesian computation,Generative models,Intractable likelihood,Latent variable models,Simulator-based models},
number = {2},
pages = {411--425},
title = {{Likelihood-free inference via classification}},
volume = {28},
year = {2018}
}
@article{Ding2017,
abstract = {Gaussian process regression generally does not scale to beyond a few thousands data points without applying some sort of kernel approximation method. Most approximations focus on the high eigenvalue part of the spectrum of the kernel matrix, {\$}K{\$}, which leads to bad performance when the length scale of the kernel is small. In this paper we introduce Multiresolution Kernel Approximation (MKA), the first true broad bandwidth kernel approximation algorithm. Important points about MKA are that it is memory efficient, and it is a direct method, which means that it also makes it easy to approximate {\$}K{\^{}}{\{}-1{\}}{\$} and {\$}\backslashmathop{\{}\backslashtextrm{\{}det{\}}{\}}(K){\$}.},
archivePrefix = {arXiv},
arxivId = {1708.02183},
author = {Ding, Yi and Kondor, Risi and Eskreis-Winkler, Jonathan},
eprint = {1708.02183},
issn = {10495258},
title = {{Multiresolution Kernel Approximation for Gaussian Process Regression}},
year = {2017}
}
@article{Gal2015c,
abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
archivePrefix = {arXiv},
arxivId = {1512.05287},
author = {Gal, Yarin and Ghahramani, Zoubin},
doi = {10.1201/9781420049176},
eprint = {1512.05287},
isbn = {9789537619084},
issn = {0302-9743},
pmid = {21803542},
title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
year = {2015}
}
@article{Lee2017b,
abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
archivePrefix = {arXiv},
arxivId = {1711.00165},
author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
eprint = {1711.00165},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2017 - Deep Neural Networks as Gaussian Processes.pdf:pdf},
title = {{Deep Neural Networks as Gaussian Processes}},
url = {http://arxiv.org/abs/1711.00165},
year = {2017}
}
@article{Khamis2014,
abstract = {In this paper, we focus on computing a consistent traffic signal config-uration at each junction that optimizes multiple performance indices, i.e., multi-objective traffic signal control. The multi-objective function includes minimizing trip waiting time, total trip time, and junction waiting time. Moreover, the multi-objective function includes maximizing flow rate, satis-fying green waves for platoons traveling in main roads, avoiding accidents especially in residential areas, and forcing vehicles to move within moderate speed range of minimum fuel consumption. In particular, we formulate our multi-objective traffic signal control as a Multi-Agent System (MAS). Traf-fic signal controllers have a distributed nature in which each traffic signal agent acts individually and possibly cooperatively in a MAS. In addition, agents act autonomously according to the current traffic situation without any human intervention. Thus, we develop a multi-agent multi-objective Re-inforcement Learning (RL) traffic signal control framework that simulates the driver's behavior (acceleration/deceleration) continuously in space and time dimensions. The proposed framework is based on a multi-objective se-quential decision making process whose parameters are estimated based on the Bayesian interpretation of probability. Using this interpretation together * Corresponding author. Mobile: +20-100-638-2428; telephone: +20-3-309-4075. with a novel adaptive cooperative exploration technique, the proposed traffic signal controller can make real-time adaptation in the sense that it responds effectively to the changing road dynamics. These road dynamics are simu-lated by the Green Light District (GLD) vehicle traffic simulator that is the testbed of our traffic signal control. We have implemented the Intelligent Driver Model (IDM) acceleration model in the GLD traffic simulator. The change in road conditions is modeled by varying the traffic demand proba-bility distribution and adapting the IDM parameters to the adverse weather conditions. Under the congested and free traffic situations, the proposed multi-objective controller significantly outperforms the underlying single ob-jective controller which only minimizes the trip waiting time (i.e., the total waiting time in the whole vehicle trip rather than at a specific junction). For instance, the average trip and waiting times are lower 8 and 6 times respectively when using the multi-objective controller.},
author = {Khamis, Mohamed A and Gomaa, Walid},
doi = {10.1016/j.engappai.2014.01.007},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Khamis, Gomaa - 2014 - Adaptive Multi-objective Reinforcement Learning with Hybrid Exploration for Traffic Signal Control Based on Coope.pdf:pdf},
issn = {0952-1976},
journal = {Eng. Appl. Artif. Intell.},
keywords = {Adaptive optimization,Cooperative multi-agent system,Exploration,Multi-objective optimization,Reinforcement learning,Traffic signal control},
pages = {134--151},
title = {{Adaptive Multi-objective Reinforcement Learning with Hybrid Exploration for Traffic Signal Control Based on Cooperative Multi-agent Framework}},
url = {http://dx.doi.org/10.1016/j.engappai.2014.01.007},
volume = {29},
year = {2014}
}
@article{Chen2018,
abstract = {We present a novel framework for iterative visual reasoning. Our framework goes beyond current recognition systems that lack the capability to reason beyond stack of convolutions. The framework consists of two core modules: a local module that uses spatial memory to store previous beliefs with parallel updates; and a global graph-reasoning module. Our graph module has three components: a) a knowledge graph where we represent classes as nodes and build edges to encode different types of semantic relationships between them; b) a region graph of the current image where regions in the image are nodes and spatial relationships between these regions are edges; c) an assignment graph that assigns regions to classes. Both the local module and the global module roll-out iteratively and cross-feed predictions to each other to refine estimates. The final predictions are made by combining the best of both modules with an attention mechanism. We show strong performance over plain ConvNets, $\backslash$eg achieving an {\$}8.4\backslash{\%}{\$} absolute improvement on ADE measured by per-class average precision. Analysis also shows that the framework is resilient to missing regions for reasoning.},
archivePrefix = {arXiv},
arxivId = {1803.11189},
author = {Chen, Xinlei and Li, Li-Jia and Fei-Fei, Li and Gupta, Abhinav},
doi = {10.1109/CVPR.2018.00756},
eprint = {1803.11189},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2018 - Iterative Visual Reasoning Beyond Convolutions.pdf:pdf},
title = {{Iterative Visual Reasoning Beyond Convolutions}},
url = {http://arxiv.org/abs/1803.11189},
year = {2018}
}
@article{Yu2013,
abstract = {We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from many neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional, noisy spiking activity in a compact form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the spike trains are first smoothed over time, then a static dimensionality-reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way and that account for spiking variability, which may vary both across neurons and across time. We then present a novel method for extracting neural trajectories-Gaussian-process factor analysis (GPFA)-which unifies the smoothing and dimensionality-reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that the proposed extensions improved the predictive ability of the two-stage methods. The predictive ability was further improved by going to GPFA. From the extracted trajectories, we directly observed a convergence in neural state during motor planning, an effect that was shown indirectly by previous studies. We then show how such methods can be a powerful tool for relating the spiking activity across a neural population to the subject's behavior on a single-trial basis. Finally, to assess how well the proposed methods characterize neural population activity when the underlying time course is known, we performed simulations that revealed that GPFA performed tens of percent better than the best two-stage method.},
author = {Yu, Byron M and Cunningham, John P and Santhanam, Gopal and Ryu, Stephen I and Krishna, V and Shenoy, Krishna V and Sahani, Maneesh},
doi = {10.1152/jn.90941.2008},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2013 - Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity Gaussian-Pro.pdf:pdf},
isbn = {0022-3077},
issn = {0022-3077},
journal = {Journal of neurophysiology},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Models, Neurological,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neurons,Neurons: physiology,Nonlinear Dynamics,Normal Distribution,Principal Component Analysis,Reaction Time,Reaction Time: physiology,Signal Processing, Computer-Assisted,Time Factors},
number = {April 2009},
pages = {614--635},
pmid = {19357332},
title = {{Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2712272{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {102},
year = {2013}
}
@article{Hensman2015b,
abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.},
archivePrefix = {arXiv},
arxivId = {1411.2005},
author = {Hensman, James and Matthews, Alexander G. and Ghahramani, Zoubin},
eprint = {1411.2005},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hensman, Matthews, Ghahramani - 2015 - Scalable variational Gaussian process classification.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
pages = {351--360},
title = {{Scalable variational Gaussian process classification}},
volume = {38},
year = {2015}
}
@inproceedings{Tamar2017,
abstract = {We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.},
archivePrefix = {arXiv},
arxivId = {1602.02867},
author = {Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
eprint = {1602.02867},
isbn = {9780999241103},
issn = {10450823},
pmid = {172808},
title = {{Value iteration networks}},
year = {2017}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
doi = {10.1109/CVPR.2016.95},
eprint = {1606.04080},
isbn = {9781467369640},
issn = {10636919},
pmid = {24920543},
title = {{Matching Networks for One Shot Learning}},
year = {2016}
}
@article{Astudillo2019a,
abstract = {We consider black-box global optimization of time-consuming-to-evaluate functions on behalf of a decision-maker whose preferences must be learned. Each feasible design is associated with a time-consuming-to-evaluate vector of attributes, each vector of attributes is assigned a utility by the decision-maker's utility function, and this utility function may be learned approximately using preferences expressed by the decision-maker over pairs of attribute vectors. Past work has used this estimated utility function as if it were error-free within single-objective optimization. However, errors in utility estimation may yield a poor suggested decision. Furthermore, this approach produces a single suggested "best" design, whereas decision-makers often prefer to choose among a menu of designs. We propose a novel Bayesian optimization algorithm that acknowledges the uncertainty in preference estimation and implicitly chooses designs to evaluate using the time-consuming function that are good not just for a single estimated utility function but a range of likely utility functions. Our algorithm then shows a menu of designs and evaluated attributes to the decision-maker who makes a final selection. We demonstrate the value of our algorithm in a variety of numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1911.05934},
author = {Astudillo, Raul and Frazier, Peter I.},
eprint = {1911.05934},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Astudillo, Frazier - 2019 - Bayesian Optimization with Uncertain Preferences over Attributes.pdf:pdf},
title = {{Bayesian Optimization with Uncertain Preferences over Attributes}},
url = {http://arxiv.org/abs/1911.05934},
year = {2019}
}
@article{Trautmann2019,
author = {Trautmann, Eric M. and Stavisky, Sergey D. and Lahiri, Subhaneil and Ames, Katherine C. and Kaufman, Matthew T. and O'Shea, Daniel J. and Vyas, Saurabh and Sun, Xulu and Ryu, Stephen I. and Ganguli, Surya and Shenoy, Krishna V.},
doi = {10.1016/j.neuron.2019.05.003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Trautmann et al. - 2019 - Accurate Estimation of Neural Population Dynamics without Spike Sorting.pdf:pdf},
issn = {08966273},
journal = {Neuron},
pages = {1--42},
title = {{Accurate Estimation of Neural Population Dynamics without Spike Sorting}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319304283},
year = {2019}
}
@article{Fortuin2018,
author = {Fortuin, Vincent and Gal, Yarin},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Fortuin, Gal - 2018 - On the Connection between Neural Processes and Gaussian Processes with Deep Kernels.pdf:pdf},
number = {NeurIPS},
pages = {1--6},
title = {{On the Connection between Neural Processes and Gaussian Processes with Deep Kernels}},
year = {2018}
}
@article{Song2017b,
abstract = {Conventional psychometric function (PF) estimation involves fitting a parametric, unidimensional sigmoid to binary subject responses, which is not readily extendible to higher order PFs. This study presents a nonparametric, Bayesian, multidimensional PF estimator that also relies upon traditional binary subject responses. This technique is built upon probabilistic classification (PC), which attempts to ascertain the subdomains corresponding to each subject response as a function of multiple independent variables. Increased uncertainty in the location of class boundaries results in a greater spread in the PF estimate, which is similar to a parametric PF estimate with a lower slope. PC was evaluated on both one-dimensional (1D) and two-dimensional (2D) simulated auditory PFs across a variety of function shapes and sample numbers. In the 1D case, PC demonstrated equivalent performance to conventional maximum likelihood regression for the same number of simulated responses. In the 2D case, where the responses...},
author = {Song, Xinyu D. and Garnett, Roman and Barbour, Dennis L.},
doi = {10.1121/1.4979594},
file = {:Users/mshvarts/Downloads/1.4979594.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {2513--2525},
pmid = {28464646},
title = {{Psychometric function estimation by probabilistic classification}},
url = {http://dx.doi.org/10.1121/1.4979594},
volume = {141},
year = {2017}
}
@article{Johnson2016,
abstract = {We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping.},
archivePrefix = {arXiv},
arxivId = {1603.06277},
author = {Johnson, Matthew J. and Duvenaud, David and Wiltschko, Alexander B. and Datta, Sandeep R. and Adams, Ryan P.},
doi = {10.1126/science.1250298},
eprint = {1603.06277},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Johnson et al. - 2016 - Composing graphical models with neural networks for structured representations and fast inference.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$n0036-8075 (Linking)},
issn = {0036-8075},
number = {Nips},
pmid = {24674869},
title = {{Composing graphical models with neural networks for structured representations and fast inference}},
url = {http://arxiv.org/abs/1603.06277},
year = {2016}
}
@article{Wang2017a,
abstract = {It has been recently shown that a convolutional neural network can learn optical flow estimation with unsupervised learning. However, the performance of the unsupervised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsupervised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Especially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.},
archivePrefix = {arXiv},
arxivId = {1711.05890},
author = {Wang, Yang and Yang, Yi and Yang, Zhenheng and Zhao, Liang and Wang, Peng and Xu, Wei},
doi = {10.4103/2230-8598.127114},
eprint = {1711.05890},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2017 - Occlusion Aware Unsupervised Learning of Optical Flow.pdf:pdf},
isbn = {2230-8598},
issn = {2230-8598},
title = {{Occlusion Aware Unsupervised Learning of Optical Flow}},
url = {http://arxiv.org/abs/1711.05890},
year = {2017}
}
@article{Chan2011,
abstract = {We propose a generalized Gaussian process model (GGPM), which is a unifying framework that encompasses many existing Gaussian process (GP) models, such as GP regression, classification, and counting. In the GGPM framework, the observation likelihood of the GP model is itself parameterized using the exponential family distribution. By deriving approximate inference algorithms for the generalized GP model, we are able to easily apply the same algorithm to all other GP models. Novel GP models are created by changing the parameterization of the likelihood function, which greatly simplifies their creation for task-specific output domains. We also derive a closed-form efficient Taylor approximation for inference on the model, and draw interesting connections with other model-specific closed-form approximations. Finally, using the GGPM, we create several new GP models and show their efficacy in building task-specific GP models for computer vision. {\textcopyright} 2011 IEEE.},
author = {Chan, Antoni B. and Dong, Daxiang},
doi = {10.1109/CVPR.2011.5995688},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chan, Dong - 2011 - Generalized gaussian process models.pdf:pdf},
isbn = {9781457703942},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2681--2688},
title = {{Generalized gaussian process models}},
year = {2011}
}
@article{Siivola2018,
abstract = {Bayesian optimization (BO) is a global optimization strategy designed to find the minimum of an expensive black-box function, typically defined on a compact subset of d, by using a Gaussian process (GP) as a surrogate model for the objective. Although currently available acquisition functions address this goal with different degree of success, an over-exploration effect of the contour of the search space is typically observed. However, in problems like the configuration of machine learning algorithms, the function domain is conservatively large and with a high probability the global minimum does not sit on the boundary of the domain. We propose a method to incorporate this knowledge into the search process by adding virtual derivative observations in the GP at the boundary of the search space. We use the properties of GPs to impose conditions on the partial derivatives of the objective. The method is applicable with any acquisition function, it is easy to use and consistently reduces the number of evaluations required to optimize the objective irrespective of the acquisition used. We illustrate the benefits of our approach in an extensive experimental comparison.},
archivePrefix = {arXiv},
arxivId = {1704.00963},
author = {Siivola, Eero and Vehtari, Aki and Vanhatalo, Jarno and Gonzalez, Javier and Andersen, Michael Riis},
doi = {10.1109/MLSP.2018.8516936},
eprint = {1704.00963},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Siivola et al. - 2018 - Correcting boundary over-exploration deficiencies in Bayesian optimization with virtual derivative sign observat.pdf:pdf},
isbn = {9781538654774},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Bayesian optimization,Gaussian process,Virtual derivative sign observation},
title = {{Correcting boundary over-exploration deficiencies in Bayesian optimization with virtual derivative sign observations}},
volume = {2018-Septe},
year = {2018}
}
@article{Saatchi2017,
abstract = {Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching, or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.},
archivePrefix = {arXiv},
arxivId = {1705.09558},
author = {Saatchi, Yunus and Wilson, Andrew Gordon},
eprint = {1705.09558},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Saatchi, Wilson - 2017 - Bayesian GAN.pdf:pdf},
issn = {10495258},
number = {Nips},
pages = {1--16},
title = {{Bayesian GAN}},
url = {http://arxiv.org/abs/1705.09558},
year = {2017}
}
@article{Roeder2017,
abstract = {We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.},
archivePrefix = {arXiv},
arxivId = {1703.09194},
author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David},
eprint = {1703.09194},
issn = {10495258},
title = {{Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference}},
year = {2017}
}
@article{Eslami2018,
abstract = {Scene representation—the process of converting visual sensory data into concise descriptions—is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.},
author = {Eslami, S. M.Ali and {Jimenez Rezende}, Danilo and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1126/science.aar6170},
issn = {10959203},
journal = {Science (New York, N.Y.)},
number = {6394},
title = {{Neural scene representation and rendering}},
volume = {360},
year = {2018}
}
@article{Brookes2020,
abstract = {Virtual Reality systems offer a powerful tool for human behaviour research. The ability to create three-dimensional visual scenes and measure responses to the visual stimuli enables the behavioural researcher to test hypotheses in a manner and scale that were previously unfeasible. For example, a researcher wanting to understand interceptive timing behaviour might wish to violate Newtonian mechanics, so objects move in novel 3D trajectories. The same researcher may wish to collect such data with hundreds of participants outside the laboratory, and the use of a VR headset makes this a realistic proposition. The difficulty facing the researcher is that sophisticated 3D graphics engines (e.g. Unity) have been created for game designers rather than behavioural scientists. In order to overcome this barrier, we have created a set of tools and programming syntaxes that allow logical encoding of the common experimental features required by the behavioural scientist. The Unity Experiment Framework (UXF) allows the researcher to readily implement several forms of data collection, and provides researchers with the ability to easily modify independent variables. UXF does not offer any stimulus presentation features, so the full power of the Unity game engine can be exploited. We use a case study experiment, measuring postural sway in response to an oscillating virtual room, to show how UXF can replicate and advance upon behavioural research paradigms. We show that UXF can simplify and speed up development of VR experiments created in commercial gaming software and facilitate the efficient acquisition of large quantities of behavioural research data.},
author = {Brookes, Jack and Warburton, Matthew and Alghadier, Mshari and Mon-Williams, Mark and Mushtaq, Faisal},
doi = {10.3758/s13428-019-01242-0},
file = {:Users/mshvarts/Downloads/Brookes2020{\_}Article{\_}StudyingHumanBehaviorWithVirtu.pdf:pdf},
issn = {1554-3528},
journal = {Behavior Research Methods},
keywords = {Behaviour,Experiment,Software,Toolkit,Unity,Virtual Reality},
month = {apr},
number = {2},
pages = {455--463},
publisher = {Behavior Research Methods},
title = {{Studying human behavior with virtual reality: The Unity Experiment Framework}},
url = {http://link.springer.com/10.3758/s13428-019-01242-0},
volume = {52},
year = {2020}
}
@article{Pastore1974,
abstract = {While there exist a number of papers describing the theory of signal detection, it appears that many psychologists are not aware of the ease with which signal detection theory can be applied, the range of applications possible, or the limitations of signal detection theory. This paper briefly summarizes the assumptions of signal detection theory and describes the procedures, the limi-tations, and practical considerations relevant to its application. A worked example of an application of signal detection theory to the study of cognitive processes is included.},
author = {Pastore, R. E. and Scheirer, C. J.},
doi = {10.1037/h0037357},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pastore, Scheirer - 1974 - Signal detection theory Considerations for general application.pdf:pdf},
issn = {00332909},
journal = {Psychological Bulletin},
keywords = {assumptions {\&} procedures {\&} limitations {\&} applicati},
number = {12},
pages = {945--958},
title = {{Signal detection theory: Considerations for general application}},
volume = {81},
year = {1974}
}
@article{Ryder2018,
abstract = {Parameter inference for stochastic differential equations is challenging due to the presence of a latent diffusion process. Working with an Euler-Maruyama discretisation for the diffusion, we use variational inference to jointly learn the parameters and the diffusion paths. We use a standard mean-field variational approximation of the parameter posterior, and introduce a recurrent neural network to approximate the posterior for the diffusion paths conditional on the parameters. This neural network learns how to provide Gaussian state transitions which bridge between observations in a very similar way to the conditioned diffusion process. The resulting black-box inference method can be applied to any SDE system with light tuning requirements. We illustrate the method on a Lotka-Volterra system and an epidemic model, producing accurate parameter estimates in a few hours.},
archivePrefix = {arXiv},
arxivId = {1802.03335},
author = {Ryder, Thomas and Golightly, Andrew and McGough, A. Stephen and Prangle, Dennis},
eprint = {1802.03335},
issn = {1938-7228},
title = {{Black-box Variational Inference for Stochastic Differential Equations}},
year = {2018}
}
@article{Chandak2019,
abstract = {Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.},
archivePrefix = {arXiv},
arxivId = {1902.00183},
author = {Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S.},
eprint = {1902.00183},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chandak et al. - 2019 - Learning Action Representations for Reinforcement Learning.pdf:pdf},
title = {{Learning Action Representations for Reinforcement Learning}},
url = {http://arxiv.org/abs/1902.00183},
year = {2019}
}
@article{Kononen2004,
abstract = {A novel model for asymmetric multiagent reinforcement learning is introduced in this paper. The model addresses the problem where the information states of the agents involved in the learning task are not equal; some agents (leaders) have information how their opponents (followers) will select their actions and based on this information leaders encourage followers to select actions that lead to improved payoffs for the leaders. This kind of configuration arises e.g. in semi-centralized multiagent systems with an external global utility associated to the system. We present a brief literature survey of multiagent reinforcement learning based on Markov games and then propose an asymmetric learning model that utilizes the theory of Markov games. Additionally, we construct a practical learning method based on the proposed learning model and study its convergence properties. Finally, we test our model with a simple example problem and a larger two-layer pricing application.},
author = {K{\"{o}}n{\"{o}}nen, V},
doi = {1570-1263/04/$17.00},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{o}}n{\"{o}}nen - 2004 - Asymmetric multiagent reinforcement learning.pdf:pdf},
issn = {1570-1263},
journal = {Web Intelligence and Agent Systems},
keywords = {Convergence,Markov games,Multiagent reinforcement learning,Nash equilibrium,Stackelberg equilibrium},
number = {2},
pages = {105--121},
title = {{Asymmetric multiagent reinforcement learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-4143066730{\&}partnerID=40{\&}md5=a6c9d880a5ce7ac144f09b222dbdefdf},
volume = {2},
year = {2004}
}
@article{Krishnan2015,
abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the "Healing MNIST" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
archivePrefix = {arXiv},
arxivId = {1511.05121},
author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
eprint = {1511.05121},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Krishnan, Shalit, Sontag - 2015 - Deep Kalman Filters.pdf:pdf},
number = {2000},
pages = {1--17},
title = {{Deep Kalman Filters}},
url = {http://arxiv.org/abs/1511.05121},
year = {2015}
}
@article{Thiede2019,
abstract = {Understanding chemical mechanisms requires estimating dynamical statistics such as expected hitting times, reaction rates, and committors. Here, we present a general framework for calculating these dynamical quantities by approximating boundary value problems using dynamical operators with a Galerkin expansion. A specific choice of basis set in the expansion corresponds to estimation of dynamical quantities using a Markov state model. More generally, the boundary conditions impose restrictions on the choice of basis sets. We demonstrate how an alternative basis can be constructed using ideas from diffusion maps. In our numerical experiments, this basis gives results of comparable or better accuracy to Markov state models. Additionally, we show that delay embedding can reduce the information lost when projecting the system's dynamics for model construction; this improves estimates of dynamical statistics considerably over the standard practice of increasing the lag time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1810.01841v2},
author = {Thiede, Erik H. and Giannakis, Dimitrios and Dinner, Aaron R. and Weare, Jonathan},
doi = {10.1063/1.5063730},
eprint = {arXiv:1810.01841v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Thiede et al. - 2019 - Galerkin approximation of dynamical quantities using trajectory data.pdf:pdf},
issn = {0021-9606},
journal = {The Journal of Chemical Physics},
number = {24},
pages = {244111},
title = {{Galerkin approximation of dynamical quantities using trajectory data}},
volume = {150},
year = {2019}
}
@article{Brunton2016,
abstract = {Background: There is a broad need in neuroscience to understand and visualize large-scale recordings of neural activity, big data acquired by tens or hundreds of electrodes recording dynamic brain activity over minutes to hours. Such datasets are characterized by coherent patterns across both space and time, yet existing computational methods are typically restricted to analysis either in space or in time separately. New method: Here we report the adaptation of dynamic mode decomposition (DMD), an algorithm originally developed for studying fluid physics, to large-scale neural recordings. DMD is a modal decomposition algorithm that describes high-dimensional dynamic data using coupled spatial-temporal modes. The algorithm is robust to variations in noise and subsampling rate; it scales easily to very large numbers of simultaneously acquired measurements. Results: We first validate the DMD approach on sub-dural electrode array recordings from human subjects performing a known motor task. Next, we combine DMD with unsupervised clustering, developing a novel method to extract spindle networks during sleep. We uncovered several distinct sleep spindle networks identifiable by their stereotypical cortical distribution patterns, frequency, and duration. Comparison with existing methods: DMD is closely related to principal components analysis (PCA) and discrete Fourier transform (DFT). We may think of DMD as a rotation of the low-dimensional PCA space such that each basis vector has coherent dynamics. Conclusions: The resulting analysis combines key features of performing PCA in space and power spectral analysis in time, making it particularly suitable for analyzing large-scale neural recordings.},
archivePrefix = {arXiv},
arxivId = {1409.5496},
author = {Brunton, Bingni W. and Johnson, Lise A. and Ojemann, Jeffrey G. and Kutz, J. Nathan},
doi = {10.1016/j.jneumeth.2015.10.010},
eprint = {1409.5496},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Brunton et al. - 2016 - Extracting spatial-temporal coherent patterns in large-scale neural recordings using dynamic mode decomposition.pdf:pdf},
isbn = {0165-0270},
issn = {1872678X},
journal = {Journal of Neuroscience Methods},
keywords = {Dynamic mode decomposition,Electrocorticography,Feature extraction,Sleep spindles,Spatiotemporal modes},
pages = {1--15},
pmid = {26529367},
title = {{Extracting spatial-temporal coherent patterns in large-scale neural recordings using dynamic mode decomposition}},
volume = {258},
year = {2016}
}
@article{Ramoser2000,
abstract = {The development of an electroencephalograph (EEG)-based brain-computer interface (BCI) requires rapid and reliable discrimination of EEG patterns, e.g., associated with imaginary movement. One-sided hand movement imagination results in EEG changes located at contra- and ipsilateral central areas. We demonstrate that spatial filters for multichannel EEG effectively extract discriminatory information from two populations of single-trial EEG, recorded during left- and right-hand movement imagery. The best classification results for three subjects are 90.8{\%}, 92.7{\%}, and 99.7{\%}. The spatial filters are estimated from a set of data by the method of common spatial patterns and reflect the specific activation of cortical areas. The method performs a weighting of the electrodes according to their importance for the classification task. The high recognition rates and computational simplicity make it a promising method for an EEG-based brain-computer interface.},
author = {Ramoser, Herbert and M{\"{u}}ller-Gerking, Johannes and Pfurtscheller, Gert},
doi = {10.1109/86.895946},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ramoser, M{\"{u}}ller-Gerking, Pfurtscheller - 2000 - Optimal spatial filtering of single trial EEG during imagined hand movement.pdf:pdf},
isbn = {1063-6528},
issn = {10636528},
journal = {IEEE Transactions on Rehabilitation Engineering},
keywords = {Assistive communication,Electroencephalograph (EEG) classification,Event-related desynchronization (ERD),Mu rhythm,Prosthesis,Sensorimotor cortex},
number = {4},
pages = {441--446},
pmid = {11204034},
title = {{Optimal spatial filtering of single trial EEG during imagined hand movement}},
volume = {8},
year = {2000}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rumelhart, Hinton, Williams - 1986 - Learning representations by back-propagating errors.pdf:pdf},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{Raissi2017,
abstract = {This work leverages recent advances in probabilistic machine learning to discover governing equations expressed by parametric linear operators. Such equations involve, but are not limited to, ordinary and partial differential, integro-differential, and fractional order operators. Here, Gaussian process priors are modified according to the particular form of such operators and are employed to infer parameters of the linear equations from scarce and possibly noisy observations. Such observations may come from experiments or “black-box” computer simulations, as demonstrated in several synthetic examples and a realistic application in functional genomics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.02440v1},
author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
doi = {10.1016/j.jcp.2017.07.050},
eprint = {arXiv:1701.02440v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Raissi, Perdikaris, Karniadakis - 2017 - Machine learning of linear differential equations using Gaussian processes.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Fractional differential equations,Functional genomics,Inverse problems,Probabilistic machine learning,Uncertainty quantification},
number = {1},
pages = {683--693},
title = {{Machine learning of linear differential equations using Gaussian processes}},
volume = {348},
year = {2017}
}
@article{Cutajar2016,
abstract = {The composition of multiple Gaussian Processes as a Deep Gaussian Process (DGP) enables a deep probabilistic nonparametric approach to flexibly tackle complex machine learning problems with sound quantification of uncertainty. Existing inference approaches for DGP models have limited scalability and are notoriously cumbersome to construct. In this work, we introduce a novel formulation of DGPs based on random feature expansions that we train using stochastic variational inference. This yields a practical learning framework which significantly advances the state-of-the-art in inference for DGPs, and enables accurate quantification of uncertainty. We extensively showcase the scalability and performance of our proposal on several datasets with up to 8 million observations, and various DGP architectures with up to 30 hidden layers.},
archivePrefix = {arXiv},
arxivId = {1610.04386},
author = {Cutajar, Kurt and Bonilla, Edwin V. and Michiardi, Pietro and Filippone, Maurizio},
eprint = {1610.04386},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cutajar et al. - 2016 - Random Feature Expansions for Deep Gaussian Processes.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Random Feature Expansions for Deep Gaussian Processes}},
url = {http://arxiv.org/abs/1610.04386},
year = {2016}
}
@article{Saeedi2014,
abstract = {Approximate inference in high-dimensional, discrete probabilistic models is a central problem in computational statistics and machine learning. This paper describes discrete particle variational inference (DPVI), a new approach that combines key strengths of Monte Carlo, variational and search-based techniques. DPVI is based on a novel family of particle-based variational approximations that can be fit using simple, fast, deterministic search techniques. Like Monte Carlo, DPVI can handle multiple modes, and yields exact results in a well-defined limit. Like unstructured mean-field, DPVI is based on optimizing a lower bound on the partition function; when this quantity is not of intrinsic interest, it facilitates convergence assessment and debugging. Like both Monte Carlo and combinatorial search, DPVI can take advantage of factorization, sequential structure, and custom search operators. This paper defines DPVI particle-based approximation family and partition function lower bounds, along with the sequential DPVI and local DPVI algorithm templates for optimizing them. DPVI is illustrated and evaluated via experiments on lattice Markov Random Fields, nonparametric Bayesian mixtures and block-models, and parametric as well as non-parametric hidden Markov models. Results include applications to real-world spike-sorting and relational modeling problems, and show that DPVI can offer appealing time/accuracy trade-offs as compared to multiple alternatives.},
archivePrefix = {arXiv},
arxivId = {1402.5715},
author = {Saeedi, Ardavan and Kulkarni, Tejas D and Mansinghka, Vikash and Gershman, Samuel},
doi = {10.1016/j.foodres.2016.07.008},
eprint = {1402.5715},
issn = {15337928},
title = {{Variational Particle Approximations}},
year = {2014}
}
@article{Dai2017a,
abstract = {Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) and that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP, of which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.},
archivePrefix = {arXiv},
arxivId = {1705.09862},
author = {Dai, Zhenwen and {\'{A}}lvarez, Mauricio A. and Lawrence, Neil D.},
eprint = {1705.09862},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Dai, {\'{A}}lvarez, Lawrence - 2017 - Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes.pdf:pdf},
title = {{Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes}},
url = {http://arxiv.org/abs/1705.09862},
year = {2017}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
doi = {10.1002/etc.712},
eprint = {1505.05424},
isbn = {9781510810587},
issn = {1552-8618},
pmid = {21994178},
title = {{Weight Uncertainty in Neural Networks}},
year = {2015}
}
@article{Avendano-Valencia2017,
abstract = {A wide range of vibrating structures are characterized by variable structural dynamics resulting from changes in environmental and operational conditions, posing challenges in their identification and associated condition assessment. To tackle this issue, the present contribution introduces a stochastic modeling methodology via Gaussian Process (GP) time-series models. In the presently introduced approach, the vibration response is represented by means of a random coefficient time-series model, whose coefficients comply with a GP regression on the environmental and operational parameters. The approach may be implemented in conjunction to any type of linear-in-the-parameters time-series model, ranging from simple AR models to more complex non-linear or non- stationary time-series models. The obtained GP time-series modeling approach provides an effective and compact global representation of the vibrational response of a structure under a wide span of environmental and operational conditions. The effectiveness of the postulated GP time-series models is demonstrated through two case studies: the first involves the identification of the vertical vibration response of the Humber bridge, evaluated over a period of three years; the second considers the long-term simulated vibration response of a wind turbine featuring non-stationary dynamics stemming from the rotor speed. In both cases, the variation of the average wind speed is the main driver of uncertainty, while, through application of the proposed GP time-series models, it is possible to track the resulting variation in modal quantities.},
author = {Avenda{\~{n}}o-Valencia, Luis David and Chatzi, Eleni N. and Koo, Ki Young and Brownjohn, James M. W.},
doi = {10.3389/fbuil.2017.00069},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Avenda{\~{n}}o-Valencia et al. - 2017 - Gaussian Process Time-Series Models for Structures under Operational Variability.pdf:pdf},
issn = {2297-3362},
journal = {Frontiers in Built Environment},
keywords = {gaussian process,metamodels,random coefficient,time-series models,time-series models, uncertainty, metamodels, rando,uncertainty},
number = {December},
pages = {1--19},
title = {{Gaussian Process Time-Series Models for Structures under Operational Variability}},
url = {http://journal.frontiersin.org/article/10.3389/fbuil.2017.00069/full},
volume = {3},
year = {2017}
}
@article{Khan2014,
abstract = {We introduce a Bayesian extension of the tensor factoriza-tion problem to multiple coupled tensors. For a single tensor it reduces to standard PARAFAC-type Bayesian factorization, and for two tensors it is the first Bayesian Tensor Canonical Correlation Analysis method. It can also be seen to solve a tensorial extension of the recent Group Factor Analysis problem. The method decomposes the set of tensors to factors shared by subsets of the tensors, and factors private to individ-ual tensors, and does not assume orthogonality. For a single tensor, the method empirically outperforms existing methods, and we demonstrate its performance on multiple tensor factorization tasks in toxicogenomics and functional neuroimaging.},
archivePrefix = {arXiv},
arxivId = {1412.4679},
author = {Khan, Suleiman A. and Kaski, Samuel},
doi = {10.1007/978-3-662-44848-9_42},
eprint = {1412.4679},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Khan, Kaski - 2014 - Bayesian multi-view tensor factorization.pdf:pdf},
isbn = {9783662448472},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {656--671},
pmid = {1902689},
title = {{Bayesian multi-view tensor factorization}},
volume = {8724 LNAI},
year = {2014}
}
@article{Gal2015a,
abstract = {Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting.},
archivePrefix = {arXiv},
arxivId = {1503.02424},
author = {Gal, Yarin and Turner, Richard},
doi = {10.1103/PhysRevB.92.024411},
eprint = {1503.02424},
isbn = {978-0-262-18253-9},
issn = {1550235X},
title = {{Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs}},
year = {2015}
}
@article{Dang2017,
abstract = {Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional posterior distributions with proposed parameter draws obtained by iterating on a discretized version of the Hamiltonian dynamics. The iterations make HMC computationally costly, especially in problems with large datasets, since it is necessary to compute posterior densities and their derivatives with respect to the parameters. Naively computing the Hamiltonian dynamics on a subset of the data causes HMC to lose its key ability to generate distant parameter proposals with high acceptance probability. The key insight in our article is that efficient subsampling HMC for the parameters is possible if both the dynamics and the acceptance probability are computed from the same data subsample in each complete HMC iteration. We show that this is possible to do in a principled way in a HMC-within-Gibbs framework where the subsample is updated using a pseudo marginal MH step and the parameters are then updated using an HMC step, based on the current subsample. We show that our subsampling methods are fast and compare favorably to two popular sampling algorithms that utilize gradient estimates from data subsampling. We also explore the current limitations of subsampling HMC algorithms by varying the quality of the variance reducing control variates used in the estimators of the posterior density and its gradients.},
archivePrefix = {arXiv},
arxivId = {1708.00955},
author = {Dang, Khue-Dung and Quiroz, Matias and Kohn, Robert and Tran, Minh-Ngoc and Villani, Mattias},
eprint = {1708.00955},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Dang et al. - 2017 - Hamiltonian Monte Carlo with Energy Conserving Subsampling.pdf:pdf},
pages = {1--31},
title = {{Hamiltonian Monte Carlo with Energy Conserving Subsampling}},
url = {http://arxiv.org/abs/1708.00955},
volume = {20},
year = {2017}
}
@article{Gelman2017,
abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
doi = {10.3390/e19100555},
issn = {10994300},
journal = {Entropy},
number = {10},
title = {{The prior can often only be understood in the context of the likelihood}},
volume = {19},
year = {2017}
}
@article{Barthelme2013,
author = {Barthelme, S. and Trukenbrod, H. and Engbert, Ralf and Wichmann, Felix},
doi = {10.1167/13.12.1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Barthelme et al. - 2013 - Modeling fixation locations using spatial point processes.pdf:pdf},
issn = {1534-7362},
journal = {Journal of Vision},
month = {oct},
number = {12},
pages = {1--1},
title = {{Modeling fixation locations using spatial point processes}},
url = {http://jov.arvojournals.org/Article.aspx?doi=10.1167/13.12.1},
volume = {13},
year = {2013}
}
@article{Saul2016,
abstract = {Gaussian process models are flexible, Bayesian non-parametric approaches to regression. Properties of multivariate Gaussians mean that they can be combined linearly in the manner of additive models and via a link function (like in generalized linear models) to handle non-Gaussian data. However, the link function formalism is restrictive, link functions are always invertible and must convert a parameter of interest to a linear combination of the underlying processes. There are many likelihoods and models where a non-linear combination is more appropriate. We term these more general models Chained Gaussian Processes: the transformation of the GPs to the likelihood parameters will not generally be invertible, and that implies that linearisation would only be possible with multiple (localized) links, i.e. a chain. We develop an approximate inference procedure for Chained GPs that is scalable and applicable to any factorized likelihood. We demonstrate the approximation on a range of likelihood functions.},
archivePrefix = {arXiv},
arxivId = {1604.05263},
author = {Saul, Alan D. and Hensman, James and Vehtari, Aki and Lawrence, Neil D.},
eprint = {1604.05263},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Saul et al. - 2016 - Chained Gaussian Processes.pdf:pdf},
pages = {1--23},
title = {{Chained Gaussian Processes}},
url = {http://arxiv.org/abs/1604.05263},
year = {2016}
}
@article{Sung2017,
abstract = {A robot operating in a real-world environment needs to perform reasoning over a variety of sensor modalities such as vision, language and motion trajectories. However, it is extremely challenging to manually design features relating such disparate modalities. In this work, we introduce an algorithm that learns to embed point-cloud, natural language, and manipulation trajectory data into a shared embedding space with a deep neural network. To learn semantically meaningful spaces throughout our network, we use a loss-based margin to bring embeddings of relevant pairs closer together while driving less-relevant cases from different modalities further apart. We use this both to pre-train its lower layers and fine-tune our final embedding space, leading to a more robust representation. We test our algorithm on the task of manipulating novel objects and appliances based on prior experience with other objects. On a large dataset, we achieve significant improvements in both accuracy and inference time over the previous state of the art. We also perform end-to-end experiments on a PR2 robot utilizing our learned embedding space.},
archivePrefix = {arXiv},
arxivId = {1509.07831},
author = {Sung, Jaeyong and Lenz, Ian and Saxena, Ashutosh},
doi = {10.1109/ICRA.2017.7989325},
eprint = {1509.07831},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sung, Lenz, Saxena - 2017 - Deep multimodal embedding Manipulating novel objects with point-clouds, language and trajectories.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2794--2801},
title = {{Deep multimodal embedding: Manipulating novel objects with point-clouds, language and trajectories}},
year = {2017}
}
@article{Bilenko2016,
author = {Bilenko, Natalia Y. and Gallant, Jack L.},
doi = {10.3389/fninf.2016.00049},
issn = {1662-5196},
journal = {Frontiers in Neuroinformatics},
month = {nov},
title = {{Pyrcca: Regularized Kernel Canonical Correlation Analysis in Python and Its Applications to Neuroimaging}},
url = {http://journal.frontiersin.org/article/10.3389/fninf.2016.00049/full},
volume = {10},
year = {2016}
}
@article{Musslick2016,
author = {Musslick, Sebastian and Dey, Biswadip and {\"{O}}zcimder, Kayhan and Patwary, M M A and Willke, T L and Cohen, Jonathan D.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Musslick et al. - 2016 - Controlled vs. Automatic Processing A Graph-Theoretic Approach to the Analysis of Serial vs. Parallel Processin.pdf:pdf},
journal = {Proceedings of the 38th Annual Conference of the Cognitive Science Society},
keywords = {capacity con-,cognitive control,multitasking},
number = {August},
pages = {1547--1552},
title = {{Controlled vs. Automatic Processing: A Graph-Theoretic Approach to the Analysis of Serial vs. Parallel Processing in Neural Network Architectures}},
year = {2016}
}
@article{Zhou2016,
abstract = {—Temporal alignment of human motion has been of recent interest due to its applications in animation, tele-rehabilitation and activity recognition. This paper presents generalized canonical time warping (GCTW), an extension of dynamic time warping (DTW) and canonical correlation analysis (CCA) for temporally aligning multi-modal sequences from multiple subjects performing similar activities. GCTW extends previous work on DTW and CCA in several ways: (1) it combines CCA with DTW to align multi-modal data (e.g., video and motion capture data); (2) it extends DTW by using a linear combination of monotonic functions to represent the warping path, providing a more flexible temporal warp. Unlike exact DTW, which has quadratic complexity, we propose a linear time algorithm to minimize GCTW. (3) GCTW allows simultaneous alignment of multiple sequences. Experimental results on aligning multi-modal data, facial expressions, motion capture data and video illustrate the benefits of GCTW.},
author = {Zhou, Feng and {De La Torre}, Fernando},
doi = {10.1109/TPAMI.2015.2414429},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, De La Torre - 2016 - Generalized Canonical Time Warping.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Canonical correlation analysis,Dynamic time warping,Multi-modal sequence alignment},
number = {2},
pages = {279--294},
publisher = {IEEE},
title = {{Generalized Canonical Time Warping}},
volume = {38},
year = {2016}
}
@inproceedings{Yildiz2018,
abstract = {We introduce a novel paradigm for learning non-parametric drift and diffusion functions for stochastic differential equation (SDE). The proposed model learns to simulate path distributions that match observations with non-uniform time increments and arbitrary sparseness, which is in contrast with gradient matching that does not optimize simulated responses. We formulate sensitivity equations for learning and demonstrate that our general stochastic distribution optimisation leads to robust and efficient learning of SDE systems.},
author = {Yildiz, Cagatay and Heinonen, Markus and Intosalmi, Jukka and Mannerstrom, Henrik and Lahdesmaki, Harri},
booktitle = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
doi = {10.1109/MLSP.2018.8516991},
isbn = {9781538654774},
issn = {21610371},
title = {{Learning stochastic differential equations with Gaussian processes without gradient matching}},
volume = {2018-Septe},
year = {2018}
}
@article{Lieder2014,
abstract = {Selecting the right algorithm is an important problem in computer science, be-cause the algorithm often has to exploit the structure of the input to be efficient. The human mind faces the same challenge. Therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa. Here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selec-tion. We apply our theory to model how people choose between cognitive strate-gies and test its prediction in a behavioral experiment. We find that people quickly learn to adaptively choose between cognitive strategies. People's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection. Rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.},
author = {Lieder, Falk and Hamrick, Jessica B. and Hay, Nicholas J. and Plunkett, Dillon and Russell, Stuart J and Griffiths, Thomas L.},
doi = {10.13140/2.1.2563.1686},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {September},
title = {{Algorithm selection by rational metareasoning as a model of human strategy selection}},
year = {2014}
}
@article{Bamler2017a,
abstract = {Continuous latent time series models are prevalent in Bayesian modeling; examples include the Kalman filter, dynamic collaborative filtering, or dynamic topic models. These models often benefit from structured, non mean field variational approximations that capture correlations between time steps. Black box variational inference with reparameterization gradients (BBVI) allows us to explore a rich new class of Bayesian non-conjugate latent time series models; however, a naive application of BBVI to such structured variational models would scale quadratically in the number of time steps. We describe a BBVI algorithm analogous to the forward-backward algorithm which instead scales linearly in time. It allows us to efficiently sample from the variational distribution and estimate the gradients of the ELBO. Finally, we show results on the recently proposed dynamic word embedding model, which was trained using our method.},
archivePrefix = {arXiv},
arxivId = {1707.01069},
author = {Bamler, Robert and Mandt, Stephan},
eprint = {1707.01069},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bamler, Mandt - 2017 - Structured Black Box Variational Inference for Latent Time Series Models.pdf:pdf},
number = {1},
title = {{Structured Black Box Variational Inference for Latent Time Series Models}},
url = {http://arxiv.org/abs/1707.01069},
year = {2017}
}
@article{Fraccaro2017,
abstract = {This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.},
archivePrefix = {arXiv},
arxivId = {1710.05741},
author = {Fraccaro, Marco and Kamronn, Simon and Paquet, Ulrich and Winther, Ole},
eprint = {1710.05741},
issn = {10495258},
title = {{A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning}},
year = {2017}
}
@article{Eckhoff2015,
author = {Eckhoff, Philip and Holmes, Philip},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Eckhoff, Holmes - 2015 - A Short Course in Mathematical Neuroscience.pdf:pdf},
pages = {216},
title = {{A Short Course in Mathematical Neuroscience}},
year = {2015}
}
@article{Kuleshov2017,
abstract = {Many problems in machine learning are naturally expressed in the language of undirected graphical models. Here, we propose black-box learning and inference algorithms for undirected models that optimize a variational approximation to the log-likelihood of the model. Central to our approach is an upper bound on the log-partition function parametrized by a function q that we express as a flexible neural network. Our bound makes it possible to track the partition function during learning, to speed-up sampling, and to train a broad class of hybrid directed/undirected models via a unified variational inference framework. We empirically demonstrate the effectiveness of our method on several popular generative modeling datasets.},
archivePrefix = {arXiv},
arxivId = {1711.02679},
author = {Kuleshov, Volodymyr and Ermon, Stefano},
eprint = {1711.02679},
issn = {10495258},
title = {{Neural Variational Inference and Learning in Undirected Graphical Models}},
year = {2017}
}
@article{Busoniu2008,
abstract = {Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity of many tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents' learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim---either explicitly or implicitly---at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges of MARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.},
author = {Busoniu, L. and Babuska, R. and {De Schutter}, B},
doi = {10.1109/TSMCC.2007.913919},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Busoniu, Babuska, De Schutter - 2008 - A Comprehensive Survey of Multiagent Reinforcement Learning.pdf:pdf},
isbn = {1094-6977},
issn = {1094-6977},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
keywords = {Distributed control,game theory,multiagent systems,reinforcement learning},
month = {mar},
number = {2},
pages = {156--172},
title = {{A Comprehensive Survey of Multiagent Reinforcement Learning}},
url = {http://ieeexplore.ieee.org/document/4445757/},
volume = {38},
year = {2008}
}
@article{Zhao2011,
abstract = {A multilinear subspace regression model based on so called latent variable de-composition is introduced. Unlike standard regression methods which typically employ matrix (2D) data representations followed by vector subspace transfor-mations, the proposed approach uses tensor subspace transformations to model common latent variables across both the independent and dependent data. The proposed approach aims to maximize the correlation between the so derived la-tent variables and is shown to be suitable for the prediction of multidimensional dependent data from multidimensional independent data, where for the estimation of the latent variables we introduce an algorithm based on Multilinear Singular Value Decomposition (MSVD) on a specially defined cross-covariance tensor. It is next shown that in this way we are also able to unify the existing Partial Least Squares (PLS) and N-way PLS regression algorithms within the same framework. Simulations on benchmark synthetic data confirm the advantages of the proposed approach, in terms of its predictive ability and robustness, especially for small sample sizes. The potential of the proposed technique is further illustrated on a real world task of the decoding of human intracranial electrocorticogram (ECoG) from a simultaneously recorded scalp electroencephalograph (EEG).},
author = {Zhao, Qibin and Caiafa, Cesar F and Mandic, Danilo P and Zhang, Liqing and Ball, Tonio and Schulze-Bonhage, Andreas and Cichocki, Andrzej},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhao et al. - 2011 - Multilinear Subspace Regression An Orthogonal Tensor Decomposition Approach.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Multilinear Subspace Regression : An Orthogonal Tensor Decomposition Approach}},
year = {2011}
}
@article{Duvenaud2014,
abstract = {Choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance. To shed light on this problem, we analyze the analogous problem of constructing useful priors on compositions of functions. Specifically, we study the deep Gaussian process, a type of infinitely-wide, deep neural network. We show that in standard architectures, the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases, retaining only a single degree of freedom in the limit. We propose an alternate network architecture which does not suffer from this pathology. We also examine deep covariance functions, obtained by composing infinitely many feature transforms. Lastly, we characterize the class of models obtained by performing dropout on Gaussian processes.},
archivePrefix = {arXiv},
arxivId = {1402.5836},
author = {Duvenaud, David and Rippel, Oren and Adams, Ryan P. and Ghahramani, Zoubin},
doi = {10.1016/B978-0-08-097086-8.22012-6},
eprint = {1402.5836},
isbn = {9780080970875},
issn = {15337928},
title = {{Avoiding pathologies in very deep networks}},
year = {2014}
}
@article{Arnold2016,
abstract = {Machine learning's advances have led to new ideas about the feasibility and importance of machine ethics keeping pace, with increasing emphasis on safety, containment, and align-ment. This paper addresses a recent suggestion that inverse reinforcement learning (IRL) could be a means to so-called " value alignment. " We critically consider how such an ap-proach can engage the social, norm-infused nature of ethical action and outline several features of ethical appraisal that go beyond simple models of behavior, including unavoidably temporal dimensions of norms and counterfactuals. We pro-pose that a hybrid approach for computational architectures still offers the most promising avenue for machines acting in an ethical fashion.},
author = {Arnold, Thomas and Kasenberg, Daniel and Scheutz, Matthias},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Arnold, Kasenberg, Scheutz - 2016 - Value Alignment or Misalignment – What Will Keep Systems Accountable.pdf:pdf},
isbn = {9781577357865},
title = {{Value Alignment or Misalignment – What Will Keep Systems Accountable ?}},
year = {2016}
}
@article{Harmon1996,
abstract = {The purpose of this tutorial is to provide an introduction to reinforcement learning (RL) at a level easily understood by students and researchers in a wide range of disciplines. The intent is not to present a$\backslash$nrigorous mathematical discussion that requires a great deal of effort on the part of the reader, but rather to present a conceptual framework that might serve as an introduction to a more rigorous study of RL. The fundamental principles and techniques used to solve RL problems are presented. The most popular RL algorithms are presented. Section 1 presents an overview of RL and provides a simple example to develop intuition of the underlying dynamic programming mechanism. In Section 2 the parts of a reinforcement learning problem are discussed. These include the environment, reinforcement function, and value function. Section 3 gives a description of the most widely used reinforcement learning algorithms. These include TD($\lambda$) and both the residual and direct forms of value iteration, Q-learning, and advantage learning. In Section 4 some of the ancillary issues in RL are briefly discussed, such as choosing an exploration strategy and an appropriate discount factor. The conclusion is given in Section 5. Finally, Section 6 is a glossary of commonly used terms followed by references in Section 7 and a bibliography of RL applications in Section 8. The tutorial structure is such that each section builds on the information provided in previous sections.$\backslash$nIt is assumed that the reader has some knowledge of learning algorithms that rely on gradient descent (such as the backpropagation of errors algorithm).},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Harmon, M E and Harmon, Stephanie S},
doi = {10.1287/ijoc.1080.0305},
eprint = {9605103},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Harmon, Harmon - 1996 - Reinforcement learning a tutorial.pdf:pdf},
isbn = {1091-9856},
issn = {10919856},
journal = {WL/AAFC, WPAFB Ohio},
pages = {237--285},
pmid = {17255001},
primaryClass = {cs},
title = {{Reinforcement learning: a tutorial}},
url = {http://www.jair.org/papers/paper301.html{\%}5Cnhttp://www.csc.kth.se/utbildning/kth/kurser/DD2432/ann12/forelasningsanteckningar/old-notes/rltutorial.pdf{\%}5Cnpapers2://publication/uuid/5276DF6D-006C-43E1-92C0-CEA9F945ACB5},
volume = {45433},
year = {1996}
}
@article{Le2014,
abstract = {Despite their successes, what makes kernel methods difficult to use in many large scale problems is the fact that storing and computing the decision function is typically expensive, especially at prediction time. In this paper, we overcome this difficulty by proposing Fastfood, an approximation that accelerates such computation significantly. Key to Fastfood is the observation that Hadamard matrices, when combined with diagonal Gaussian matrices, exhibit properties similar to dense Gaussian random matrices. Yet unlike the latter, Hadamard and diagonal matrices are inexpensive to multiply and store. These two matrices can be used in lieu of Gaussian matrices in Random Kitchen Sinks proposed by Rahimi and Recht (2009) and thereby speeding up the computation for a large range of kernel functions. Specifically, Fastfood requires O(n log d) time and O(n) storage to compute n non-linear basis functions in d dimensions, a significant improvement from O(nd) computation and storage, without sacrificing accuracy. Our method applies to any translation invariant and any dot-product kernel, such as the popular RBF kernels and polynomial kernels. We prove that the approximation is unbiased and has low variance. Experiments show that we achieve similar accuracy to full kernel expansions and Random Kitchen Sinks while being 100x faster and using 1000x less memory. These improvements, especially in terms of memory usage, make kernel methods more practical for applications that have large training sets and/or require real-time prediction.},
archivePrefix = {arXiv},
arxivId = {1408.3060},
author = {Le, Quoc Viet and Sarlos, Tamas and Smola, Alexander Johannes},
doi = {10.1007/s00229-006-0034-6},
eprint = {1408.3060},
issn = {00252611},
title = {{Fastfood: Approximate Kernel Expansions in Loglinear Time}},
year = {2014}
}
@article{Pang2019,
abstract = {Neural-net-induced Gaussian process (NNGP) regression inherits both the high expressivity of deep neural networks (deep NNs) as well as the uncertainty quantification property of Gaussian processes (GPs). We generalize the current NNGP to first include a larger number of hyperparameters and subsequently train the model by maximum likelihood estimation. Unlike previous works on NNGP that targeted classification, here we apply the generalized NNGP to function approximation and to solving partial differential equations (PDEs). Specifically, we develop an analytical iteration formula to compute the covariance function of GP induced by deep NN with an error-function nonlinearity. We compare the performance of the generalized NNGP for function approximations and PDE solutions with those of GPs and fully-connected NNs. We observe that for smooth functions the generalized NNGP can yield the same order of accuracy with GP, while both NNGP and GP outperform deep NN. For non-smooth functions, the generalized NNGP is superior to GP and comparable or superior to deep NN.},
archivePrefix = {arXiv},
arxivId = {1806.11187},
author = {Pang, Guofei and Yang, Liu and Karniadakis, George Em},
doi = {10.1016/j.jcp.2019.01.045},
eprint = {1806.11187},
issn = {10902716},
journal = {Journal of Computational Physics},
title = {{Neural-net-induced Gaussian process regression for function approximation and PDE solution}},
volume = {384},
year = {2019}
}
@article{Burroni2018,
abstract = {Deep probabilistic programming combines deep neural networks (for automatic hierarchical representation learning) with probabilistic models (for principled handling of uncertainty). Unfortunately, it is difficult to write deep probabilistic models, because existing programming frameworks lack concise, high-level, and clean ways to express them. To ease this task, we extend Stan, a popular high-level probabilistic programming language, to use deep neural networks written in PyTorch. Training deep probabilistic models works best with variational inference, so we also extend Stan for that. We implement these extensions by translating Stan programs to Pyro. Our translation clarifies the relationship between different families of probabilistic programming languages. Overall, our paper is a step towards making deep probabilistic programming easier.},
archivePrefix = {arXiv},
arxivId = {1810.00873},
author = {Burroni, Javier and Baudart, Guillaume and Mandel, Louis and Hirzel, Martin and Shinnar, Avraham},
doi = {10.1016/j.jip.2007.09.006},
eprint = {1810.00873},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Burroni et al. - 2018 - Extending Stan for Deep Probabilistic Programming.pdf:pdf},
issn = {0022-2011},
pages = {1--11},
pmid = {17961589},
title = {{Extending Stan for Deep Probabilistic Programming}},
url = {http://arxiv.org/abs/1810.00873},
year = {2018}
}
@article{Wilson2015b,
abstract = {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximations for fast computations through kernel interpolation. The SKI framework clarifies how the quality of an inducing point approach depends on the number of inducing (aka interpolation) points, interpolation strategy, and GP covariance kernel. SKI also provides a mechanism to create new scalable kernel methods, through choosing different kernel interpolation strategies. Using SKI, with local cubic kernel interpolation, we introduce KISS-GP, which is 1) more scalable than inducing point alternatives, 2) naturally enables Kronecker and Toeplitz algebra for substantial additional gains in scalability, without requiring any grid data, and 3) can be used for fast and expressive kernel learning. KISS-GP costs O(n) time and storage for GP inference. We evaluate KISS-GP for kernel matrix approximation, kernel learning, and natural sound modelling.},
archivePrefix = {arXiv},
arxivId = {1503.01057},
author = {Wilson, Andrew Gordon and Nickisch, Hannes},
eprint = {1503.01057},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilson, Nickisch - 2015 - Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP).pdf:pdf},
pages = {1--19},
title = {{Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)}},
url = {http://arxiv.org/abs/1503.01057},
year = {2015}
}
@article{Hadfield-Menell2016,
abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
archivePrefix = {arXiv},
arxivId = {1606.03137},
author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
eprint = {1606.03137},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf:pdf},
number = {Nips},
title = {{Cooperative Inverse Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.03137},
year = {2016}
}
@article{Usher2002,
abstract = {We present an analytic solution for a race model of n stochastic accumulators for multiple choice reaction time. We show that to maintain a constant level of accuracy, the response criterion needs to be increased approximately logarithmically with n, to compensate for the increase with n in the likelihood that an incorrect alternative will be most active after any fixed amount of time accumulating information. Assuming that participants monitor and maintain a constant level of performance can then explain the logarithmic dependency of the response latency on n as specified by Hick's law. Moreover, we show that for short time intervals, the Shannon information that observers extract from a stimulus, is predicted to increase linearly with processing time. {\textcopyright} 2002 Elsevier Science (USA).},
author = {Usher, Marius and Olami, Zeev and McClelland, James L.},
doi = {10.1006/jmps.2002.1420},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Usher, Olami, McClelland - 2002 - Hick's law in a stochastic race model with speed-accuracy tradeoff.pdf:pdf},
isbn = {00222496},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Accumulators,Choice-RT,Diffusion,Hick's law,Race models,Shannon information,Speed-accuracy tradeoff,Wiener process},
number = {6},
pages = {704--715},
title = {{Hick's law in a stochastic race model with speed-accuracy tradeoff}},
volume = {46},
year = {2002}
}
@article{Lakshminarayanan2016,
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1612.01474},
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
doi = {10.1007/s00217-008-0904-0},
eprint = {1612.01474},
issn = {10495258},
title = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
year = {2016}
}
@article{Duvenaud2014a,
abstract = {Several large software engineering projects have been undertaken to support black-box inference methods. In contrast, we emphasize how easy it is to con-struct scalable and easy-to-use automatic inference methods using only automatic differentiation. We present a small function which computes stochastic gradients of the evidence lower bound for any differentiable posterior. As an example, we perform stochastic variational inference in a deep Bayesian neural network.},
author = {Duvenaud, David and Adams, Ryan P},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Duvenaud, Adams - 2014 - Black-Box Stochastic Variational Inference in Five Lines of Python.pdf:pdf},
pages = {1--4},
title = {{Black-Box Stochastic Variational Inference in Five Lines of Python}},
year = {2014}
}
@article{Manning2018,
abstract = {How our experiences unfold over time define unique trajectories through the relevant representational spaces. Within this geometric framework, one can compare the shape of the trajectory formed by an experience to that defined by our later remembering of that experience. We propose a framework for mapping naturalistic experiences onto geometric spaces that characterize how they unfold over time. We apply this approach to a naturalistic memory experiment which had participants view and recount a video. We found that the shapes of the trajectories formed by participants' recountings were all highly similar to that of the original video, but participants differed in the level of detail they remembered. We also identified a network of brain structures that are sensitive to the "shapes" of our ongoing experiences, and an overlapping network that is sensitive to how we will later remember those experiences.},
author = {Manning, Jeremy R and Fitzpatrick, Paxton C and Heusser, Andrew C},
doi = {10.1101/409987},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Manning, Fitzpatrick, Heusser - 2018 - How is experience transformed into memory.pdf:pdf},
journal = {bioRxiv},
pages = {409987},
title = {{How is experience transformed into memory?}},
url = {https://www.biorxiv.org/content/early/2018/09/06/409987},
year = {2018}
}
@article{Derbinsky2009,
abstract = {Endowing an intelligent agent with an episodic memory affords it a multitude of cognitive capabilities. However, providing efficient storage and retrieval in a task-independent episodic memory presents considerable theoretical and practical challenges. We characterize the computational issues bounding an episodic memory. We explore whether even with intractable asymptotic growth, it is possible to develop efficient algorithms and data structures for episodic memory systems that are practical for real-world tasks. We present and evaluate formal and empirical results using Soar-EpMem: a task-independent integration of episodic memory with Soar 9, providing a baseline for graph-based, task-independent episodic memory systems. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Derbinsky, Nate and Laird, John E.},
doi = {10.1007/978-3-642-02998-1_29},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Derbinsky, Laird - 2009 - Efficiently implementing episodic memory.pdf:pdf},
isbn = {3642029973},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {403--417},
title = {{Efficiently implementing episodic memory}},
volume = {5650 LNAI},
year = {2009}
}
@article{Higham1992,
author = {Higham, Nicholas J. and Croz, Jeremy J. Du},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Higham, Croz - 1992 - Stabilit y of Methods for Matrix In ersion.pdf:pdf},
journal = {IMA J. Numer. Anal.},
keywords = {65g05,ac,ams mos subject classi,block al-,cations,error analysis,jordan hill road,matrix inversion,nagjdc,numerical algorithms group ltd,ox2 8dr,oxford,primary 65f05,triangular matrix,uk,vax,wilkinson house},
number = {January},
pages = {1--28},
title = {{Stabilit y of Methods for Matrix In ersion}},
volume = {12},
year = {1992}
}
@article{Howes2016,
abstract = {It is known that, on average, people adapt their choice of memory strategy to the subjective utility of interaction. What is not known is whether an individual's choices are boundedly optimal. Two experiments are reported that test the hypothesis that an individual's decisions about the distribution of remembering between internal and external resources are boundedly optimal where optimality is defined relative to experience, cognitive constraints, and reward. The theory makes predictions that are tested against data, not fitted to it. The experiments use a no-choice/choice utility learning paradigm where the no-choice phase is used to elicit a profile of each participant's performance across the strategy space and the choice phase is used to test predicted choices within this space. They show that the majority of individuals select strategies that are boundedly optimal. Further, individual differences in what people choose to do are successfully predicted by the analysis. Two issues are discussed: (a) the performance of the minority of participants who did not find boundedly optimal adaptations, and (b) the possibility that individuals anticipate what, with practice, will become a bounded optimal strategy, rather than what is boundedly optimal during training.},
author = {Howes, Andrew and Duggan, Geoffrey B. and Kalidindi, Kiran and Tseng, Yuan Chi and Lewis, Richard L.},
doi = {10.1111/cogs.12271},
issn = {15516709},
journal = {Cognitive Science},
number = {5},
pmid = {26294328},
title = {{Predicting Short-Term Remembering as Boundedly Optimal Strategy Choice}},
volume = {40},
year = {2016}
}
@article{Wilson2016a,
abstract = {Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1611.00336},
author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
doi = {10.1016/j.neucom.2008.12.019},
eprint = {1611.00336},
isbn = {1406.2661},
issn = {10495258},
pmid = {264993200014},
title = {{Stochastic Variational Deep Kernel Learning}},
year = {2016}
}
@article{Author2018,
author = {Author, Anonymous and Address, Affiliation},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Author, Address - 2018 - Non-Adversarial Mapping with VAEs.pdf:pdf},
number = {Nips},
title = {{Non-Adversarial Mapping with VAEs}},
year = {2018}
}
@article{Dai2013,
abstract = {Cosegmentation refers to the problem of segmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in cosegmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for cosegmentation, by coupling cosegmentation with what we call ``cosketch''. The goal of cosketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the cosketch of the images helps to align foreground objects, thereby providing crucial information for cosegmentation. We present a statistical model whose energy function couples cosketch and cosegmentation. We then present an unsupervised learning algorithm that performs cosketch and cosegmentation by energy minimization. Experiments show that our method outperforms state of the art methods for cosegmentation on the challenging MSRC and iCoseg datasets. We also illustrate our method on a new dataset called Coseg-Rep where cosegmentation can be performed within a single image with repetitive patterns.},
author = {Dai, Jifeng and Wu, Ying Nian and Zhou, Jie and Zhu, Song Chun},
doi = {10.1109/ICCV.2013.165},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Dai et al. - 2013 - Cosegmentation and cosketch by unsupervised learning.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
number = {1},
pages = {1305--1312},
publisher = {IEEE},
title = {{Cosegmentation and cosketch by unsupervised learning}},
year = {2013}
}
@article{Rolls2010,
abstract = {A quantitative computational theory of the operation of the hippocampus as an episodic memory system is described. The CA3 system operates as a single attractor or autoassociation network to enable rapid, one-trial associations between any spatial location (place in rodents or spatial view in primates) and an object or reward and to provide for completion of the whole memory during recall from any part. The theory is extended to associations between time and object or reward to implement temporal order memory, also important in episodic memory. The dentate gyrus performs pattern separation by competitive learning to produce sparse representations, producing for example neurons with place-like fields from entorhinal cortex grid cells. The dentate granule cells produce by the very small number of mossy fibre connections to CA3 a randomizing pattern separation effect important during learning but not recall that separates out the patterns represented by CA3 firing to be very different from each other, which is optimal for an unstructured episodic memory system in which each memory must be kept distinct from other memories. The direct perforant path input to CA3 is quantitatively appropriate to provide the cue for recall in CA3, but not for learning. The CA1 recodes information from CA3 to set up associatively learned backprojections to neocortex to allow subsequent retrieval of information to neocortex, providing a quantitative account of the large number of hippocampo-neocortical and neocortical-neocortical backprojections. Tests of the theory including hippocampal subregion analyses and hippocampal NMDA receptor knockouts are described and support the theory. {\textcopyright} 2010 Elsevier B.V.},
author = {Rolls, Edmund T.},
doi = {10.1016/j.bbr.2010.03.027},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rolls - 2010 - A computational theory of episodic memory formation in the hippocampus.pdf:pdf},
issn = {01664328},
journal = {Behavioural Brain Research},
keywords = {Attractor network,Competitive network,Completion,Episodic memory,Hippocampus,Object-place memory,Pattern separation,Recall,Spatial view neurons},
number = {2},
pages = {180--196},
title = {{A computational theory of episodic memory formation in the hippocampus}},
volume = {215},
year = {2010}
}
@article{Limerick2014,
abstract = {The sense of agency is the experience of controlling both one's body and the external environment. Although the sense of agency has been studied extensively, there is a paucity of studies in applied "real-life" situations. One applied domain that seems highly relevant is human-computer-interaction (HCI), as an increasing number of our everyday agentive interactions involve technology. Indeed, HCI has long recognized the feeling of control as a key factor in how people experience interactions with technology. The aim of this review is to summarize and examine the possible links between sense of agency and understanding control in HCI. We explore the overlap between HCI and sense of agency for computer input modalities and system feedback, computer assistance, and joint actions between humans and computers. An overarching consideration is how agency research can inform HCI and vice versa. Finally, we discuss the potential ethical implications of personal responsibility in an ever-increasing society of technology users and intelligent machine interfaces.},
author = {Limerick, Hannah and Coyle, David and Moore, James W.},
doi = {10.3389/fnhum.2014.00643},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Limerick, Coyle, Moore - 2014 - The experience of agency in human-computer interactions a review.pdf:pdf},
isbn = {1662-5161 (Electronic) 1662-5161 (Linking)},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {computer assistance,control,human computer interaction,joint action,sense of agency,sense of agency, human computer interaction, contr,technology},
number = {August},
pages = {1--10},
pmid = {25191256},
title = {{The experience of agency in human-computer interactions: a review}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2014.00643/abstract},
volume = {8},
year = {2014}
}
@article{Lieder2017,
author = {Lieder, Falk and Griffiths, Thomas L},
doi = {10.1037/rev0000075},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lieder, Griffiths - 2017 - Strategy selection as rational metareasoning.pdf:pdf},
issn = {1939-1471},
journal = {Psychological Review},
month = {nov},
number = {6},
pages = {762--794},
title = {{Strategy selection as rational metareasoning.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/rev0000075},
volume = {124},
year = {2017}
}
@article{Varoquaux2010,
abstract = {Spatial Independent Component Analysis (ICA) is an increasingly used data-driven method to analyze functional Magnetic Resonance Imaging (fMRI) data. To date, it has been used to extract sets of mutually correlated brain regions without prior information on the time course of these regions. Some of these sets of regions, interpreted as functional networks, have recently been used to provide markers of brain diseases and open the road to paradigm-free population comparisons. Such group studies raise the question of modeling subject variability within ICA: how can the patterns representative of a group be modeled and estimated via ICA for reliable inter-group comparisons? In this paper, we propose a hierarchical model for patterns in multi-subject fMRI datasets, akin to mixed-effect group models used in linear-model-based analysis. We introduce an estimation procedure, CanICA (Canonical ICA), based on i) probabilistic dimension reduction of the individual data, ii) canonical correlation analysis to identify a data subspace common to the group iii) ICA-based pattern extraction. In addition, we introduce a procedure based on cross-validation to quantify the stability of ICA patterns at the level of the group. We compare our method with state-of-the-art multi-subject fMRI ICA methods and show that the features extracted using our procedure are more reproducible at the group level on two datasets of 12 healthy controls: a resting-state and a functional localizer study. {\textcopyright} 2010 Elsevier Inc.},
author = {Varoquaux, G. and Sadaghiani, S. and Pinel, P. and Kleinschmidt, A. and Poline, J. B. and Thirion, B.},
doi = {10.1016/j.neuroimage.2010.02.010},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Varoquaux et al. - 2010 - A group model for stable multi-subject ICA on fMRI datasets.pdf:pdf},
isbn = {1095-9572 (Electronic)1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
number = {1},
pages = {288--299},
pmid = {20153834},
publisher = {Elsevier Inc.},
title = {{A group model for stable multi-subject ICA on fMRI datasets}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2010.02.010},
volume = {51},
year = {2010}
}
@article{Gonzalez2017,
abstract = {Bayesian optimization (BO) has emerged during the last few years as an effective approach to optimizing black-box functions where direct queries of the objective arc expensive. In this paper wc consider the case where direct access to the function is not possible, but information about user preferences is. Such scenarios arise in problems where human preferences are modeled, such as A/B tests or recommender systems. We present a new framework for this scenario that we call Preferential Bayesian Optimization (PBO) which allows us to find the optimum of a latent function that can only be queried through pairwise comparisons, the so-called duels. PBO extends the applicability of standard BO ideas and generalizes previous discrete dueling approaches by modeling the probability of the winner of each duel by means of a Gaussian process model with a Bernoulli likelihood. The latent preference function is used to define a family of acquisition functions that extend usual policies used in BO. We illustrate the benefits of PBO in a variety of exper-iments, showing that PBO needs drastically fewer comparisons for finding the optimum. According to our experiments, the way of modeling correlations in PBO is key in obtaining this advantage.},
archivePrefix = {arXiv},
arxivId = {1704.03651},
author = {Gonzalez, Javier and Dai, Zhenwen and Damianou, Andreas and Lawrence, Neil D.},
eprint = {1704.03651},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gonzalez et al. - 2017 - Preferential Bayesian optimization.pdf:pdf},
isbn = {9781510855144},
journal = {34th International Conference on Machine Learning, ICML 2017},
pages = {2080--2089},
title = {{Preferential Bayesian optimization}},
volume = {3},
year = {2017}
}
@article{Duncker2018,
abstract = {We introduce a novel scalable approach to identifying common latent structure in neural population spike-trains, which allows for variability both in the trajectory and in the rate of progression of the underlying computation. Our approach is based on shared latent Gaussian processes (GPs) which are combined linearly, as in the Gaussian Process Factor Analysis (GPFA) algorithm. We extend GPFA to handle unbinned spike-train data by incorporating a continuous time point-process likelihood model, achieving scalability with a sparse variational approximation. Shared variability is separated into terms that express condition dependence, as well as trial-to-trial variation in trajectories. Finally, we introduce a nested GP formulation to capture variability in the rate of evolution along the trajectory. We show that the new method learns to recover latent trajectories in synthetic data, and can accurately identify the trial-to-trial timing of movement-related parameters from motor cortical data without any supervision.},
author = {Duncker, Lea and Sahani, Maneesh},
doi = {10.1101/331751},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Duncker, Sahani - 2018 - Temporal alignment and latent Gaussian process factor inference in population spike trains.pdf:pdf},
journal = {The advances in Neural Information Processing Systems 31},
number = {Nips},
pages = {331751},
title = {{Temporal alignment and latent Gaussian process factor inference in population spike trains}},
url = {https://www.biorxiv.org/content/early/2018/05/27/331751},
year = {2018}
}
@article{Noh2018,
abstract = {We used pattern classifiers to extract features related to recognition memory retrieval from the temporal information in single-trial EEG data during attempted memory retrieval. Two-class classification was conducted on correctly remembered trials with accurate context (or source) judgments vs. correctly rejected trials. The average accuracy for datasets recorded in a single session was 61{\%} while the average accuracy for datasets recorded in two separate sessions was 56{\%}. To further understand the basis of the classifier's performance, two other pattern classifiers were trained on different pairs of behavioral conditions. The first of these was designed to use information related to remembering the item and the second to use information related to remembering the contextual information (or source) about the item. Mollison and Curran (2012) had earlier showed that subject's familiarity judgements contributed to improved memory of spatial contextual information but not of extrinsic associated color information.These behavioral results were similarly reflected in the event-related potential (ERP) known as the FN400 (an early frontal effect relating to familiarity) which revealed differences between correct and incorrect context memories in the spatial but not color conditions. In our analyses we show that a classifier designed to distinguish between correct and incorrect context memories, more strongly involves early activity (400-500 ms) over the frontal channels for the location distinctions, than for the extrinsic color associations. In contrast, the classifier designed to classify memory for the item (without memory for the context), had more frontal channel involvement for the color associated experiments than for the spatial experiments. Taken together these results argue that location may be bound more tightly with the item than an extrinsic color association. The multivariate classification approach also showed that trial-by-trial variation in EEG corresponding to these ERP components were predictive of subject's behavioral responses. Additionally, the multivariate classification approach enabled analysis of error conditions that did not have sufficient trials for standard ERP analyses. These results suggested that false alarms were primarily attributable to item memory (as opposed to memory of associated context), as commonly predicted, but with little previous corroborating EEG evidence.},
author = {Noh, Eunho and Liao, Kueida and Mollison, Matthew V. and Curran, Tim and de Sa, Virginia R.},
doi = {10.3389/fnhum.2018.00258},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Noh et al. - 2018 - Single-Trial EEG Analysis Predicts Memory Retrieval and Reveals Source-Dependent Differences.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {EEG, memory retrieval, old/new effect, multi-varia,eeg,frontiers in human neuroscience,frontiersin,memory retrieval,multi-variate analysis,new effect,old,org,prediction,www},
number = {July},
pages = {1--17},
title = {{Single-Trial EEG Analysis Predicts Memory Retrieval and Reveals Source-Dependent Differences}},
url = {https://www.frontiersin.org/article/10.3389/fnhum.2018.00258/full},
volume = {12},
year = {2018}
}
@inproceedings{Liu2017a,
abstract = {Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics. We develop a geometric perspective that views SVGD as a gradient flow of the KL divergence functional under a new metric structure on the space of distributions induced by Stein operator.},
archivePrefix = {arXiv},
arxivId = {1704.07520},
author = {Liu, Qiang},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1704.07520},
issn = {10495258},
title = {{Stein variational gradient descent as gradient flow}},
volume = {2017-Decem},
year = {2017}
}
@article{Kim2007,
abstract = {Data sets involving multiple groups with shared characteristics frequently arise in practice. In this paper we extend hierarchical Dirichlet processes to model such data. Each group is assumed to be generated from a template mixture model with group level variability in both the mixing proportions and the component parameters. Variabilities in mixing proportions across groups are handled using hierarchical Dirichlet processes, also allowing for automatic determination of the number of components. In addition, each group is allowed to have its own component parameters coming from a prior described by a template mixture model. This group-level variability in the component parameters is handled using a random effects model. We present a Markov Chain Monte Carlo (MCMC) sampling algorithm to estimate model parameters and demonstrate the method by applying it to the problem of modeling spatial brain activation patterns across multiple images collected via functional magnetic resonance imaging (fMRI).},
author = {Kim, Seyoung and Smyth, Padhraic},
doi = {10.7551/mitpress/7503.003.0092},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Smyth - 2007 - Hierarchical Dirichlet processes with random effects.pdf:pdf},
isbn = {9780262195683},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {697--704},
title = {{Hierarchical Dirichlet processes with random effects}},
year = {2007}
}
@article{Belevitch1959,
abstract = {The rank-frequency diagrams of statistical linguistics are reinterpreted as distribution curves of the cumulative probability of types in the ccttaloguc versus the probability of tokens in the text. For such distributions, the closure condition "Sum of p =1" (which does not hold in general statistics for the independent variables) imposes certain relations between the mean, the variance, the number of elements in the catalogue and the average information content (negative entropy). Sections 2 to 4 are devoted to the mathematics of these reations, especially to their particular forms for truncated normal distributions. First and second order Taylor approximations to an arbitrary distribution law take the form of Zipf's and Mandelbrot's laws respectively. Experimented data lead to accept the truncated normal distribution with sigma close to 2.8 bits as the general law for words. Data on letter and phoneme distributions seem to indicate that the standard deviation has the universal value sigma close to 1.4 bits.},
author = {Belevitch, V.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Belevitch - 1959 - On the statistical laws of linguistic distributions.pdf:pdf},
journal = {Annales de la Soci{\'{e}}t{\'{e}} Scientifique de Bruxelles},
number = {3},
pages = {310--326},
title = {{On the statistical laws of linguistic distributions}},
url = {http://www.csl.sri.com/users/neumann/belevitch.pdf},
volume = {73},
year = {1959}
}
@article{Lawrence2007,
author = {Lawrence, Neil D and Moore, Andrew J},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lawrence, Moore - 2007 - Hierarchical Gaussian Process Latent Variable Models.pdf:pdf},
title = {{Hierarchical Gaussian Process Latent Variable Models}},
year = {2007}
}
@article{Noe,
archivePrefix = {arXiv},
arxivId = {1808.06918v1},
author = {Noe, Umberto and Husmeier, Dirk},
eprint = {1808.06918v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Noe, Husmeier - Unknown - On a New Improvement-Based Acquisition Function for Bayesian Optimization.pdf:pdf},
keywords = {bayesian optimization,gaussian processes,global optimization,pulmonary},
title = {{On a New Improvement-Based Acquisition Function for Bayesian Optimization}}
}
@article{Nastase2019,
author = {Nastase, Samuel A and Gazzola, Valeria and Hasson, Uri and Keysers, Christian},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nastase et al. - 2019 - Measuring shared responses across subjects using intersubject correlation.pdf:pdf},
keywords = {communication,fmri,naturalistic stimuli,reliability,social cognition},
pages = {1--40},
title = {{Measuring shared responses across subjects using intersubject correlation}},
year = {2019}
}
@article{Dubno2013,
abstract = {Age-related hearing loss (presbyacusis) has a complex etiology. Results from animal models detailing the effects of specific cochlear injuries on audiometric profiles may be used to understand the mechanisms underlying hearing loss in older humans and predict cochlear pathologies associated with certain audiometric configurations ("audiometric phenotypes"). Patterns of hearing loss associated with cochlear pathology in animal models were used to define schematic boundaries of human audiograms. Pathologies included evidence for metabolic, sensory, and a mixed metabolic + sensory phenotype; an older normal phenotype without threshold elevation was also defined. Audiograms from a large sample of older adults were then searched by a human expert for "exemplars" (best examples) of these phenotypes, without knowledge of the human subject demographic information. Mean thresholds and slopes of higher frequency thresholds of the audiograms assigned to the four phenotypes were consistent with the predefined schematic boundaries and differed significantly from each other. Significant differences in age, gender, and noise exposure history provided external validity for the four phenotypes. Three supervised machine learning classifiers were then used to assess reliability of the exemplar training set to estimate the probability that newly obtained audiograms exhibited one of the four phenotypes. These procedures classified the exemplars with a high degree of accuracy; classifications of the remaining cases were consistent with the exemplars with respect to average thresholds and demographic information. These results suggest that animal models of age-related hearing loss can be used to predict human cochlear pathology by classifying audiograms into phenotypic classifications that reflect probable etiologies for hearing loss in older humans. {\textcopyright} 2013 Association for Research in Otolaryngology.},
author = {Dubno, Judy R. and Eckert, Mark A. and Lee, Fu Shing and Matthews, Lois J. and Schmiedt, Richard A.},
doi = {10.1007/s10162-013-0396-x},
file = {:Users/mshvarts/Downloads/10162{\_}2013{\_}Article{\_}396.pdf:pdf},
isbn = {1016201303},
issn = {15253961},
journal = {JARO - Journal of the Association for Research in Otolaryngology},
keywords = {animal models,audiogram classification,endocochlear potential,metabolic presbyacusis,sensory presbyacusis,supervised machine learning classifiers},
number = {5},
pages = {687--701},
pmid = {23740184},
title = {{Classifying human audiometric phenotypes of age-related hearing loss from animal models}},
volume = {14},
year = {2013}
}
@article{Kolda2009,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition:CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
archivePrefix = {arXiv},
arxivId = {1404.3905},
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
eprint = {1404.3905},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kolda, Bader - 2009 - Tensor Decompositions and Applications.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {tensor decompositions, multiway arrays, multilinea},
number = {3},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://epubs.siam.org/doi/10.1137/07070111X},
volume = {51},
year = {2009}
}
@article{Salvucci2016,
abstract = {For several decades, production systems have been the dominant framework in which (primarily) symbolic cognitive models have been developed. This paper proposes a different approach, cognitive code, in which behavioral models are developed directly in a modern programming language. However, unlike standard code, cognitive code has simulated timing and error characteristics intended to mimic those of human cognitive, perceptual, and motor processes. Some of the benefits of this new approach are illustrated in sample models of a paired-associates task, reading task, and dual-choice task.},
author = {Salvucci, Dario D},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salvucci - 2016 - Cognitive Code An Embedded Approach to Cognitive Modeling.pdf:pdf},
journal = {Proceedings of the 14th International Conference on Cognitive Modeling},
keywords = {act-r,c,cognitive architectures,develop code using modern,etc,java,learning,paradigms and patterns used,procedural and object-oriented,programming languages,python,the very different programming},
number = {Iccm},
pages = {15--20},
title = {{Cognitive Code : An Embedded Approach to Cognitive Modeling}},
year = {2016}
}
@inproceedings{Bower2009,
abstract = {Curcumin, a natural diphenolic compound derived from turmeric Curcuma longa, has proven to be a modulator of intracellular signaling pathways that control cancer cell growth, inflammation, invasion and apoptosis, revealing its anticancer potential. In this review, we focus on the design and development of nanoparticles, self-assemblies, nanogels, liposomes and complex fabrication for sustained and efficient curcumin delivery. We also discuss the anticancer applications and clinical benefits of nanocurcumin formulations. Only a few novel multifunctional and composite nanosystem strategies offer simultaneous therapy as well as imaging characteristics. We also summarize the challenges to developing curcumin delivery platforms and up-to-date solutions for improving curcumin bioavailability and anticancer potential for therapy.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bower, M.R. and Stead, Matt and Brinkmann, B.H. and Dufendach, Kevin and Worrell, G.A.},
booktitle = {2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
doi = {10.1109/IEMBS.2009.5333570},
eprint = {NIHMS150003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bower et al. - 2009 - Metadata and annotations for multi-scale electrophysiological data.pdf:pdf},
isbn = {9781424432967},
issn = {1878-5832},
month = {sep},
pages = {2811--2814},
pmid = {21959306},
publisher = {IEEE},
title = {{Metadata and annotations for multi-scale electrophysiological data}},
url = {http://ieeexplore.ieee.org/document/5333570/},
year = {2009}
}
@inproceedings{Gotovos2013,
author = {Gotovos, Alkis and Casati, Nathalie and Zurich, E T H and Hitz, Gregory and Krause, Andreas},
booktitle = {IJCAI 2013},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gotovos et al. - 2013 - Active Learning for Level Set Estimation.pdf:pdf},
keywords = {Machine Learning},
pages = {1344--1350},
title = {{Active Learning for Level Set Estimation}},
url = {papers3://publication/uuid/07F3A6A8-2905-4B9E-874A-556175725FAF},
year = {2013}
}
@article{Ngo2017,
author = {Ngo, Hung and Nguyen, XuanLong and Olteanu, Dan and Schleich, Maximilian},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ngo et al. - 2017 - In-Database Learning with Sparse Tensors.pdf:pdf},
pages = {1--37},
title = {{In-Database Learning with Sparse Tensors.}},
year = {2017}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
doi = {10.1017/S0952523813000308},
eprint = {1706.03762},
isbn = {1469-8714},
issn = {1469-8714},
pmid = {24016424},
title = {{Attention Is All You Need}},
year = {2017}
}
@inproceedings{Kingma2015a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{Miyake2012,
abstract = {Executive functions (EFs)—a set of general-purpose control processes that regulate one's thoughts and behaviors—have become a popular research topic lately and have been studied in many subdisciplines of psychological science. This article summarizes the EF research that our group has conducted to understand the nature of individual differences in EFs and their cognitive and biological underpinnings. In the context of a new theoretical framework that we have been developing (the unity/diversity framework), we describe four general conclusions that have emerged from our research. Specifically, we argue that individual differences in EFs, as measured with simple laboratory tasks, (1) show both unity and diversity (different EFs are correlated yet separable); (2) reflect substantial genetic contributions; (3) are related to various clinically and societally important phenomena; and (4) show some developmental stability.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Miyake, Akira and Friedman, Naomi P},
doi = {10.1177/0963721411429458.The},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Miyake, Friedman - 2012 - The Nature and Organisation of Individual Differences in Executive Functions Four General Conclusions.pdf:pdf},
isbn = {9788578110796},
issn = {0963-7214},
journal = {Current Directions in Psychological Science},
keywords = {behavioral genetics,cake,executive functions,for example,individual differences,it even,people can resist the,people differ greatly in,self-regulation,some,temptation to eat chocolate,their abilities to regulate,thoughts and behaviors,whereas others cannot help},
number = {1},
pages = {8--14},
pmid = {22773897},
title = {{The Nature and Organisation of Individual Differences in Executive Functions : Four General Conclusions}},
volume = {21},
year = {2012}
}
@inproceedings{Eleftheriadis2017,
abstract = {We address the task of simultaneous feature fusion and modeling of discrete ordinal outputs. We propose a novel Gaussian process(GP) auto-encoder modeling approach. In particular, we introduce GP encoders to project multiple observed features onto a latent space, while GP decoders are responsible for reconstructing the original features. Inference is performed in a novel variational framework, where the recovered latent representations are further constrained by the ordinal output labels. In this way, we seamlessly integrate the ordinal structure in the learned manifold, while attaining robust fusion of the input features. We demonstrate the representation abilities of our model on benchmark datasets from machine learning and affect analysis. We further evaluate the model on the tasks of feature fusion and joint ordinal prediction of facial action units. Our experiments demonstrate the benefits of the proposed approach compared to the state of the art.},
archivePrefix = {arXiv},
arxivId = {1608.04664},
author = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Deisenroth, Marc Peter and Pantic, Maja},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-54184-6_10},
eprint = {1608.04664},
isbn = {9783319541839},
issn = {16113349},
pmid = {10463930},
title = {{Variational gaussian process auto-Encoder for ordinal prediction of facial action units}},
volume = {10112 LNCS},
year = {2017}
}
@article{Figurnov2018,
abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
archivePrefix = {arXiv},
arxivId = {1805.08498},
author = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
eprint = {1805.08498},
title = {{Implicit Reparameterization Gradients}},
year = {2018}
}
@article{Nemeth2016,
abstract = {This paper proposes a new sampling scheme based on Langevin dynamics that is applicable within pseudo-marginal and particle Markov chain Monte Carlo algorithms. We investigate this algorithm's theoretical properties under standard asymptotics, which correspond to an increasing dimension of the parameters, {\$}n{\$}. Our results show that the behaviour of the algorithm depends crucially on how accurately one can estimate the gradient of the log target density. If the error in the estimate of the gradient is not sufficiently controlled as dimension increases, then asymptotically there will be no advantage over the simpler random-walk algorithm. However, if the error is sufficiently well-behaved, then the optimal scaling of this algorithm will be {\$}O(n{\^{}}{\{}-1/6{\}}){\$} compared to {\$}O(n{\^{}}{\{}-1/2{\}}){\$} for the random walk. Our theory also gives guidelines on how to tune the number of Monte Carlo samples in the likelihood estimate and the proposal step-size.},
author = {Nemeth, Christopher and Sherlock, Chris and Fearnhead, Paul},
doi = {10.1093/biomet/asw020},
issn = {14643510},
journal = {Biometrika},
number = {3},
title = {{Particle Metropolis-adjusted Langevin algorithms}},
volume = {103},
year = {2016}
}
@article{Nguyen2018,
abstract = {We present Vision-based Navigation with Language-based Assistance (VNLA), a grounded vision-language task where an agent with visual perception is guided via language to find objects in photorealistic indoor environments. The task emulates a real-world scenario in that (a) the requester may not know how to navigate to the target objects and thus makes requests by only specifying high-level end-goals, and (b) the agent is capable of sensing when it is lost and querying an advisor, who is more qualified at the task, to obtain language subgoals to make progress. To model language-based assistance, we develop a general framework termed Imitation Learning with Indirect Intervention (I3L), and propose a solution that is effective on the VNLA task. Empirical results show that this approach significantly improves the success rate of the learning agent over other baselines in both seen and unseen environments. Our code and data are publicly available at https://github.com/debadeepta/vnla .},
archivePrefix = {arXiv},
arxivId = {1812.04155},
author = {Nguyen, Khanh and Dey, Debadeepta and Brockett, Chris and Dolan, Bill},
eprint = {1812.04155},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2018 - Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention.pdf:pdf},
number = {c},
title = {{Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention}},
url = {http://arxiv.org/abs/1812.04155},
year = {2018}
}
@article{Deisenroth2015a,
abstract = {Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.},
archivePrefix = {arXiv},
arxivId = {1502.02860},
author = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
doi = {10.1109/TPAMI.2013.218},
eprint = {1502.02860},
file = {:Users/mshvarts/Downloads/06654139.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Bayesian inference,Control,Gaussian processes,Policy search,Reinforcement learning,Robotics},
number = {2},
pages = {408--423},
publisher = {IEEE},
title = {{Gaussian processes for data-efficient learning in robotics and control}},
volume = {37},
year = {2015}
}
@article{Micchelli2006,
abstract = {In this paper we investigate conditions on the features of a continuous kernel so that it may approx- imate an arbitrary continuous target function uniformly on any compact subset of the input space. A number of concrete examples are given of kernels with this universal approximating property.},
author = {Micchelli, Charles A and Xu, Yuesheng and Zhang, Haizang},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Micchelli, Xu, Zhang - 2006 - Universal Kernels.pdf:pdf},
isbn = {1532-4435},
issn = {1533-7928},
journal = {Journal of Machine Learning Research},
keywords = {density,radial kernels,translation invariant kernels},
pages = {2651--2667},
title = {{Universal Kernels}},
volume = {7},
year = {2006}
}
@inproceedings{Hoang2014,
author = {Hoang, Trong Nghia and Low, Kian Hsiang and Jaillet, Patrick and Kankanhalli, Mohan},
booktitle = {Proceedings of the 31 st International Conference on Machine Learning},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hoang et al. - 2014 - Nonmyopic $\epsilon$-Bayes-optimal active learning of Gaussian processes.pdf:pdf},
pages = {1--9},
title = {{Nonmyopic $\epsilon$-Bayes-optimal active learning of Gaussian processes}},
url = {papers://d471b97a-e92c-44c2-8562-4efc271c8c1b/Paper/p848},
year = {2014}
}
@article{Zhang2017f,
abstract = {L1-regularized multiway canonical correlation analysis (L1-MCCA) has been introduced to reference signal optimization in steady-state visual evoked potential (SSVEP)-based brain–computer interface (BCI). The effectiveness of L1-regularization on significant trial selection highly depends on the regularization parameter setting, which can be typically determined by cross-validation (CV). However, CV will substantially reduce the practicability of BCI system due to additional data requirement for the parameter validation and relatively high computational cost. To solve the problem, this study proposes a Bayesian version of L1-MCCA (called SBMCCA) by exploiting sparse Bayesian learning. The SBMCCA method avoids CV and can efficiently estimate the model parameters under the Bayesian evidence framework. Experimental results show that the SBMCCA method achieved comparable recognition accuracy but much higher computational efficiency in contrast to the L1-MCCA method.},
author = {Zhang, Yu and Zhou, Guoxu and Jin, Jing and Zhang, Yangsong and Wang, Xingyu and Cichocki, Andrzej},
doi = {10.1016/j.neucom.2016.11.008},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Sparse Bayesian multiway canonical correlation analysis for EEG pattern recognition(2).pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Brain–computer interface (BCI),Electroencephalogram (EEG),Multiway canonical correlation analysis (MCCA),Sparse Bayesian learning,Steady-state visual evoked potential (SSVEP)},
title = {{Sparse Bayesian multiway canonical correlation analysis for EEG pattern recognition}},
year = {2017}
}
@article{Liu2017,
abstract = {Copyright {\textcopyright} 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Electroencephalography (EEG) source analysis is one of the most important noninvasive human brain imaging tools that provides millisecond temporal accuracy. However, discovering essential activated brain sources associated with different brain status is still a challenging problem. In this study, we propose for the first time that the ill-posed EEG inverse problem can be formulated and solved as a sparse over-complete dictionary learning problem. In particular, a novel supervised sparse dictionary learning framework was developed for EEG source reconstruction. A revised version of discriminative K-SVD (DK-SVD) algorithm is exploited to solve the formulated supervised dictionary learning problem. As the proposed learning framework incorporated the EEG label information of different brain status, it is capable of learning a sparse representation that reveal the most discriminative brain activity sources among different brain states. Compared to the state-of-the-art EEG source analysis methods, proposed sparse dictionary learning framework achieved significant superior performance in both computing speed and accuracy for the challenging EEG source reconstruction problem through extensive numerical experiments. More importantly, the experimental results also validated that the proposed sparse learning framework is effective to discover the discriminative task-related brain activation sources, which shows the potential to advance the high resolution EEG source analysis for real-time non-invasive brain imaging research.},
author = {Liu, Feng and Wang, Shouyi and Rosenberger, Jay and Su, Jianzhong and Liu, Hanli},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2017 - A Sparse Dictionary Learning Framework to Discover Discriminative Source Activations in EEG Brain Mapping.pdf:pdf},
journal = {Proceedings of the 31th Conference on Artificial Intelligence (AAAI 2017)},
keywords = {Machine Learning Applications},
pages = {1431--1437},
title = {{A Sparse Dictionary Learning Framework to Discover Discriminative Source Activations in EEG Brain Mapping}},
year = {2017}
}
@article{Jangraw2014,
abstract = {OBJECTIVE: As we move through an environment, we are constantly making assessments, judgments and decisions about the things we encounter. Some are acted upon immediately, but many more become mental notes or fleeting impressions-our implicit 'labeling' of the world. In this paper, we use physiological correlates of this labeling to construct a hybrid brain-computer interface (hBCI) system for efficient navigation of a 3D environment. APPROACH: First, we record electroencephalographic (EEG), saccadic and pupillary data from subjects as they move through a small part of a 3D virtual city under free-viewing conditions. Using machine learning, we integrate the neural and ocular signals evoked by the objects they encounter to infer which ones are of subjective interest to them. These inferred labels are propagated through a large computer vision graph of objects in the city, using semi-supervised learning to identify other, unseen objects that are visually similar to the labeled ones. Finally, the system plots an efficient route to help the subjects visit the 'similar' objects it identifies. MAIN RESULTS: We show that by exploiting the subjects' implicit labeling to find objects of interest instead of exploring naively, the median search precision is increased from 25{\%} to 97{\%}, and the median subject need only travel 40{\%} of the distance to see 84{\%} of the objects of interest. We also find that the neural and ocular signals contribute in a complementary fashion to the classifiers' inference of subjects' implicit labeling. SIGNIFICANCE: In summary, we show that neural and ocular signals reflecting subjective assessment of objects in a 3D environment can be used to inform a graph-based learning model of that environment, resulting in an hBCI system that improves navigation and information delivery specific to the user's interests.},
author = {Jangraw, David C. and Wang, Jun and Lance, Brent J. and Chang, Shih Fu and Sajda, Paul},
doi = {10.1088/1741-2560/11/4/046003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jangraw et al. - 2014 - Neurally and ocularly informed graph-based models for searching 3D environments.pdf:pdf},
isbn = {1741-2552},
issn = {17412552},
journal = {Journal of Neural Engineering},
keywords = {Brain-computer interface,Computer vision,EEG,Eye tracking,Pupillometry},
number = {4},
pmid = {24891496},
title = {{Neurally and ocularly informed graph-based models for searching 3D environments}},
volume = {11},
year = {2014}
}
@article{Shi2011,
abstract = {Collective classification in relational data has become an important and active research topic in the last decade. It exploits the dependencies of instances in a network to improve predictions. Related applications include hyperlinked document classification, social network analysis and collaboration network analysis. Most of the traditional collective classification models mainly study the scenario that there exists a large amount of labeled examples (labeled nodes). However, in many real-world applications, labeled data are extremely difficult to obtain. For example, in network intrusion detection, there may be only a limited number of identified intrusions whereas there are a huge set of unlabeled nodes. In this situation, most of the data have no connection to labeled nodes; hence, no supervision knowledge can be obtained from the local connections. In this paper, we propose to explore various latent linkages among the nodes and judiciously integrate the linkages to generate a latent graph. This is achieved by finding a graph that maximizes the linkages among the training data with the same label, and maximizes the separation among the data with different labels. The objective is further cast into an optimization problem and is solved with quadratic programming. Finally, we apply label propagation on the latent graph to make prediction. Experiments show that the proposed model LNP (Latent Network Propagation) can improve the learning accuracy significantly. For instance, when there are only 10{\%} of labeled examples, the accuracies of all the comparison models are less than 63{\%}, while that of the proposed model is 74{\%}. {\textcopyright} 2011 ACM.},
author = {Shi, Xiaoxiao and Li, Yao and Yu, Philip},
doi = {10.1145/2063576.2063739},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shi, Li, Yu - 2011 - Collective prediction with latent graphs.pdf:pdf},
isbn = {9781450307178},
journal = {Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11},
pages = {1127},
title = {{Collective prediction with latent graphs}},
url = {http://dl.acm.org/citation.cfm?doid=2063576.2063739},
year = {2011}
}
@article{Park,
archivePrefix = {arXiv},
arxivId = {arXiv:1302.5964v1},
author = {Park, Il Memming and Seth, Sohan and Paiva, R C and Li, Lin and Pr, C},
eprint = {arXiv:1302.5964v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - Unknown - Kernel methods on spike train space for neuroscience a tutorial.pdf:pdf},
pages = {1--12},
title = {{Kernel methods on spike train space for neuroscience : a tutorial}}
}
@article{Zhang2020,
abstract = {Although Bayesian Optimization (BO) has been employed for accelerating materials design in computational materials engineering, existing works are restricted to problems with quantitative variables. However, real designs of materials systems involve both qualitative and quantitative design variables representing material compositions, microstructure morphology, and processing conditions. For mixed-variable problems, existing Bayesian Optimization (BO) approaches represent qualitative factors by dummy variables first and then fit a standard Gaussian process (GP) model with numerical variables as the surrogate model. This approach is restrictive theoretically and fails to capture complex correlations between qualitative levels. We present in this paper the integration of a novel latent-variable (LV) approach for mixed-variable GP modeling with the BO framework for materials design. LVGP is a fundamentally different approach that maps qualitative design variables to underlying numerical LV in GP, which has strong physical justification. It provides flexible parameterization and representation of qualitative factors and shows superior modeling accuracy compared to the existing methods. We demonstrate our approach through testing with numerical examples and materials design examples. The chosen materials design examples represent two different scenarios, one on concurrent materials selection and microstructure optimization for optimizing the light absorption of a quasi-random solar cell, and another on combinatorial search of material constitutes for optimal Hybrid Organic-Inorganic Perovskite (HOIP) design. It is found that in all test examples the mapped LVs provide intuitive visualization and substantial insight into the nature and effects of the qualitative factors. Though materials designs are used as examples, the method presented is generic and can be utilized for other mixed variable design optimization problems that involve expensive physics-based simulations.},
archivePrefix = {arXiv},
arxivId = {1910.01688},
author = {Zhang, Yichi and Apley, Daniel W. and Chen, Wei},
doi = {10.1038/s41598-020-60652-9},
eprint = {1910.01688},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Apley, Chen - 2020 - Bayesian Optimization for Materials Design with Mixed Quantitative and Qualitative Variables.pdf:pdf},
isbn = {4159802060},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--13},
pmid = {32188873},
title = {{Bayesian Optimization for Materials Design with Mixed Quantitative and Qualitative Variables}},
volume = {10},
year = {2020}
}
@article{Georgeson1975,
author = {Georgeson, M A and Sullivan, G D},
doi = {10.1113/jphysiol.1975.sp011162},
file = {:Users/mshvarts/Downloads/jphysiol.1975.sp011162.pdf:pdf},
issn = {00223751},
journal = {The Journal of Physiology},
month = {nov},
number = {3},
pages = {627--656},
title = {{Contrast constancy: deblurring in human vision by spatial frequency channels.}},
url = {http://doi.wiley.com/10.1113/jphysiol.1975.sp011162},
volume = {252},
year = {1975}
}
@article{Eleftheriadis2013,
author = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Pantic, Maja},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Eleftheriadis, Rudovic, Pantic - 2013 - Shared Gaussian Process Latent Variable Model for Multi-view Facial Expression Recognition.pdf:pdf},
isbn = {978-3-319-14364-4; 978-3-319-14363-7},
journal = {International Symposium on Visual Computing},
pages = {527--538},
title = {{Shared Gaussian Process Latent Variable Model for Multi-view Facial Expression Recognition}},
year = {2013}
}
@article{Silver,
archivePrefix = {arXiv},
arxivId = {1712.01815v1},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
eprint = {1712.01815v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Silver et al. - Unknown - Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm arXiv 1712 . 01815v1.pdf:pdf},
pages = {1--19},
title = {{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm arXiv : 1712 . 01815v1 [ cs . AI ] 5 Dec 2017}}
}
@article{Chen1999,
abstract = {Fluorescence correlation spectroscopy (FCS) is generally used to obtain information about the number of fluorescent particles in a small volume and the diffusion coefficient from the autocorrelation function of the fluorescence signal. Here we demonstrate that photon counting histogram (PCH) analysis constitutes a novel tool for extracting quantities from fluorescence fluctuation data, i.e. the measured photon counts per molecule and the average number of molecules within the observation volume. The photon counting histogram of fluorescence fluctuation experiments, in which few molecules are present in the excitation volume exhibits a super-Poissonian behavior: The additional broadening of the PCH compared to a Poisson distribution is due to fluorescence intensity fluctuations. For diffusing particles these intensity fluctuations are caused by an inhomogeneous excitation profile and the fluctuations in the number of particles in the observation volume N̄. The quantitative relationship between the detected photon counts and the fluorescence intensity reaching the detector is given by Mandel's formula. Based on this equation and considering the fluorescence intensity distribution in the two-photon excitation volume, a theoretical expression for the PCH as a function of the number of molecules in the excitation volume is derived. For a single molecular species two parameters are sufficient to characterize the histogram completely, namely the average number of molecules within the observation volume and the detected photon counts per molecule per sampling time $\epsilon$. The PCH for multiple molecular species, on the other hand, is generated by successively convoluting the photon counting distribution of each species with the others. The influence of the excitation profile upon the photon counting statistics for two relevant point spread functions (PSFs), the three-dimensional Gaussian PSF conventionally employed in confocal detection and the square of the Gaussian- Lorentzian PSF for two photon excitation, is explicitly treated. Measured photon counting distributions obtained with a two-photon excitation source agree, within experimental error with the theoretical PCHs calculated for the square of a Gaussian-Lorentzian beam profile. We demonstrate and discuss the influence of the average number of particles within the observation volume and the detected photon counts per molecule per sampling interval upon the super-Poissonian character of the photon counting distribution.},
author = {Chen, Yan and M{\"{u}}ller, Joachim D. and So, Peter T.C. and Gratton, Enrico},
doi = {10.1016/S0006-3495(99)76912-2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 1999 - The photon counting histogram in fluorescence fluctuation spectroscopy.pdf:pdf},
issn = {00063495},
journal = {Biophysical Journal},
number = {1},
pages = {553--567},
title = {{The photon counting histogram in fluorescence fluctuation spectroscopy}},
volume = {77},
year = {1999}
}
@article{Lewis2014,
abstract = {We propose a framework for including information processing bounds in rational analyses. It is an appli- cation of bounded optimality (Russell {\&} Subramanian, 1995) to the challenges of developing theories of mechanism and behavior. ?e framework is based on the idea that behaviors are generated by cognitive mechanisms that are adapted to the structure of not only the environment, but also the mind and brain itself. We call the framework computational rationality to emphasize the incorporation of computational mechanism into the de?nition of rational action. ?eories are speci?ed as optimal program problems, de?ned by an adaptation environment, a bounded machine, and a utility function. Such theories yield different classes of explanation, depending on the extent to which they emphasize adaptation to bounds, and adaptation to some ecology that differs fromtheimmediate local environment.We illustrate this vari- ation with examples fromthree domains: visual attention in a linguistic task,manual response ordering, and reasoning. We explore the relation of this framework to existing “levels” approaches to explanation, and to other optimality-basedmodeling approaches. 1},
author = {Lewis, Richard L. and Howes, Andrew and Singh, Satinder},
doi = {10.1111/tops.12086},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lewis, Howes, Singh - 2014 - Computational rationality Linking mechanism and behavior through bounded utility maximization.pdf:pdf},
isbn = {1756-8765},
issn = {17568765},
journal = {Topics in Cognitive Science},
keywords = {Bounded optimality,Bounded rationality,Cognitive architecture,Cognitive modeling,Rational analysis,Rationality,Utility maximization},
number = {2},
pages = {279--311},
pmid = {24648415},
title = {{Computational rationality: Linking mechanism and behavior through bounded utility maximization}},
volume = {6},
year = {2014}
}
@article{Dostal2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1906.00900v1},
author = {Dostal, L and Namachchivaya, N Sri},
eprint = {arXiv:1906.00900v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Dostal, Namachchivaya - 2012 - First Passage Time of Nonlinear Diffusion Processes with Singular Boundary.pdf:pdf},
keywords = {first passage time,random excitation,stochastic dynamics},
pages = {1--24},
title = {{First Passage Time of Nonlinear Diffusion Processes with Singular Boundary}},
year = {2012}
}
@inproceedings{Snoek2012,
abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
eprint = {1206.2944},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Larmarange et al. - 2017 - Social and implementation research for ending AIDS in Africa.pdf:pdf},
month = {jun},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
url = {http://arxiv.org/abs/1206.2944},
year = {2012}
}
@article{Finn2016,
abstract = {Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.},
archivePrefix = {arXiv},
arxivId = {1603.00448},
author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
doi = {1603.00448v3},
eprint = {1603.00448},
isbn = {9781510829008},
title = {{Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization}},
year = {2016}
}
@article{Chen2014,
abstract = {Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.},
archivePrefix = {arXiv},
arxivId = {1402.4102},
author = {Chen, Tianqi and Fox, Emily B. and Guestrin, Carlos},
doi = {10.1016/j.apnr.2015.05.018},
eprint = {1402.4102},
isbn = {9781634393973},
issn = {08971897},
title = {{Stochastic Gradient Hamiltonian Monte Carlo}},
year = {2014}
}
@inproceedings{Tishby2015,
abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1503.02406},
author = {Tishby, Naftali and Zaslavsky, Noga},
booktitle = {2015 IEEE Information Theory Workshop, ITW 2015},
doi = {10.1109/ITW.2015.7133169},
eprint = {1503.02406},
isbn = {9781479955268},
issn = {1058-4838},
pmid = {10433581},
title = {{Deep learning and the information bottleneck principle}},
year = {2015}
}
@article{Frank2012,
author = {Frank, Michael J and Badre, David},
doi = {10.1093/cercor/bhr114},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Frank, Badre - 2012 - Mechanisms of Hierarchical Reinforcement Learning in Corticostriatal Circuits 1 Computational Analysis.pdf:pdf},
keywords = {basal ganglia,computational model,hierarchical reinforcement,learning,prefrontal cortex},
number = {March},
pages = {509--526},
title = {{Mechanisms of Hierarchical Reinforcement Learning in Corticostriatal Circuits 1 : Computational Analysis}},
year = {2012}
}
@article{Sønderby2016,
abstract = {Variational Autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
archivePrefix = {arXiv},
arxivId = {1602.02282},
author = {S{\o}nderby, Casper Kaae and Raiko, Tapani and Maal{\o}e, Lars and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
doi = {10.1353/cjl.2008.0013},
eprint = {1602.02282},
issn = {10495258},
pmid = {548515},
title = {{Ladder Variational Autoencoders}},
year = {2016}
}
@article{Wilson2014,
author = {Wilson, Andrew Gordon},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilson - 2014 - Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes.pdf:pdf},
number = {January},
title = {{Covariance Kernels for Fast Automatic Pattern Discovery and Extrapolation with Gaussian Processes}},
year = {2014}
}
@article{Larsen2015,
abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.},
archivePrefix = {arXiv},
arxivId = {1512.09300},
author = {Larsen, Anders Boesen Lindbo and S{\o}nderby, S{\o}ren Kaae and Larochelle, Hugo and Winther, Ole},
eprint = {1512.09300},
isbn = {9781510829008},
issn = {1938-7228},
title = {{Autoencoding beyond pixels using a learned similarity metric}},
year = {2015}
}
@article{Aston-Jones2005,
abstract = {Historically, the locus coeruleus-norepinephrine (LC-NE) system has been implicated in arousal, but recent findings suggest that this system plays a more complex and specific role in the control of behavior than investigators previously thought. We review neurophysiological and modeling studies in monkey that support a new theory of LC-NE function. LC neurons exhibit two modes of activity, phasic and tonic. Phasic LC activation is driven by the outcome of task-related decision processes and is proposed to facilitate ensuing behaviors and to help optimize task performance (exploitation). When utility in the task wanes, LC neurons exhibit a tonic activity mode, associated with disengagement from the current task and a search for alternative behaviors (exploration). Monkey LC receives prominent, direct inputs from the anterior cingulate (ACC) and orbitofrontal cortices (OFC), both of which are thought to monitor task-related utility. We propose that these frontal areas produce the above patterns of LC activity to optimize utility on both short and long timescales.},
author = {Aston-Jones, Gary and Cohen, Jonathan D},
doi = {10.1146/annurev.neuro.28.061604.135709},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Aston-Jones, Cohen - 2005 - An integrative theory of locus coeruleus-norepinephrine function adaptive gain and optimal performance.pdf:pdf},
issn = {0147-006X},
journal = {Annual review of neuroscience},
keywords = {anterior cingulate cortex,decision making,neuromodulation,optimization,orbitofrontal cortex,utility},
pages = {403--50},
pmid = {16022602},
title = {{An integrative theory of locus coeruleus-norepinephrine function: adaptive gain and optimal performance.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16022602},
volume = {28},
year = {2005}
}
@article{Buchanan2018,
abstract = {Calcium imaging has revolutionized systems neuroscience, providing the ability to image large neural populations with single-cell resolution. The resulting datasets are quite large, which has presented a barrier to routine open sharing of this data, slowing progress in reproducible research. State of the art methods for analyzing this data are based on non-negative matrix factorization (NMF); these approaches solve a non-convex optimization problem, and are effective when good initializations are available, but can break down in low-SNR settings where common initialization approaches fail. Here we introduce an approach to compressing and denoising functional imaging data. The method is based on a spatially-localized penalized matrix decomposition (PMD) of the data to separate (low-dimensional) signal from (temporally-uncorrelated) noise. This approach can be applied in parallel on local spatial patches and is therefore highly scalable, does not impose non-negativity constraints or require stringent identifiability assumptions (leading to significantly more robust results compared to NMF), and estimates all parameters directly from the data, so no hand-tuning is required. We have applied the method to a wide range of functional imaging data (including one-photon, two-photon, three-photon, widefield, somatic, axonal, dendritic, calcium, and voltage imaging datasets): in all cases, we observe {\~{}}2-4x increases in SNR and compression rates of 20-300x with minimal visible loss of signal, with no adjustment of hyperparameters; this in turn facilitates the process of demixing the observed activity into contributions from individual neurons. We focus on two challenging applications: dendritic calcium imaging data and voltage imaging data in the context of optogenetic stimulation. In both cases, we show that our new approach leads to faster and much more robust extraction of activity from the data.},
archivePrefix = {arXiv},
arxivId = {1807.06203},
author = {Buchanan, E. Kelly and Kinsella, Ian and Zhou, Ding and Zhu, Rong and Zhou, Pengcheng and Gerhard, Felipe and Ferrante, John and Ma, Ying and Kim, Sharon and Shaik, Mohammed and Liang, Yajie and Lu, Rongwen and Reimer, Jacob and Fahey, Paul and Muhammad, Taliah and Dempsey, Graham and Hillman, Elizabeth and Ji, Na and Tolias, Andreas and Paninski, Liam},
eprint = {1807.06203},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Buchanan et al. - 2018 - Penalized matrix decomposition for denoising, compression, and improved demixing of functional imaging data.pdf:pdf},
pages = {1--36},
title = {{Penalized matrix decomposition for denoising, compression, and improved demixing of functional imaging data}},
url = {http://arxiv.org/abs/1807.06203},
year = {2018}
}
@article{Horvitz1988,
abstract = {Although many investigators arm a desire to build reasoning systems that behave consistently with the axiomatic basis de ned by probability theory and utility theory, limited resources for engineering and computation can make a complete normative analysis impossible. We attempt to move discussion beyond the debate over the scope of problems that can be handled e ectively to cases where it is clear that there are in- sucient computational resources to perform an analysis deemed as complete. Under these conditions, we stress the importance of considering the expected costs and bene ts of applying alternative approximation procedures and heuristics for computation and knowledge acquisition. We discuss how knowledge about the structure of user utility can be used to control value tradeo s for tailoring inference to alternative contexts. We address the notion of real-time rationality, focusing on the application of knowledge about the expected timewise-re nement abilities of reasoning strategies to balance the bene ts of additional computation with the costs of acting with a partial result. We dis- cuss the bene ts of applying decision theory to control the solution of dicult problems given limitations and uncertainty in reasoning resources.},
archivePrefix = {arXiv},
arxivId = {1304.2759},
author = {Horvitz, Eric J},
doi = {10.1016/0888-613X(88)90148-X},
eprint = {1304.2759},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Horvitz - 1988 - Reasoning about beliefs and actions under computational resource constraints.pdf:pdf},
isbn = {0-444-87417-8},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
month = {jul},
number = {3},
pages = {337--338},
pmid = {606},
title = {{Reasoning about beliefs and actions under computational resource constraints}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0888613X8890148X},
volume = {2},
year = {1988}
}
@article{Cutajar2019,
abstract = {Multi-fidelity methods are prominently used when cheaply-obtained, but possibly biased and noisy, observations must be effectively combined with limited or expensive true data in order to construct reliable models. This arises in both fundamental machine learning procedures such as Bayesian optimization, as well as more practical science and engineering applications. In this paper we develop a novel multi-fidelity model which treats layers of a deep Gaussian process as fidelity levels, and uses a variational inference scheme to propagate uncertainty across them. This allows for capturing nonlinear correlations between fidelities with lower risk of overfitting than existing methods exploiting compositional structure, which are conversely burdened by structural assumptions and constraints. We show that the proposed approach makes substantial improvements in quantifying and propagating uncertainty in multi-fidelity set-ups, which in turn improves their effectiveness in decision making pipelines.},
archivePrefix = {arXiv},
arxivId = {1903.07320},
author = {Cutajar, Kurt and Pullin, Mark and Damianou, Andreas and Lawrence, Neil and Gonz{\'{a}}lez, Javier},
eprint = {1903.07320},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cutajar et al. - 2019 - Deep Gaussian Processes for Multi-fidelity Modeling.pdf:pdf},
number = {2},
title = {{Deep Gaussian Processes for Multi-fidelity Modeling}},
url = {http://arxiv.org/abs/1903.07320},
volume = {1},
year = {2019}
}
@article{La1996,
author = {La, D and Chen, G},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/La, Chen - 1996 - Dynamical Systems Identification from Time-Series Data A Hankel Matrix Approach.pdf:pdf},
keywords = {-conditional},
number = {3},
title = {{Dynamical Systems Identification from Time-Series Data : A Hankel Matrix Approach}},
volume = {24},
year = {1996}
}
@article{Srivastava2017,
abstract = {Research in psychology and neuroscience has successfully modeled decision making as a process of noisy evidence accumulation to a decision bound. While there are several variants and implementations of this idea, the majority of these models make use of a noisy accumulation between two absorbing boundaries. A common assumption of these models is that decision parameters, e.g., the rate of accumulation (drift rate), remain fixed over the course of a decision, allowing the derivation of analytic formulas for the probabilities of hitting the upper or lower decision threshold, and the mean decision time. There is reason to believe, however, that many types of behavior would be better described by a model in which the parameters were allowed to vary over the course of the decision process. In this paper, we use martingale theory to derive formulas for the mean decision time, hitting probabilities, and first passage time (FPT) densities of a Wiener process with time-varying drift between two time-varying absorbing boundaries. This model was first studied by Ratcliff (1980) in the two-stage form, and here we consider the same model for an arbitrary number of stages (i.e. intervals of time during which parameters are constant). Our calculations enable direct computation of mean decision times and hitting probabilities for the associated multistage process. We also provide a review of how martingale theory may be used to analyze similar models employing Wiener processes by re-deriving some classical results. In concert with a variety of numerical tools already available, the current derivations should encourage mathematical analysis of more complex models of decision making with time-varying evidence.},
archivePrefix = {arXiv},
arxivId = {1508.03373},
author = {Srivastava, Vaibhav and Feng, Samuel F and Cohen, Jonathan D and Leonard, Naomi Ehrich and Shenhav, Amitai},
doi = {10.1016/j.jmp.2016.10.001},
eprint = {1508.03373},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2017 - A martingale analysis of first passage times of time-dependent Wiener diffusion models.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
month = {aug},
pages = {94--110},
pmid = {28630524},
title = {{A martingale analysis of first passage times of time-dependent Wiener diffusion models}},
url = {http://arxiv.org/abs/1508.03373},
volume = {77},
year = {2017}
}
@article{Li2015b,
abstract = {Effective training of deep neural networks suffers from two main issues. The first is that the parameter spaces of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk. We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models.},
archivePrefix = {arXiv},
arxivId = {1512.07666},
author = {Li, Chunyuan and Chen, Changyou and Carlson, David and Carin, Lawrence},
eprint = {1512.07666},
isbn = {9781577357605},
title = {{Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks}},
year = {2015}
}
@article{Xie2019,
abstract = {Despite its success, deep learning still needs large labeled datasets to succeed. Data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA outperforms the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN with 1,000 examples, UDA outperforms all previous approaches and reduces more than {\$}30\backslash{\%}{\$} of the error rates of state-of-the-art methods: going from 7.66{\%} to 5.27{\%} and from 3.53{\%} to 2.46{\%} respectively. UDA also works well on datasets that have a lot of labeled data. For example, on ImageNet, with 1.3M extra unlabeled data, UDA improves the top-1/top-5 accuracy from 78.28/94.36{\%} to 79.04/94.45{\%} when compared to AutoAugment.},
archivePrefix = {arXiv},
arxivId = {1904.12848},
author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
eprint = {1904.12848},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Xie et al. - 2019 - Unsupervised Data Augmentation.pdf:pdf},
pages = {1--20},
title = {{Unsupervised Data Augmentation}},
url = {http://arxiv.org/abs/1904.12848},
year = {2019}
}
@article{Owen2017,
author = {Owen, Lucy L W and Heusser, Andrew C and Manning, Jeremy R},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Owen, Heusser, Manning - 2017 - A Gaussian process model of human electrocorticographic data.pdf:pdf},
keywords = {ecog,electrocorticography,epilepsy,gaussian process regression,ieeg,intracranial electroencephalography,lfp,local field,maximum likelihood estimation,potential},
title = {{A Gaussian process model of human electrocorticographic data}},
year = {2017}
}
@article{Salvucci2008,
abstract = {The authors propose the idea of threaded cognition, an integrated theory of concurrent multitasking--that is, performing 2 or more tasks at once. Threaded cognition posits that streams of thought can be represented as threads of processing coordinated by a serial procedural resource and executed across other available resources (e.g., perceptual and motor resources). The theory specifies a parsimonious mechanism that allows for concurrent execution, resource acquisition, and resolution of resource conflicts, without the need for specialized executive processes. By instantiating this mechanism as a computational model, threaded cognition provides explicit predictions of how multitasking behavior can result in interference, or lack thereof, for a given set of tasks. The authors illustrate the theory in model simulations of several representative domains ranging from simple laboratory tasks such as dual-choice tasks to complex real-world domains such as driving and driver distraction.},
author = {Salvucci, Dario D. and Taatgen, Niels A.},
doi = {10.1037/0033-295X.115.1.101},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salvucci, Taatgen - 2008 - Threaded Cognition An Integrated Theory of Concurrent Multitasking.pdf:pdf},
isbn = {0033-295X},
issn = {0033295X},
journal = {Psychological Review},
keywords = {cognitive architectures,driving,dual-task performance,executive control,perceptual refractory period},
number = {1},
pages = {101--130},
pmid = {18211187},
title = {{Threaded Cognition: An Integrated Theory of Concurrent Multitasking}},
volume = {115},
year = {2008}
}
@article{Baydin2015,
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
archivePrefix = {arXiv},
arxivId = {1502.05767},
author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
doi = {10.1016/j.advwatres.2018.01.009},
eprint = {1502.05767},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Baydin et al. - 2015 - Automatic differentiation in machine learning a survey.pdf:pdf},
isbn = {0002-9645 (Print)$\backslash$r0002-9645 (Linking)},
issn = {15337928},
pmid = {7020497},
title = {{Automatic differentiation in machine learning: a survey}},
url = {http://arxiv.org/abs/1502.05767},
year = {2015}
}
@article{Kamesawa2018,
abstract = {Learning using privileged information is an attractive problem setting that helps many learning scenarios in the real world. A state-of-the-art method of Gaussian process classification (GPC) with privileged information is GPC+, which incorporates privileged information into a noise term of the likelihood. A drawback of GPC+ is that it requires numerical quadrature to calculate the posterior distribution of the latent function, which is extremely time-consuming. To overcome this limitation, we propose a novel classification method with privileged information based on Gaussian processes, called "soft-label-transferred Gaussian process (SLT-GP)." Our basic idea is that we construct another learning task of predicting soft labels (continuous values) obtained from privileged information and we perform transfer learning from this task to the target task of predicting hard labels. We derive a PAC-Bayesian bound of our proposed method, which justifies optimizing hyperparameters by the empirical Bayes method. We also experimentally show the usefulness of our proposed method compared with GPC and GPC+.},
archivePrefix = {arXiv},
arxivId = {1802.03877},
author = {Kamesawa, Ryosuke and Sato, Issei and Sugiyama, Masashi},
eprint = {1802.03877},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kamesawa, Sato, Sugiyama - 2018 - Gaussian Process Classification with Privileged Information by Soft-to-Hard Labeling Transfer.pdf:pdf},
pages = {1--20},
title = {{Gaussian Process Classification with Privileged Information by Soft-to-Hard Labeling Transfer}},
url = {http://arxiv.org/abs/1802.03877},
year = {2018}
}
@article{Lyu2018,
abstract = {We consider the problem of learning the level set for which a noisy black-box function exceeds a given threshold. To efficiently reconstruct the level set, we investigate Gaussian process (GP) metamodels. Our focus is on strongly stochastic samplers, in particular with heavy-tailed simulation noise and low signal-to-noise ratio. To guard against noise misspecification, we assess the performance of three variants: (i) GPs with Student-{\$}t{\$} observations; (ii) Student-{\$}t{\$} processes (TPs); and (iii) classification GPs modeling the sign of the response. In conjunction with these metamodels, we analyze several acquisition functions for guiding the sequential experimental designs, extending existing stepwise uncertainty reduction criteria to the stochastic contour-finding context. This also motivates our development of (approximate) updating formulas to efficiently compute such acquisition functions. Our schemes are benchmarked by using a variety of synthetic experiments in 1--6 dimensions. We also consider an application of level set estimation for determining the optimal exercise policy of Bermudan options in finance.},
archivePrefix = {arXiv},
arxivId = {1807.06712},
author = {Lyu, Xiong and Binois, Mickael and Ludkovski, Michael},
eprint = {1807.06712},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lyu, Binois, Ludkovski - 2018 - Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation.pdf:pdf},
pages = {1--36},
title = {{Evaluating Gaussian Process Metamodels and Sequential Designs for Noisy Level Set Estimation}},
url = {http://arxiv.org/abs/1807.06712},
year = {2018}
}
@article{Saif-Ur-Rehman,
abstract = {In electrophysiology, microelectrodes are the primary source for recording neural data of single neurons (single unit activity). These Microelectrodes can be implanted individually, or in the form of microelectrode arrays, consisting of hundreds of electrodes. During recording, some channels capture the activity of neurons, which is usually contaminated with noise and/or artifacts. Another considerable fraction of channels does not record any neural data, but only external artifacts and/or noise. Furthermore, some units get lost from a channel over time, or new units appear on a channel during the recording. Therefore, an automatic identification and continuous monitoring of channels containing neural data is of great significance and can accelerate the process of analysis, e.g. automatic selection of meaningful channels during offline and online spike sorting. Another important aspect is the selection of meaningful channels during online decoding in brain-computer interface applications, where threshold crossing events are usually considered for feature extraction, even though they don't necessarily correspond to neural events. Here, we propose a novel algorithm based on newly proposed way of feature vector extraction and supervised deep learning algorithm: a universal spike classifier (USC). The USC enables us to address both above-raised issues. The USC uses a state-of-the-art deep convolutional neural networks (CNNs) architecture. It takes a batch of raw waveforms as input, propagates it through multiple layered structures, and finally classifies it as a channel containing neural spike data or only artifacts/noise. We have trained the model of USC on data recorded from a single tetraplegic patient with two Utah arrays implanted in different areas of the brain. The trained model was then evaluated on data collected from six epileptic patients implanted with depth electrodes and two tetraplegic patients implanted with two Utah arrays, separately. The implanted electrodes targeted different areas of the brain. The test accuracy was 97.20{\%} on 1.56 million hand labeled test inputs, collected from all eight patients. The results demonstrate that the USC generalizes not only to the new data, but also to brain areas, subjects, and electrodes not used for training. 2},
author = {Saif-Ur-Rehman, Muhammad and Lienk{\"{a}}mper, Robin and Parpaley, Yaroslav and Wellmer, J{\"{o}}rg and Liu, Charles and Lee, Brian and Kellis, Spencer and Andersen, Richard and Iossifidis, Ioannis and Glasmachers, Tobias and Klaes, Christian},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Saif-Ur-Rehman et al. - Unknown - Universal Spike Classifier.pdf:pdf},
title = {{Universal Spike Classifier}}
}
@article{Shah2014,
abstract = {We investigate the Student-t process as an alternative to the Gaussian process as a nonparametric prior over functions. We derive closed form expressions for the marginal likelihood and predictive distribution of a Student-t process, by integrating away an inverse Wishart process prior over the covariance kernel of a Gaussian process model. We show surprising equivalences between different hierarchical Gaussian process models leading to Student-t processes, and derive a new sampling scheme for the inverse Wishart process, which helps elucidate these equivalences. Overall, we show that a Student-t process can retain the attractive properties of a Gaussian process -- a nonparametric representation, analytic marginal and predictive distributions, and easy model selection through covariance kernels -- but has enhanced flexibility, and predictive covariances that, unlike a Gaussian process, explicitly depend on the values of training observations. We verify empirically that a Student-t process is especially useful in situations where there are changes in covariance structure, or in applications like Bayesian optimization, where accurate predictive covariances are critical for good performance. These advantages come at no additional computational cost over Gaussian processes.},
archivePrefix = {arXiv},
arxivId = {1402.4306},
author = {Shah, Amar and Wilson, Andrew Gordon and Ghahramani, Zoubin},
eprint = {1402.4306},
issn = {15337928},
title = {{Student-t Processes as Alternatives to Gaussian Processes}},
year = {2014}
}
@article{Montanari2014,
abstract = {We consider the Principal Component Analysis problem for large tensors of arbitrary order {\$}k{\$} under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio {\$}\backslashbeta{\$} becomes larger than {\$}C\backslashsqrt{\{}k\backslashlog k{\}}{\$} (and in particular {\$}\backslashbeta{\$} can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.},
archivePrefix = {arXiv},
arxivId = {1411.1076},
author = {Montanari, Andrea and Richard, Emile},
eprint = {1411.1076},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Montanari, Richard - 2014 - A statistical model for tensor PCA.pdf:pdf},
isbn = {0780308417},
issn = {0780308417},
number = {1},
pages = {1--9},
title = {{A statistical model for tensor PCA}},
url = {http://arxiv.org/abs/1411.1076},
volume = {1},
year = {2014}
}
@inproceedings{Heinonen2018,
abstract = {In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model's capabilities to infer dynamics from sparse data and to simulate the system forward into future.},
archivePrefix = {arXiv},
arxivId = {1803.04303},
author = {Heinonen, Markus and Yildiz, {\c{C}}agatay and Mannerstr{\"{o}}m, Henrik and Intosalmi, Jukka and L{\"{a}}hdesm{\"{a}}ki, Harri},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1803.04303},
isbn = {9781510867963},
title = {{Learning unknown ODE models with Gaussian processes}},
volume = {5},
year = {2018}
}
@article{Orbanz2015,
abstract = {The natural habitat of most Bayesian methods is data represented by exchangeable sequences of observations, for which de Finetti's theorem provides the theoretical foundation. Dirichlet process clustering, Gaussian process regression, and many other parametric and nonparametric Bayesian models fall within the remit of this framework; many problems arising in modern data analysis do not. This article provides an introduction to Bayesian models of graphs, matrices, and other data that can be modeled by random structures. We describe results in probability theory that generalize de Finetti's theorem to such data and discuss their relevance to nonparametric Bayesian modeling. With the basic ideas in place, we survey example models available in the literature; applications of such models include collaborative filtering, link prediction, and graph and network analysis. We also highlight connections to recent developments in graph theory and probability, and sketch the more general mathematical foundation of Bayesian methods for other types of data beyond sequences and arrays.},
author = {Orbanz, Peter and Roy, Daniel M.},
doi = {10.1109/TPAMI.2014.2334607},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
title = {{Bayesian models of graphs, arrays and other exchangeable random structures}},
volume = {37},
year = {2015}
}
@article{Salimans2018,
abstract = {We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.},
archivePrefix = {arXiv},
arxivId = {1803.05573},
author = {Salimans, Tim and Metaxas, Dimitris and Zhang, Han and Radford, Alec},
eprint = {1803.05573},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salimans et al. - 2018 - Improving GANs using optimal transport.pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
pages = {1--13},
title = {{Improving GANs using optimal transport}},
year = {2018}
}
@article{Gelman2018,
abstract = {A standard mode of inference in social and behavioral science is to establish stylized facts using statistical significance in quantitative studies. However, in a world in which measurements are noisy and effects are small, this will not work: selection on statistical significance leads to effect sizes which are overestimated and often in the wrong direction. After a brief discussion of two examples, one in economics and one in social psychology, we consider the procedural solution of open postpublication review, the design solution of devoting more effort to accurate measurements and within-person comparisons, and the statistical analysis solution of multilevel modeling and reporting all results rather than selection on significance. We argue that the current replication crisis in science arises in part from the ill effects of null hypothesis significance testing being used to study small effects with noisy data. In such settings, apparent success comes easy but truly replicable results require a more se...},
author = {Gelman, Andrew},
doi = {10.1177/0146167217729162},
issn = {15527433},
journal = {Personality and Social Psychology Bulletin},
number = {1},
title = {{The Failure of Null Hypothesis Significance Testing When Studying Incremental Changes, and What to Do About It}},
volume = {44},
year = {2018}
}
@misc{Czerlinski1999,
abstract = {Multiattribute choice rules can be classified as being either alternative-based or attribute-based. Conventional accounts of intertemporal choice, hyperbolic and exponential discounting, assume alternative-based rules. One consequence of using these rules is that choices will be transitive, meaning that if a is preferred to b, and b is preferred to c, then a will be preferred to c. There have been many demonstrations of intransitivity in domains other than intertemporal choice, and in this paper we undertake to establish whether intransitive intertemporal choice can be explained by a stochastic specification of exponential discounting, or if we need to invoke an attribute-based choice process. In an experiment, we demonstrate that the pattern of intransitive responses is inconsistent with alternative-based choice. We argue that intransitive choices can best be explained by a version of Tversky's (1969) lexicographic-semiorder rule, in which choice is based on the amount of money when that amount exceeds a threshold, but on delay otherwise. Transitive choices, on the other hand, seem to be based on the rule that lsquoearlier is betterrsquo or else on a consistent rate of discount. Copyright {\textcopyright} 2000 John Wiley {\&} Sons, Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Czerlinski, Jean and Gigerenzer, Gerd and Goldstein, Daniel G.},
booktitle = {Simple Heuristics that make us smart},
doi = {10.1002/(SICI)1099-0771(200004/06)13:2<161::AID-BDM348>3.0.CO;2-P},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Czerlinski, Gigerenzer, Goldstein - 1999 - How good are simple heuristics.pdf:pdf},
isbn = {0195121562},
issn = {0195121562},
pages = {97--118},
pmid = {15003161},
title = {{How good are simple heuristics?}},
year = {1999}
}
@article{Dezfouli2015,
abstract = {We propose a sparse method for scalable automated variational inference (AVI) in a large class of models with Gaussian process (GP) priors, multiple latent func-tions, multiple outputs and non-linear likelihoods. Our approach maintains the statistical efficiency property of the original AVI method, requiring only expec-tations over univariate Gaussian distributions to approximate the posterior with a mixture of Gaussians. Experiments on small datasets for various problems includ-ing regression, classification, Log Gaussian Cox processes, and warped GPs show that our method can perform as well as the full method under high sparsity levels. On larger experiments using the MNIST and the SARCOS datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models.},
author = {Dezfouli, Amir and Bonilla, Edwin V},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 28},
title = {{Scalable Inference for Gaussian Process Models with Black-Box Likelihoods}},
year = {2015}
}
@article{Massey1996,
abstract = {Motivated by telecommunication applications, we investigate ways to estimate the parameters of a nonhomogeneous Poisson process with linear rate over a finite interval, based on the number of counts in measurement subintervals. Such a linear arrival-rate function can serve as a component of a piecewise-linear approximation to a general arrival-rate function. We consider ordinary least squares (OLS), iterative weighted least squares (IWLS) and maximum likelihood (ML), all constrained to yield a nonnegative rate function. We prove that ML coincides with IWLS. As a reference point, we also consider the theoretically optimal weighted least squares (TWLS), which is least squares with weights inversely proportional to the variances (which would not be known with data). Overall, ML performs almost as well as TWLS. We describe computer simulations conducted to evaluate these estimation procedures. None of the procedures differ greatly when the rate function is not near 0 at either end, but when the rate function is near 0 at one end, TWLS and ML are significantly more effective than OLS. The number of measurement subintervals (with fixed total interval) makes surprisingly little difference when the rate function is not near 0 at either end. The variances are higher with only two or three subintervals, but there usually is little benefit from going above ten. In contrast, more measurement intervals help TWLS and ML when the rate function is near 0 at one end. We derive explicit formulas for the OLS variances and the asymptotic TWLS variances (as the number of measurement intervals increases), assuming the nonnegativity constraints are not violated. These formulas reveal the statistical precision of the estimators and the influence of the parameters and the method. Knowing how the variance depends on the interval length can help determine how to approximate general arrival-rate functions by piecewise-linear ones. We also develop statistical tests to determine whether the linear Poisson model is appropriate. {\textcopyright} J.C. Baltzer AG, Science Publishers.},
author = {Massey, William A. and Parker, Geraldine A. and Whitt, Ward},
doi = {10.1007/bf02112523},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Massey, Parker, Whitt - 1996 - Estimating the parameters of a nonhomogeneous Poisson process with linear rate.pdf:pdf},
issn = {10184864},
journal = {Telecommunication Systems},
number = {4},
pages = {361--388},
title = {{Estimating the parameters of a nonhomogeneous Poisson process with linear rate}},
volume = {5},
year = {1996}
}
@article{Fraccaro2016,
abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
archivePrefix = {arXiv},
arxivId = {1605.07571},
author = {Fraccaro, Marco and S{\o}nderby, S{\o}ren Kaae and Paquet, Ulrich and Winther, Ole},
doi = {10.1163/156853991X00490},
eprint = {1605.07571},
isbn = {2078-5585},
issn = {2078-5585},
pmid = {191146},
title = {{Sequential Neural Models with Stochastic Layers}},
year = {2016}
}
@article{Sun2019,
abstract = {Large-scale and multidimensional spatiotemporal data sets are becoming ubiquitous in many real-world applications such as monitoring urban traffic and air quality. Making predictions on these time series has become a critical challenge due to not only the large-scale and high-dimensional nature but also the considerable amount of missing data. In this paper, we propose a Bayesian temporal factorization (BTF) framework for modeling multidimensional time series---in particular spatiotemporal data---in the presence of missing values. By integrating low-rank matrix/tensor factorization and vector autoregressive (VAR) process into a single probabilistic graphical model, this framework can characterize both global and local consistencies in large-scale time series data. The graphical model allows us to effectively perform probabilistic predictions and produce uncertainty estimates without imputing those missing values. We develop efficient Gibbs sampling algorithms for model inference and test the proposed BTF framework on several real-world spatiotemporal data sets for both missing data imputation and short-term/long-term rolling prediction tasks. The numerical experiments demonstrate the superiority of the proposed BTF approaches over many state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {1910.06366},
author = {Sun, Lijun and Chen, Xinyu},
eprint = {1910.06366},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sun, Chen - 2019 - Bayesian Temporal Factorization for Multidimensional Time Series Prediction.pdf:pdf},
pages = {1--13},
title = {{Bayesian Temporal Factorization for Multidimensional Time Series Prediction}},
url = {http://arxiv.org/abs/1910.06366},
year = {2019}
}
@article{JianhuaZhao2012,
author = {{Jianhua Zhao} and Yu, Philip L H and Kwok, James T},
doi = {10.1109/TNNLS.2012.2183006},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jianhua Zhao, Yu, Kwok - 2012 - Bilinear Probabilistic Principal Component Analysis.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {mar},
number = {3},
pages = {492--503},
title = {{Bilinear Probabilistic Principal Component Analysis}},
url = {http://ieeexplore.ieee.org/document/6138323/},
volume = {23},
year = {2012}
}
@article{Laird2012,
author = {Laird, John E.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Laird - 2012 - The Soar Cognitive Architecture.pdf:pdf},
isbn = {7035250050},
journal = {AISB Quarterly},
title = {{The Soar Cognitive Architecture}},
volume = {134},
year = {2012}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Ong2018,
abstract = {Variational approximation methods have proven to be useful for scaling Bayesian computations to large data sets and highly parametrized models. Applying variational methods involves solving an optimization problem, and recent research in this area has focused on stochastic gradient ascent methods as a general approach to implementation. Here variational approximation is considered for a posterior distribution in high dimensions using a Gaussian approximating family. Gaussian variational approximation with an unrestricted covariance matrix can be computationally burdensome in many problems because the number of elements in the covariance matrix increases quadratically with the dimension of the model parameter. To circumvent this problem, low-dimensional factor covariance structures are considered. General stochastic gradient approaches to efficiently perform the optimization are described, with gradient estimates obtained using the so-called "reparametrization trick". The end result is a flexible and efficient approach to high-dimensional Gaussian variational approximation, which we illustrate using eight real datasets.},
author = {Ong, Victor M.H. and Nott, David J. and Smith, Michael S.},
doi = {10.1080/10618600.2017.1390472},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
number = {3},
title = {{Gaussian Variational Approximation With a Factor Covariance Structure}},
volume = {27},
year = {2018}
}
@article{Khan2019,
abstract = {Machine-learning algorithms are commonly derived using ideas from optimization and statistics, followed by an extensive empirical efforts to make them practical as there is a lack of underlying principles to guide this process. In this paper, we present a learning rule derived from Bayesian principles, which enables us to connect a wide-variety of learning algorithms. Using this rule, we can derive a wide-range of learning-algorithms in fields such as probabilistic graphical models, continuous optimization, deep learning, reinforcement learning, online learning, and black-box optimization. This includes classical algorithms such as least-squares, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop and Adam. Overall, we show that Bayesian principles not only unify, generalize, and improve existing learning-algorithms, but also help us design new ones. [This is a working draft and a work in progress]},
author = {Khan, Mohammad Emtiyaz and Rue, Haavard},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Khan, Rue - 2019 - Learning-Algorithms from Bayesian Principles.pdf:pdf},
pages = {1--6},
title = {{Learning-Algorithms from Bayesian Principles}},
year = {2019}
}
@article{Acerbi2018,
abstract = {Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as MCMC, which would require access to the gradient or a large number of likelihood evaluations. We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo (VBMC). VBMC combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective. Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection. We demonstrate VBMC both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to {\$}D = 10{\$}), VBMC performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.},
archivePrefix = {arXiv},
arxivId = {1810.05558},
author = {Acerbi, Luigi},
eprint = {1810.05558},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Acerbi - 2018 - Variational Bayesian Monte Carlo.pdf:pdf},
number = {NeurIPS},
title = {{Variational Bayesian Monte Carlo}},
url = {http://arxiv.org/abs/1810.05558},
year = {2018}
}
@article{Veksler2015,
abstract = {A good fit of model predictions to empirical data are often used as an argument for model validity. However, if the model is flexible enough to fit a large proportion of potential empirical outcomes, finding a good fit becomes less meaningful. We propose a method for estimating the proportion of potential empirical outcomes that the model can fit: Model Flexibility Analysis (MFA). MFA aids model evaluation by providing a metric for gauging the persuasiveness of a given fit. We demonstrate that MFA can be more informative than merely discounting the fit by the number of free parameters in the model, and show how the number of free parameters does not necessarily correlate with the flexibility of the model. Additionally, we contrast MFA with other flexibility assessment techniques, including Parameter Space Partitioning, Model Mimicry, Minimum Description Length, and Prior Predictive Evaluation. Finally, we provide examples of how MFA can help to inform modeling results and discuss a variety of issues relating to the use of MFA in model validation.},
author = {Veksler, Vladislav D. and Myers, Christopher W. and Gluck, Kevin A.},
doi = {10.1037/a0039657},
isbn = {1939-1471 (Electronic) 0033-295X (Linking)},
issn = {0033295X},
journal = {Psychological Review},
number = {4},
pmid = {26322547},
title = {{Model flexibility analysis}},
volume = {122},
year = {2015}
}
@article{Ruiz2016,
abstract = {The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.},
archivePrefix = {arXiv},
arxivId = {1610.02287},
author = {Ruiz, Francisco J. R. and Titsias, Michalis K. and Blei, David M.},
eprint = {1610.02287},
issn = {10495258},
title = {{The Generalized Reparameterization Gradient}},
year = {2016}
}
@techreport{Garrido-Merchan,
abstract = {Bayesian Optimization (BO) is useful for optimizing functions that are expensive to evaluate, lack an analytical expression and whose evaluations can be contaminated by noise. These methods rely on a probabilistic model of the objective function, typically a Gaussian process (GP), upon which an acquisition function is built. The acquisition function guides the optimization process and measures the expected utility of performing an evaluation of the objective at a new point. GPs assume continuous input variables. When this is not the case, for example when some of the input variables take categorical or integer values, one has to introduce extra approximations. Consider a suggested input location taking values in the real line. Before doing the evaluation of the objective, a common approach is to use a one hot encoding approximation for categorical variables, or to round to the closest integer, in the case of integer-valued variables. We show that this can lead to optimization problems and describe a more principled approach to account for input variables that are categorical or integer-valued. We illustrate in both synthetic and a real experiments the utility of our approach, which significantly improves the results of standard BO methods using Gaussian processes on problems with categorical or integer-valued variables.},
archivePrefix = {arXiv},
arxivId = {1805.03463v2},
author = {Garrido-Merch{\'{a}}n, Eduardo C and Hern{\'{a}}ndez-Lobato, Daniel},
eprint = {1805.03463v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Garrido-Merch{\'{a}}n, Hern{\'{a}}ndez-Lobato - Unknown - Dealing with Categorical and Integer-valued Variables in Bayesian Optimization with Gaus.pdf:pdf},
title = {{Dealing with Categorical and Integer-valued Variables in Bayesian Optimization with Gaussian Processes}}
}
@article{Recht2018,
abstract = {This manuscript surveys reinforcement learning from the perspective of optimization and control with a focus on continuous control applications. It surveys the general formulation, terminology, and typical experimental implementations of reinforcement learning and reviews competing solution paradigms. In order to compare the relative merits of various techniques, this survey presents a case study of the Linear Quadratic Regulator (LQR) with unknown dynamics, perhaps the simplest and best studied problem in optimal control. The manuscript describes how merging techniques from learning theory and control can provide non-asymptotic characterizations of LQR performance and shows that these characterizations tend to match experimental behavior. In turn, when revisiting more complex applications, many of the observed phenomena in LQR persist. In particular, theory and experiment demonstrate the role and importance of models and the cost of generality in reinforcement learning algorithms. This survey concludes with a discussion of some of the challenges in designing learning systems that safely and reliably interact with complex and uncertain environments and how tools from reinforcement learning and controls might be combined to approach these challenges.},
archivePrefix = {arXiv},
arxivId = {1806.09460},
author = {Recht, Benjamin},
eprint = {1806.09460},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Recht - 2018 - A Tour of Reinforcement Learning The View from Continuous Control.pdf:pdf},
pages = {1--28},
title = {{A Tour of Reinforcement Learning: The View from Continuous Control}},
url = {http://arxiv.org/abs/1806.09460},
year = {2018}
}
@article{Rogers2013,
abstract = {Data in the sciences frequently occur as sequences of multidimensional arrays called tensors. How can hidden, evolving trends in such data be extracted while preserving the tensor structure? The model that is traditionally used is the linear dynamical system (LDS) with Gaussian noise, which treats the latent state and observation at each time slice as a vector. We present the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation-maximization (EM) algorithm to estimate the parameters. The MLDS models each tensor observation in the time series as the multilinear projection of the corresponding member of a sequence of latent tensors. The latent tensors are again evolving with respect to a multilinear projection. Compared to the LDS with an equal number of parameters, the MLDS achieves higher prediction accuracy and marginal likelihood for both artificial and real datasets.},
author = {Rogers, Mark and Li, Lei and Russell, Stuart},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rogers, Li, Russell - 2013 - Multilinear dynamical systems for tensor time series.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Multilinear dynamical systems for tensor time series}},
year = {2013}
}
@article{Auer2002,
abstract = {In the multiarmed bandit problem, a gambler must decide which arm of K non- identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the per-round payoff of our algorithm approaches that of the best arm at the rate O(T−1/2). We show by a matching lowerbound that this is the best possible. We also prove that our algorithm approaches the per-round payoff of any set of strategies at a similar rate: if the best strategy is chosen from a pool of N strategies, then our algorithm approaches the per-round payoff of the strategy at the rate O((logN)1/2T−1/2). Finally, we apply ourresults to the problem of playing an unknown repeated matrix game. We show that our algorithm approaches the minimax payoff of the unknown game at the rate O(T−1/2).},
author = {Auer, Peter and Cesa-Bianchi, Nicolo and Freund, Yoav and Schapire, Robert E.},
doi = {10.1137/S0097539701398375},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Auer et al. - 2002 - The nonstochastic multiarmed bandit problem.pdf:pdf},
isbn = {0097-5397},
issn = {00975397},
journal = {SIAM Journal on Computing},
pages = {48--77},
title = {{The nonstochastic multiarmed bandit problem}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.8735{\&}rep=rep1{\&}type=pdf},
volume = {32},
year = {2002}
}
@article{Snoek2015a,
abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
archivePrefix = {arXiv},
arxivId = {1502.05700},
author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Prabhat and Adams, Ryan P.},
doi = {10.1111/acer.12162},
eprint = {1502.05700},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural Networks.pdf:pdf},
isbn = {9781510810587},
issn = {1938-7228},
title = {{Scalable Bayesian Optimization Using Deep Neural Networks}},
url = {http://arxiv.org/abs/1502.05700},
year = {2015}
}
@article{Liu2018a,
abstract = {Stein variational gradient descent (SVGD) is a non-parametric inference algorithm that evolves a set of particles to fit a given distribution of interest. We analyze the non-asymptotic properties of SVGD, showing that there exists a set of functions, which we call the Stein matching set, whose expectations are exactly estimated by any set of particles that satisfies the fixed point equation of SVGD. This set is the image of Stein operator applied on the feature maps of the positive definite kernel used in SVGD. Our results provide a theoretical framework for analyzing the properties of SVGD with different kernels, shedding insight into optimal kernel choice. In particular, we show that SVGD with linear kernels yields exact estimation of means and variances on Gaussian distributions, while random Fourier features enable probabilistic bounds for distributional approximation. Our results offer a refreshing view of the classical inference problem as fitting Stein's identity or solving the Stein equation, which may motivate more efficient algorithms.},
archivePrefix = {arXiv},
arxivId = {1810.11693},
author = {Liu, Qiang and Wang, Dilin},
eprint = {1810.11693},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Wang - 2018 - Stein Variational Gradient Descent as Moment Matching.pdf:pdf},
number = {Nips},
title = {{Stein Variational Gradient Descent as Moment Matching}},
url = {http://arxiv.org/abs/1810.11693},
year = {2018}
}
@article{Latimer2016,
abstract = {Latimeret al (Reports, 10 July 2015, p. 184) claim that during perceptual decision formation, parietal neurons undergo one-time, discrete steps in firing rate instead of gradual changes that represent the accumulation of evidence. However, that conclusion rests on unsubstantiated assumptions about the time window of evidence accumulation, and their stepping model cannot explain existing data as effectively as evidence-accumulation models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.7245v1},
author = {Latimer, Kenneth W. and Yates, Jacob L. and Meister, Miriam L.R. and Huk, Alexander C. and Pillow, Jonathan W.},
doi = {10.1126/science.aad3596},
eprint = {arXiv:1402.7245v1},
isbn = {10.1126/science.aaa4056},
issn = {10959203},
journal = {Science},
number = {6280},
pmid = {27013723},
title = {{Response to Comment on "single-trial spike trains in parietal cortex reveal discrete steps during decision-making"}},
volume = {351},
year = {2016}
}
@inproceedings{Alexander2016,
abstract = {The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes.},
archivePrefix = {arXiv},
arxivId = {1504.07027},
author = {Alexander, Alexander G. and Hensman, James and Turner, Richard E. and Ghahramani, Zoubin},
booktitle = {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016},
doi = {10.17863/CAM.15597},
eprint = {1504.07027},
issn = {1533-7928},
title = {{On sparse variational methods and the Kullback-Leibler divergence between stochastic processes}},
year = {2016}
}
@article{Hadjeres2018,
abstract = {VAEs (Variational AutoEncoders) have proved to be powerful in the context of density modeling and have been used in a variety of contexts for creative purposes. In many settings, the data we model possesses continuous attributes that we would like to take into account at generation time. We propose in this paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational AutoEncoder architecture and its generalizations which allows a fine control on the embedding of the data into the latent space. When augmenting the VAE loss with this regularization, changes in the learned latent space reflects changes of the attributes of the data. This deeper understanding of the VAE latent space structure offers the possibility to modulate the attributes of the generated data in a continuous way. We demonstrate its efficiency on a monophonic music generation task where we manage to generate variations of discrete sequences in an intended and playful way.},
archivePrefix = {arXiv},
arxivId = {1707.04588},
author = {Hadjeres, G{\"{a}}etan and Nielsen, Frank and Pachet, Fran{\c{c}}ois},
doi = {10.1109/SSCI.2017.8280895},
eprint = {1707.04588},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hadjeres, Nielsen, Pachet - 2018 - GLSR-VAE Geodesic latent space regularization for variational autoencoder architectures.pdf:pdf},
isbn = {9781538627259},
journal = {2017 IEEE Symposium Series on Computational Intelligence, SSCI 2017 - Proceedings},
pages = {1--7},
title = {{GLSR-VAE: Geodesic latent space regularization for variational autoencoder architectures}},
volume = {2018-Janua},
year = {2018}
}
@article{Calandra2016,
abstract = {New technologies for innovative interactive experience represent a powerful medium to deliver cultural heritage content to a wider range of users. Among them, Natural User Interfaces (NUI), i.e. non-intrusive technologies not requiring to the user to wear devices nor use external hardware (e.g. keys or trackballs), are considered a promising way to broader the audience of specific cultural heritage domains, like the navigation/interaction with digital artworks presented on wall-sized displays. Starting from a collaboration with a worldwide famous Italian designer, we defined a NUI to explore 360 panoramic artworks presented on wall-sized displays, like virtual reconstruction of ancient cultural sites, or rendering of imaginary places. Specifically, we let the user to "move the head" as way of natural interaction to explore and navigate through these large digital artworks. To this aim, we developed a system including a remote head pose estimator to catch movements of users standing in front of the wall-sized display: starting from a central comfort zone, as users move their head in any direction, the virtual camera rotates accordingly. With NUIs, it is difficult to get feedbacks from the users about the interest for the point of the artwork he/she is looking at. To solve this issue, we complemented the gaze estimator with a preliminary emotional analysis solution, able to implicitly infer the interest of the user for the shown content from his/her pupil size. A sample of 150 subjects was invited to experience the proposed interface at an International Design Week. Preliminary results show that the most of the subjects were able to properly interact with the system from the very first use, and that the emotional module is an interesting solution, even if further work must be devoted to address specific situations. {\textcopyright} 2016 for this paper by its authors. Copying permitted for private and academic purposes.},
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0005074v1},
author = {Calandra, Davide Maria and Mauro, Dario Di and Cutugno, Francesco and Martino, Sergio Di},
doi = {10.1023/A},
eprint = {0005074v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Calandra et al. - 2016 - Navigating wall-sized displays with the gaze A proposal for cultural heritage.pdf:pdf},
isbn = {1009982220290},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {October},
pages = {36--43},
pmid = {1284},
primaryClass = {arXiv:astro-ph},
title = {{Navigating wall-sized displays with the gaze: A proposal for cultural heritage}},
volume = {1621},
year = {2016}
}
@article{Parpart2018,
abstract = {Simple heuristics are often regarded as tractable decision strategies because they ignore a great deal of information in the input data. One puzzle is why heuristics can outperform full-information models, such as linear regression, which make full use of the available information. These “less-is-more” effects, in which a relatively simpler model outperforms a more complex model, are prevalent throughout cognitive science, and are frequently argued to demonstrate an inherent advantage of simplifying computation or ignoring information. In contrast, we show at the computational level (where algorithmic restrictions are set aside) that it is never optimal to discard information. Through a formal Bayesian analysis, we prove that popular heuristics, such as tallying and take-the-best, are formally equivalent to Bayesian inference under the limit of infinitely strong priors. Varying the strength of the prior yields a continuum of Bayesian models with the heuristics at one end and ordinary regression at the other. Critically, intermediate models perform better across all our simulations, suggesting that down-weighting information with the appropriate prior is preferable to entirely ignoring it. Rather than because of their simplicity, our analyses suggest heuristics perform well because they implement strong priors that approximate the actual structure of the environment. We end by considering how new heuristics could be derived by infinitely strengthening the priors of other Bayesian models. These formal results have implications for work in psychology, machine learning and economics.},
author = {Parpart, Paula and Jones, Matt and Love, Bradley C.},
doi = {10.1016/j.cogpsych.2017.11.006},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Parpart, Jones, Love - 2018 - Heuristics as Bayesian inference under extreme priors.pdf:pdf},
issn = {00100285},
journal = {Cognitive Psychology},
keywords = {Bayesian inference,Decision making,Heuristics,Ridge regression},
number = {March},
pages = {127--144},
publisher = {Elsevier},
title = {{Heuristics as Bayesian inference under extreme priors}},
url = {https://doi.org/10.1016/j.cogpsych.2017.11.006},
volume = {102},
year = {2018}
}
@article{Vexler2018,
abstract = {A common statistical doctrine supported by many introductory courses and textbooks is that t-test type procedures based on normally distributed data points are anticipated to provide a standard in decision-making. In order to motivate scholars to examine this convention, we introduce a simple approach based on graphical tools of receiver operating characteristic (ROC) curve analysis, a well-established biostatistical methodology. In this context, we propose employing a p-values-based method, taking into account the stochastic nature of p-values. We focus on the modern statistical literature to address the expected p-value (EPV) as a measure of the performance of decision-making rules. During the course of our study, we extend the EPV concept to be considered in terms of the ROC curve technique. This provides expressive evaluations and visualizations of a wide spectrum of testing mechanisms' properties. We show that the conventional power characterization of tests is a partial aspect of the presented EPV/ROC technique. We desire that this explanation of the EPV/ROC approach convinces researchers of the usefulness of the EPV/ROC approach for depicting different characteristics of decision-making procedures, in light of the growing interest regarding correct p-values-based applications.},
author = {Vexler, Albert and Yu, Jihnhee},
doi = {10.1089/cmb.2017.0216},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Vexler, Yu - 2018 - To t-Test or Not to t-Test A p-Values-Based Point of View in the Receiver Operating Characteristic Curve Framework.pdf:pdf},
issn = {10665277},
journal = {Journal of Computational Biology},
keywords = {AUC,ROC curve,Wilcoxon test,expected p-value,p-value,partial AUC,partial expected p-value,power,t-test},
number = {6},
pages = {541--550},
title = {{To t-Test or Not to t-Test? A p-Values-Based Point of View in the Receiver Operating Characteristic Curve Framework}},
volume = {25},
year = {2018}
}
@misc{Westbrook2015,
abstract = {Cognitive effort has been implicated in numerous theories regarding normal and aberrant behavior and the physiological response to engagement with demanding tasks. Yet, despite broad interest, no unifying, operational definition of cognitive effort itself has been proposed. Here, we argue that the most intuitive and epistemologically valuable treatment is in terms of effort-based decision-making, and advocate a neuroeconomics-focused research strategy. We first outline psychological and neuroscientific theories of cognitive effort. Then we describe the benefits of a neuroeconomic research strategy, highlighting how it affords greater inferential traction than do traditional markers of cognitive effort, including self-reports and physiologic markers of autonomic arousal. Finally, we sketch a future series of studies that can leverage the full potential of the neuroeconomic approach toward understanding the cognitive and neural mechanisms that give rise to phenomenal, subjective cognitive effort.},
author = {Westbrook, Andrew and Braver, Todd S.},
booktitle = {Cognitive, Affective and Behavioral Neuroscience},
doi = {10.3758/s13415-015-0334-y},
isbn = {1531-135X (Electronic)$\backslash$r1530-7026 (Linking)},
issn = {15307026},
number = {2},
pmid = {25673005},
title = {{Cognitive effort: A neuroeconomic approach}},
volume = {15},
year = {2015}
}
@article{Kucukelbir2015,
abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
archivePrefix = {arXiv},
arxivId = {1506.03431},
author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
eprint = {1506.03431},
issn = {10495258},
title = {{Automatic Variational Inference in Stan}},
year = {2015}
}
@article{Tucker,
archivePrefix = {arXiv},
arxivId = {arXiv:1909.12316v1},
author = {Tucker, Maegan and Novoseller, Ellen and Kann, Claudia and Sep, R O},
eprint = {arXiv:1909.12316v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tucker et al. - Unknown - Preference-Based Learning for Exoskeleton Gait Optimization.pdf:pdf},
title = {{Preference-Based Learning for Exoskeleton Gait Optimization}}
}
@article{Aytar2018,
abstract = {Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.},
archivePrefix = {arXiv},
arxivId = {1805.11592},
author = {Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and de Freitas, Nando},
eprint = {1805.11592},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Aytar et al. - 2018 - Playing hard exploration games by watching YouTube.pdf:pdf},
title = {{Playing hard exploration games by watching YouTube}},
url = {http://arxiv.org/abs/1805.11592},
year = {2018}
}
@article{Salimbeni2017,
abstract = {Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to over-fitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalisations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm, which does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.},
archivePrefix = {arXiv},
arxivId = {1705.08933},
author = {Salimbeni, Hugh and Deisenroth, Marc},
eprint = {1705.08933},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salimbeni, Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep Gaussian Processes.pdf:pdf},
issn = {10495258},
title = {{Doubly Stochastic Variational Inference for Deep Gaussian Processes}},
year = {2017}
}
@article{Sinha2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.05370v1},
author = {Sinha, Subhrajit and Vaidya, Umesh},
eprint = {arXiv:1704.05370v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sinha, Vaidya - 2018 - Causality Preserving Information Transfer Measure for Control Dynamical System.pdf:pdf},
pages = {1--18},
title = {{Causality Preserving Information Transfer Measure for Control Dynamical System}},
year = {2018}
}
@article{Gupta2018,
abstract = {Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Although it involves a more complex update rule, Shampoo's runtime per step is comparable to that of simple gradient methods such as SGD, AdaGrad, and Adam.},
archivePrefix = {arXiv},
arxivId = {1802.09568},
author = {Gupta, Vineet and Koren, Tomer and Singer, Yoram},
eprint = {1802.09568},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gupta, Koren, Singer - 2018 - Shampoo Preconditioned Stochastic Tensor Optimization.pdf:pdf},
pages = {1--21},
title = {{Shampoo: Preconditioned Stochastic Tensor Optimization}},
url = {http://arxiv.org/abs/1802.09568},
year = {2018}
}
@article{Zhou2012,
abstract = {Temporal alignment of human motion has been a topic of recent interest due to its applications in animation, telerehabilitation and activity recognition among others. This paper presents generalized time warping (GTW), an extension of dynamic time warping (DTW) for temporally aligning multi-modal sequences from multiple subjects performing similar activities. GTW solves three major drawbacks of existing approaches based on DTW: (1) GTW provides a feature weighting layer to adapt different modalities (e.g., video and motion capture data), (2) GTW extends DTW by allowing a more flexible time warping as combination of monotonic functions, (3) unlike DTW that typically incurs in quadratic cost, GTW has linear complexity. Experimental results demonstrate that GTW can efficiently solve the multi-modal temporal alignment problem and outperforms state-of-the-art DTW methods for temporal alignment of time series within the same modality. View full abstract},
author = {Zhou, Feng and {De La Torre}, Fernando},
doi = {10.1109/CVPR.2012.6247812},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, De La Torre - 2012 - Generalized time warping for multi-modal alignment of human motion.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1282--1289},
title = {{Generalized time warping for multi-modal alignment of human motion}},
year = {2012}
}
@article{Csato2000,
abstract = {We present three simple approximations for the calculation of the posterior mean in Gaussian Process classification. The first two methods are related to mean field ideas known in Statistical Physics. The third approach is based on Bayesian online approach which was motivated by recent results in the Statistical Mechanics of Neural Networks. We present simulation results showing: 1. that the mean field Bayesian evidence may be used for hyperparameter tuning and 2. that the online approach may achieve a low training error fast.},
author = {Csat{\'{o}}, Lehel and Fokoue, Ernest and Opper, Manfred and Schottky, Bernhard and Winther, Ole},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Csat{\'{o}} et al. - 2000 - Efficient approaches to Gaussian process classification.pdf:pdf},
isbn = {0262194503},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 12},
keywords = {mathematical computing sciences not elsewhere clas},
number = {x},
pages = {251--257},
title = {{Efficient approaches to Gaussian process classification}},
url = {http://mitpress.mit.edu/catalog/item/default.asp?ttype=2{\&}tid=3554},
volume = {12},
year = {2000}
}
@article{Jorgensen2015,
abstract = {We study reproducing kernels, and associated reproducing kernel Hilbert spaces (RKHSs) {\$}\backslashmathscr{\{}H{\}}{\$} over infinite, discrete and countable sets {\$}V{\$}. In this setting we analyze in detail the distributions of the corresponding Dirac point-masses of {\$}V{\$}. Illustrations include certain models from neural networks: An Extreme Learning Machine (ELM) is a neural network-configuration in which a hidden layer of weights are randomly sampled, and where the object is then to compute resulting output. For RKHSs {\$}\backslashmathscr{\{}H{\}}{\$} of functions defined on a prescribed countable infinite discrete set {\$}V{\$}, we characterize those which contain the Dirac masses {\$}\backslashdelta{\_}{\{}x{\}}{\$} for all points {\$}x{\$} in {\$}V{\$}. Further examples and applications where this question plays an important role are: (i) discrete Brownian motion-Hilbert spaces, i.e., discrete versions of the Cameron-Martin Hilbert space; (ii) energy-Hilbert spaces corresponding to graph-Laplacians where the set {\$}V{\$} of vertices is then equipped with a resistance metric; and finally (iii) the study of Gaussian free fields.},
archivePrefix = {arXiv},
arxivId = {1501.02310},
author = {Jorgensen, Palle and Tian, Feng},
eprint = {1501.02310},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jorgensen, Tian - 2015 - Discrete reproducing kernel Hilbert spaces Sampling and distribution of Dirac-masses.pdf:pdf},
pages = {1--38},
title = {{Discrete reproducing kernel Hilbert spaces: Sampling and distribution of Dirac-masses}},
url = {http://arxiv.org/abs/1501.02310},
year = {2015}
}
@article{Osa2018,
archivePrefix = {arXiv},
arxivId = {1811.06711},
author = {Osa, Takayuki and Pajarinen, Joni and Neumann, Gerhard and Bagnell, J. Andrew and Abbeel, Pieter and Peters, Jan},
doi = {10.1561/2300000053},
eprint = {1811.06711},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Osa et al. - 2018 - An Algorithmic Perspective on Imitation Learning.pdf:pdf},
isbn = {9781439881576},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
keywords = {Actions,Algorithms,Analytic philosophy,Clues,Computer,Denotation,Language},
number = {1-2},
pages = {1--179},
title = {{An Algorithmic Perspective on Imitation Learning}},
url = {http://www.nowpublishers.com/article/Details/ROB-053},
volume = {7},
year = {2018}
}
@article{Figurnov2018a,
abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
archivePrefix = {arXiv},
arxivId = {1805.08498},
author = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
eprint = {1805.08498},
month = {may},
title = {{Implicit Reparameterization Gradients}},
url = {http://arxiv.org/abs/1805.08498},
year = {2018}
}
@article{Varoquaux2010a,
author = {Varoquaux, G and Sadaghiani, S and Pinel, P and Kleinschmidt, A and Poline, J B and Thirion, B},
doi = {10.1016/j.neuroimage.2010.02.010},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Varoquaux et al. - 2010 - NeuroImage A group model for stable multi-subject ICA on fMRI datasets.pdf:pdf},
issn = {1053-8119},
journal = {NeuroImage},
number = {1},
pages = {288--299},
publisher = {Elsevier Inc.},
title = {{NeuroImage A group model for stable multi-subject ICA on fMRI datasets}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2010.02.010},
volume = {51},
year = {2010}
}
@article{Vamplew2011,
abstract = {While a number of algorithms for multiobjective reinforcement learning have been proposed, and a small number of applications developed, there has been very little rigorous empirical evaluation of the performance and limitations of these algorithms. This paper proposes standard methods for such empirical evaluation, to act as a foundation for future comparative studies. Two classes of multiobjective reinforcement learning algorithms are identified, and appropriate evaluation metrics and methodologies are proposed for each class. A suite of benchmark problems with known Pareto fronts is described, and future extensions and implementations of this benchmark suite are discussed. The utility of the proposed evaluation methods are demonstrated via an empirical comparison of two example learning algorithms.},
author = {Vamplew, Peter and Dazeley, Richard and Berry, Adam and Issabekov, Rustam and Dekker, Evan},
doi = {10.1007/s10994-010-5232-5},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Vamplew et al. - 2011 - Empirical evaluation methods for multiobjective reinforcement learning algorithms.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Empirical methods,Multiobjective reinforcement learning,Multiple objectives,Pareto fronts,Pareto optimal policies},
number = {1-2},
pages = {51--80},
title = {{Empirical evaluation methods for multiobjective reinforcement learning algorithms}},
volume = {84},
year = {2011}
}
@article{Gorham2017,
abstract = {Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution. This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points. We develop a theory of weak convergence for KSDs based on Stein's method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.},
archivePrefix = {arXiv},
arxivId = {1703.01717},
author = {Gorham, Jackson and Mackey, Lester},
eprint = {1703.01717},
title = {{Measuring Sample Quality with Kernels}},
year = {2017}
}
@article{Yu2016a,
abstract = {We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from {\$}\backslashmathcal{\{}O{\}}(d{\^{}}2){\$} to {\$}\backslashmathcal{\{}O{\}}(d \backslashlog d){\$}, where {\$}d{\$} is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORF. Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.},
archivePrefix = {arXiv},
arxivId = {1610.09072},
author = {Yu, Felix X. and Suresh, Ananda Theertha and Choromanski, Krzysztof and Holtmann-Rice, Daniel and Kumar, Sanjiv},
eprint = {1610.09072},
issn = {10495258},
pmid = {17180377133609788624},
title = {{Orthogonal Random Features}},
year = {2016}
}
@article{Welvaert2014,
abstract = {Simulation studies that validate statistical techniques for fMRI data are challenging due to the complexity of the data. Therefore, it is not surprising that no common data generating process is available (i.e. several models can be found to model BOLD activation and noise). Based on a literature search, a database of simulation studies was compiled. The information in this database was analysed and critically evaluated focusing on the parameters in the simulation design, the adopted model to generate fMRI data, and on how the simulation studies are reported. Our literature analysis demonstrates that many fMRI simulation studies do not report a thorough experimental design and almost consistently ignore crucial knowledge on how fMRI data are acquired. Advice is provided on how the quality of fMRI simulation studies can be improved. {\textcopyright} 2014 Welvaert, Rosseel.},
author = {Welvaert, Marijke and Rosseel, Yves},
doi = {10.1371/journal.pone.0101953},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Welvaert, Rosseel - 2014 - A review of fMRI simulation studies.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
title = {{A review of fMRI simulation studies}},
volume = {9},
year = {2014}
}
@article{Rezende2015,
abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
archivePrefix = {arXiv},
arxivId = {1505.05770},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
eprint = {1505.05770},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rezende, Mohamed - 2015 - Variational Inference with Normalizing Flows.pdf:pdf},
isbn = {1505.05770},
issn = {1938-7228},
title = {{Variational Inference with Normalizing Flows}},
url = {http://arxiv.org/abs/1505.05770},
volume = {37},
year = {2015}
}
@article{Kusner2017,
abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.},
archivePrefix = {arXiv},
arxivId = {1703.01925},
author = {Kusner, Matt J. and Paige, Brooks and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel},
doi = {10.1016/j.egypro.2014.11.918},
eprint = {1703.01925},
isbn = {9781510855144},
issn = {1938-7228},
pmid = {17759226},
title = {{Grammar Variational Autoencoder}},
year = {2017}
}
@article{Flaxman2015,
abstract = {Gaussian processes (GPs) are a flexible class of methods with state of the art performance on spatial statistics applications. However, GPs re-quire O(n 3) computations and O(n 2) storage, and popular GP kernels are typically limited to smoothing and interpolation. To address these difficulties, Kronecker methods have been used to exploit structure in the GP covariance ma-trix for scalability, while allowing for expres-sive kernel learning (Wilson et al., 2014). How-ever, fast Kronecker methods have been confined to Gaussian likelihoods. We propose new scal-able Kronecker methods for Gaussian processes with non-Gaussian likelihoods, using a Laplace approximation which involves linear conjugate gradients for inference, and a lower bound on the GP marginal likelihood for kernel learning. Our approach has near linear scaling, requir-ing O(Dn D+1 D) operations and O(Dn 2 D) stor-age, for n training data-points on a dense D {\textgreater} 1 dimensional grid. Moreover, we introduce a log Gaussian Cox process, with highly expres-sive kernels, for modelling spatiotemporal count processes, and apply it to a point pattern (n = 233,088) of a decade of crime events in Chicago. Using our model, we discover spatially varying multiscale seasonal trends and produce highly accurate long-range local area forecasts.},
archivePrefix = {arXiv},
arxivId = {1705.07674},
author = {Flaxman, Seth R and Wilson, Andrew Gordon and Neill, Daniel B and Org, Alex Smola and Nickisch, Hannes and Smola, Alexander J},
eprint = {1705.07674},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Flaxman et al. - 2015 - Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods.pdf:pdf},
isbn = {9781510810587},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
number = {1978},
pages = {607--616},
title = {{Fast Kronecker Inference in Gaussian Processes with non-Gaussian Likelihoods}},
volume = {37},
year = {2015}
}
@article{Zhang2017e,
abstract = {{\textcopyright} 2017 Wiley Periodicals, Inc. Pooling neural imaging data across subjects requires aligning recordings from different subjects. In magnetoencephalography (MEG) recordings, sensors across subjects are poorly correlated both because of differences in the exact location of the sensors, and structural and functional differences in the brains. It is possible to achieve alignment by assuming that the same regions of different brains correspond across subjects. However, this relies on both the assumption that brain anatomy and function are well correlated, and the strong assumptions that go into solving the under-determined inverse problem given the high-dimensional source space. In this article, we investigated an alternative method that bypasses source-localization. Instead, it analyzes the sensor recordings themselves and aligns their temporal signatures across subjects. We used a multivariate approach, multiset canonical correlation analysis (M-CCA), to transform individual subject data to a low-dimensional common representational space. We evaluated the robustness of this approach over a synthetic dataset, by examining the effect of different factors that add to the noise and individual differences in the data. On an MEG dataset, we demonstrated that M-CCA performs better than a method that assumes perfect sensor correspondence and a method that applies source localization. Last, we described how the standard M-CCA algorithm could be further improved with a regularization term that incorporates spatial sensor information. Hum Brain Mapp 38:4287–4301, 2017. {\textcopyright} 2017 Wiley Periodicals, Inc.},
author = {Zhang, Qiong and Borst, Jelmer P. and Kass, Robert E. and Anderson, John R.},
doi = {10.1002/hbm.23689},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Inter-subject alignment of MEG datasets in a common representational space.pdf:pdf},
issn = {10970193},
journal = {Human Brain Mapping},
keywords = {canonical correlation analysis,common representational space,magnetoencephalography,subject alignment},
number = {9},
pages = {4287--4301},
title = {{Inter-subject alignment of MEG datasets in a common representational space}},
volume = {38},
year = {2017}
}
@article{Dwork2013,
abstract = {The Algorithmic Foundations of Differential Privacy},
author = {Dwork, Cynthia and Roth, Aaron},
doi = {10.1561/0400000042},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Dwork, Roth - 2013 - The Algorithmic Foundations of Differential Privacy.pdf:pdf},
isbn = {9781601988188},
issn = {1551-305X},
journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
number = {3-4},
pages = {211--407},
pmid = {21455981},
title = {{The Algorithmic Foundations of Differential Privacy}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-theoretical-computer-science/TCS-042},
volume = {9},
year = {2013}
}
@inproceedings{Lieder2016,
abstract = {Game elements like points and levels are a popular tool to nudge and engage students and customers. Yet, no theory can tell us which incentive structures work and how to design them. Here we connect the practice of gamification to the theory of reward shaping in reinforcement learning. We leverage this connection to develop a method for designing effective incen-tive structures and delineating when gamification will succeed from when it will fail. We evaluate our method in two behav-ioral experiments. The results of the first experiment demon-strate that incentive structures designed by our method help people make better, less short-sighted decisions and avoid the pitfalls of less principled approaches. The results of the sec-ond experiment illustrate that such incentive structures can be effectively implemented using game elements like points and badges. These results suggest that our method provides a prin-cipled way to leverage gamification to help people make better decisions.},
author = {Lieder, Falk and Griffiths, Thomas L.},
booktitle = {Proceedings of the 38th Annual Meeting of the Cognitive Science Society},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lieder, Griffiths - 2016 - Helping people make better decisions using optimal gamification.pdf:pdf},
keywords = {2014,2015,2016,and improve themselves,bounded ratio-,decision-making,decision-support,gamification,henry,kamb,mcgonigal,nality,ple achieve their goals,reinforcement learning},
number = {August},
title = {{Helping people make better decisions using optimal gamification}},
year = {2016}
}
@article{Kumar2018,
abstract = {Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative to standard parametric deep learning models. A DGP is formed by stacking multiple GPs resulting in a well-regularized composition of functions. The Bayesian framework that equips the model with attractive properties, such as implicit capacity control and predictive uncertainty, makes it at the same time challenging to combine with a convolutional structure. This has hindered the application of DGPs in computer vision tasks, an area where deep parametric models (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such as radial basis functions (RBFs) are insufficient for handling pixel variability in raw images. In this paper, we build on the recent convolutional GP to develop Convolutional DGP (CDGP) models which effectively capture image level features through the use of convolution kernels, therefore opening up the way for applying DGPs to computer vision tasks. Our model learns local spatial influence and outperforms strong GP based baselines on multi-class image classification. We also consider various constructions of convolution kernel over the image patches, analyze the computational trade-offs and provide an efficient framework for convolutional DGP models. The experimental results on image data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate the effectiveness of the proposed approaches.},
archivePrefix = {arXiv},
arxivId = {1806.01655},
author = {Kumar, Vinayak and Singh, Vaibhav and Srijith, P. K. and Damianou, Andreas},
eprint = {1806.01655},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - 2018 - Deep Gaussian Processes with Convolutional Kernels.pdf:pdf},
keywords = {bayesian deep learning,convolutional neu-,gaussian processes,ral network,variational inference},
title = {{Deep Gaussian Processes with Convolutional Kernels}},
url = {http://arxiv.org/abs/1806.01655},
year = {2018}
}
@inproceedings{Khan2018,
abstract = {Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximumlikelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.},
archivePrefix = {arXiv},
arxivId = {1806.04854},
author = {Khan, Mohammad Emtiyaz and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1806.04854},
isbn = {9781510867963},
title = {{Fast and scalable Bayesian deep learning by weight-perturbation in Adam}},
volume = {6},
year = {2018}
}
@article{Lewis2009,
abstract = {Reinforcement learning (RL) has achieved broad and successful application in cognitive science in part because of its gen- eral formulation of the adaptive control problem as the maximiza- tion of a scalar reward function. The computational RL framework is motivated by correspondences to animal reward processes, but it leaves the source and nature of the rewards unspecified. This paper advances a general computational framework for reward that places it in an evolutionary context, formulating a notion of an optimal re- ward function given a fitness function and some distribution of envi- ronments. Novel results from computational experiments show how traditional notions of extrinsically and intrinsically motivated behav- iors may emerge from such optimal reward functions. In the experi- ments these rewards are discovered through automated search rather than crafted by hand. The precise form of the optimal reward func- tions need not bear a direct relationship to the fitness function, but may nonetheless confer significant advantages over rewards based only on fitness.},
author = {Lewis, Richard L and Singh, Satinder and Barto, Andrew G},
doi = {10.1.1.151.8250},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lewis, Singh, Barto - 2009 - Where Do Rewards Come From.pdf:pdf},
isbn = {1902956923},
journal = {Proceedings of the Annual Conference of the Cognitive Science Society},
pages = {2601--2606},
title = {{Where Do Rewards Come From?}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.151.8250{\&}rep=rep1{\&}type=pdf},
year = {2009}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
isbn = {9783319245737},
issn = {16113349},
pages = {1--8},
pmid = {23285570},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{Chen2015a,
abstract = {We present a multi-instance object segmentation algo- rithm to tackle occlusions. As an object is split into two parts by an occluder, it is nearly impossible to group the two separate regions into an instance by purely bottom- up schemes. To address this problem, we propose to in- corporate top-down category specific reasoning and shape prediction through exemplars into an intuitive energy min- imization framework. We perform extensive evaluations of our method on the challenging PASCAL VOC 2012 segmen- tation set. The proposed algorithm achieves favorable re- sults on the joint detection and segmentation task against the state-of-the-art method both quantitatively and qualita- tively. 1.},
author = {Chen, Yi Ting and Liu, Xiaokai and Yang, Ming Hsuan},
doi = {10.1109/CVPR.2015.7298969},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Liu, Yang - 2015 - Multi-instance object segmentation with occlusion handling.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3470--3478},
title = {{Multi-instance object segmentation with occlusion handling}},
volume = {07-12-June},
year = {2015}
}
@article{Sutton1999,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, Richard S. and Barto, Andrew G.},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sutton, Barto - 1999 - Reinforcement Learning An Introduction.pdf:pdf},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Ye2013,
abstract = {This article gives a new insight of kernel-based (approximation) methods to solve the high-dimensional stochastic partial differential equations. We will combine the techniques of meshfree approximation and kriging interpolation to extend the kernel-based methods for the deterministic data to the stochastic data. The main idea is to endow the Sobolev spaces with the probability measures induced by the positive definite kernels such that the Gaussian random variables can be well-defined on the Sobolev spaces. The constructions of these Gaussian random variables provide the kernel-based approximate solutions of the stochastic models. In the numerical examples of the stochastic Poisson and heat equations, we show that the approximate probability distributions are well-posed for various kinds of kernels such as the compactly supported kernels (Wendland functions) and the Sobolev-spline kernels (Mat$\backslash$'ern functions).},
archivePrefix = {arXiv},
arxivId = {1303.5381},
author = {Ye, Qi},
eprint = {1303.5381},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ye - 2013 - Kernel-based Methods for Stochastic Partial Differential Equations.pdf:pdf},
keywords = {2010 msc,46e22,60g15,60h15,65d05,65n35,erential equation,gaussian field,kernel-based method,kriging interpolation,meshfree approximation,noise,positive definite kernel,stochastic data interpolation,stochastic partial di ff,time and space white},
title = {{Kernel-based Methods for Stochastic Partial Differential Equations}},
url = {http://arxiv.org/abs/1303.5381},
year = {2013}
}
@article{Liu2016a,
abstract = {SBuilding accurate predictive models of clinical multivariate time series is crucial for understanding of the patient condition, the dynamics of a disease, and clinical decision making. A challenging aspect of this process is that the model should be flexible and adaptive to reflect well patientspecific temporal behaviors and this also in the case when the available patient-specific data are sparse and short span. To address this problem we propose and develop an adaptive two-stage forecasting approach for modeling multivariate, irregularly sampled clinical time series of varying lengths. The proposed model (1) learns the population trend from a collection of time series for past patients; (2) captures individual-specific short-term multivariate variability; and (3) adapts by automatically adjusting its predictions based on new observations. The proposed forecasting model is evaluated on a real-world clinical time series dataset. The results demonstrate the benefits of our approach on the prediction tasks for multivariate, irregularly sampled clinical time series, and show that it can outperform both the population based and patient-specific time series prediction models in terms of prediction accuracy.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Liu, Zitao and Hauskrecht, Milos},
doi = {10.1126/science.1249098.Sleep},
eprint = {15334406},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Hauskrecht - 2016 - Learning Adaptive Forecasting Models from Irregularly Sampled Multivariate Clinical Data.pdf:pdf},
isbn = {0000000000000},
issn = {1527-5418},
journal = {Proc Conf AAAI Artif Intell.},
keywords = {Technical Papers: Machine Learning Applications},
number = {February},
pages = {1273--1279},
pmid = {24655651},
title = {{Learning Adaptive Forecasting Models from Irregularly Sampled Multivariate Clinical Data}},
year = {2016}
}
@article{Hill2006,
abstract = {A dynamic network is a special type of network composed of connected transactors which have repeated evolving interaction. Data on large dynamic networks such as telecommunications networks and the Internet are pervasive. However, representing dynamic networks in a manner that is conducive to efficient large-scale analysis is a challenge. In this article, we represent dynamic graphs using a data structure introduced in an earlier article. We advocate their representation because it accounts for the evolution of relationships between transactors through time, mitigates noise at the local transactor level, and allows for the removal of stale relationships. Our work improves on their heuristic arguments by formalizing the representation with three tunable parameters. In doing this, we develop a generic framework for evaluating and tuning any dynamic graph. We show that the storage saving approximations involved in the representation do not affect predictive performance, and typically improve it. We motivate ...},
author = {Hill, Shawndra and Agarwal, Deepak K. and Bell, Robert and Volinsky, Chris},
doi = {10.1198/106186006X139162},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hill et al. - 2006 - Building an effective representation for dynamic networks.pdf:pdf},
isbn = {106186006X},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Approximate subgraphs,Dynamic graphs,Exponential averaging,Fraud detection,Link prediction,Statistical relational learning,Transactional data streams},
number = {3},
pages = {584--608},
title = {{Building an effective representation for dynamic networks}},
volume = {15},
year = {2006}
}
@article{Jayasumana2013,
abstract = {Symmetric Positive Definite (SPD) matrices have become popular to encode image information. Accounting for the geometry of the Riemannian manifold of SPD matrices has proven key to the success of many algorithms. However, most existing methods only approximate the true shape of the manifold locally by its tangent plane. In this paper, inspired by kernel methods, we propose to map SPD matrices to a high dimensional Hilbert space where Euclidean geometry applies. To encode the geometry of the manifold in the mapping, we introduce a family of provably positive definite kernels on the Riemannian manifold of SPD matrices. These kernels are derived from the Gaussian ker- nel, but exploit different metrics on the manifold. This lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of SPD matrices. We demonstrate the benefits of our approach on the problems of pedestrian detection, ob- ject categorization, texture analysis, 2D motion segmentation and Diffusion Tensor Imaging (DTI) segmentation.},
author = {Jayasumana, Sadeep and Hartley, Richard and Salzmann, Mathieu and Li, Hongdong and Harandi, Mehrtash},
doi = {10.1109/CVPR.2013.17},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jayasumana et al. - 2013 - Kernel methods on the riemannian manifold of symmetric positive definite matrices.pdf:pdf},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Hilbert space embedding,RKHS,Riemannian manifolds,Symmetric positive definite matrices,kernel methods,positive definite kernels},
pages = {73--80},
title = {{Kernel methods on the riemannian manifold of symmetric positive definite matrices}},
year = {2013}
}
@article{Silver2017,
abstract = {Three bacterial strains were isolated from activated sludge samples of two treatment plants receiving domestic and industrial wastewaters containing polyethoxylated nonylphenols. One strain (VA160) was isolated on rich medium, and the other two (BCaL1 and BCaL2) on mineral medium containing two industrial mixtures of nonylphenol ethoxylates as the sole carbon source. Strain VA160 was a Gram-positive, spore forming, filamentous bacterium, producing aggregates during growth in liquid medium. On the basis of phylogenetic analysis the strains were assigned to the Bacillus (VA160), Acinetobacter (BCaL1) and Stenothrophomonas (BCaL2) genera. High performance liquid chromatography analysis showed that only the Acinetobacter and Stenothrophomonas strains were involved in the degradation of polyethoxylated nonylphenols. Bacillus VA160, however, when co-cultured with the two degrading strains, induced the formation of cell aggregates and facilitated NPEO degradation. Fluorescent in situ hybridisation on the activated sludge sample from which Bacillus VA160 was isolated, using probes for Gram-positive bacteria with low G + C content, showed that bacteria belonging to this group specifically occurred inside the examined flocs. These observations suggest that the enhanced biodegradation of polyethoxylated nonylphenols in the three-membered co-culture is favoured by VA160-induced aggregation of BcaL1 and BcaL2 cells involved in the process.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {Van Den Driessche}, George and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature24270},
eprint = {1610.00633},
isbn = {3013372370},
issn = {14764687},
journal = {Nature},
number = {7676},
pmid = {15501654},
title = {{Mastering the game of Go without human knowledge}},
volume = {550},
year = {2017}
}
@article{Derbinsky2012,
abstract = {Episodic memory endows agents with numerous general cognitive capabilities, such as action modeling and virtual sensing. However, for long-lived agents, there are numerous unexplored computational challenges in supporting useful episodic-memory functions while maintaining real-time reactivity. In this paper, we review the implementation of episodic memory in Soar and present an expansive evaluation of that system. We demonstrate useful applications of episodic memory across a variety of domains, including games, mobile robotics, planning, and linguistics. In these domains, we characterize properties of environments, tasks, and episodic cues that affect performance, and evaluate the ability of Soar?s episodic memory to support hours to days of real-time operation.},
author = {Derbinsky, Nate and Li, Justin and Laird, John E},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Derbinsky, Li, Laird - 2012 - A Multi-Domain Evaluation of Scaling in a General Episodic Memory.pdf:pdf},
journal = {Proceedings of the 26th AAAI Conference on Artificial Intelligence},
title = {{A Multi-Domain Evaluation of Scaling in a General Episodic Memory}},
year = {2012}
}
@article{Benaim2018,
abstract = {Given a single image x from domain A and a set of images from domain B, our task is to generate the analogous of x in B. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain B is trained. Then, given the new sample x, we create a variational autoencoder for domain A by adapting the layers that are close to the image in order to directly fit x, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample x, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain A. Our code is made publicly available at https://github.com/sagiebenaim/OneShotTranslation},
archivePrefix = {arXiv},
arxivId = {1806.06029},
author = {Benaim, Sagie and Wolf, Lior},
eprint = {1806.06029},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Benaim, Wolf - 2018 - One-Shot Unsupervised Cross Domain Translation.pdf:pdf},
number = {Nips},
title = {{One-Shot Unsupervised Cross Domain Translation}},
url = {http://arxiv.org/abs/1806.06029},
year = {2018}
}
@article{Pineau2016,
abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/ exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
archivePrefix = {arXiv},
arxivId = {1609.04436},
author = {Pineau, Joelle and Tamar, Aviv and Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
doi = {10.1561/2200000049},
eprint = {1609.04436},
isbn = {2200000049},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {5-6},
pmid = {18255791},
title = {{Bayesian Reinforcement Learning: A Survey}},
volume = {8},
year = {2016}
}
@article{Demberg,
author = {Demberg, Vera and Kiagia, Evangelia and Sayeed, Asad},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Demberg, Kiagia, Sayeed - Unknown - The Index of Cognitive Activity as a Measure of Linguistic Processing.pdf:pdf},
keywords = {index of cognitive activity,language,pupillometry,relative clause,self-paced reading,semantic},
number = {30},
title = {{The Index of Cognitive Activity as a Measure of Linguistic Processing}}
}
@article{Mei2006,
abstract = {In the sequential change-point detection literature, most research specifies a required frequency of false alarms at a given pre-change distribution {\$}f{\_}{\{}\backslashtheta{\}}{\$} and tries to minimize the detection delay for every possible post-change distribution {\$}g{\_}{\{}\backslashlambda{\}}{\$}. In this paper, motivated by a number of practical examples, we first consider the reverse question by specifying a required detection delay at a given post-change distribution and trying to minimize the frequency of false alarms for every possible pre-change distribution {\$}f{\_}{\{}\backslashtheta{\}}{\$}. We present asymptotically optimal procedures for one-parameter exponential families. Next, we develop a general theory for change-point problems when both the pre-change distribution {\$}f{\_}{\{}\backslashtheta{\}}{\$} and the post-change distribution {\$}g{\_}{\{}\backslashlambda{\}}{\$} involve unknown parameters. We also apply our approach to the special case of detecting shifts in the mean of independent normal observations.},
author = {Mei, Yajun},
doi = {10.1214/009053605000000859},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mei - 2006 - Sequential change-point detection when unknown parameters are present in the pre-change distribution.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Asymptotic optimality,Change-point,Optimizer,Power one tests,Quality control,Statistical process control,Surveillance},
number = {1},
pages = {92--122},
title = {{Sequential change-point detection when unknown parameters are present in the pre-change distribution}},
volume = {34},
year = {2006}
}
@book{Press1988,
author = {Press, William H and Teukolsky, Saul A and Vetterling, William T and Flannery, Brian P},
booktitle = {Cambridge University Press},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Press et al. - 1988 - The Art of Scientific Computing Second Edition.pdf:pdf},
isbn = {0-521-43108-5},
title = {{The Art of Scientific Computing Second Edition}},
year = {1988}
}
@article{Nickel2016,
abstract = {Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HOLE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator, HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. Experimentally, we show that holographic embeddings are able to outperform state-ofthe-Art methods for link prediction on knowledge graphs and relational learning benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {1510.04935},
author = {Nickel, Maximilian and Rosasco, Lorenzo and Poggio, Tomaso},
eprint = {1510.04935},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nickel, Rosasco, Poggio - 2016 - Holographic embeddings of knowledge graphs.pdf:pdf},
isbn = {9781577357605},
journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
keywords = {Technical Papers: Machine Learning Methods},
pages = {1955--1961},
title = {{Holographic embeddings of knowledge graphs}},
year = {2016}
}
@article{Scherbaum2010,
abstract = {To study the process of decision-making under conflict, researchers typically analyze response latency and accuracy. However, these tools provide little evidence regarding how the resolution of conflict unfolds over time. Here, we analyzed the trajectories of mouse movements while participants performed a continuous version of a spatial conflict task (the Simon task). We applied a novel combination of multiple regression analysis and distribution analysis to determine how conflict on the present trial and carry-over from the previous trial affect responding. Response on the previous trial and the degree of conflict on the previous and the current trial all influenced performance, but they did so differently: The previous response influenced the early part of the mouse trajectory, but the degree of previous and current conflict influenced later parts. This suggests that in this task experiencing conflict may not proactively ready the system to handle conflict on the next trial; rather, when conflict is experienced on the subsequent trial the previous compensatory processing may be re-activated more efficiently. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Scherbaum, Stefan and Dshemuchadse, Maja and Fischer, Rico and Goschke, Thomas},
doi = {10.1016/j.cognition.2010.02.004},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Scherbaum et al. - 2010 - How decisions evolve The temporal dynamics of action selection.pdf:pdf},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
keywords = {Action dynamics,Cognitive control,Conflict,Decision-making,Mouse trajectory,Simon task},
number = {3},
pages = {407--416},
pmid = {20227687},
publisher = {Elsevier B.V.},
title = {{How decisions evolve: The temporal dynamics of action selection}},
url = {http://dx.doi.org/10.1016/j.cognition.2010.02.004},
volume = {115},
year = {2010}
}
@article{Mandt2017,
abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
archivePrefix = {arXiv},
arxivId = {1704.04289},
author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
doi = {1704.04289v1},
eprint = {1704.04289},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mandt, Hoffman, Blei - 2017 - Stochastic Gradient Descent as Approximate Bayesian Inference.pdf:pdf},
issn = {15337928},
keywords = {approximate bayesian inference,stochas-,stochastic differential equations,stochastic optimization,tic gradient mcmc,variational inference},
pages = {1--35},
title = {{Stochastic Gradient Descent as Approximate Bayesian Inference}},
url = {http://arxiv.org/abs/1704.04289},
volume = {18},
year = {2017}
}
@article{Bach2006,
abstract = {We give a probabilistic interpretation of canonical correlation (CCA) analysis as a latent variable model for two Gaussian random vectors. Our interpretation is similar to the prob- abilistic interpretation of principal component analysis (Tipping and Bishop, 1999, Roweis, 1998). In addition, we can interpret Fisher linear discriminant analysis (LDA) as CCA between appropriately defined vectors.},
author = {Bach, Francis R and Jordan, Michael I},
doi = {10.1002/ps.2016},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bach, Jordan - 2006 - A Probabilistic Interpretation of Canonical Correlation Analysis.pdf:pdf},
issn = {15264998},
journal = {Dept Statist Univ California Berkeley CA Tech Rep},
pages = {1--11},
pmid = {20814880},
title = {{A Probabilistic Interpretation of Canonical Correlation Analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.4942{\&}rep=rep1{\&}type=pdf},
volume = {688},
year = {2006}
}
@article{Gold2017,
author = {Gold, David A. and Zacks, Jeffrey M. and Flores, Shaney},
doi = {10.1186/s41235-016-0043-2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gold, Zacks, Flores - 2017 - Effects of cues to event segmentation on subsequent memory.pdf:pdf},
issn = {2365-7464},
journal = {Cognitive Research: Principles and Implications},
month = {dec},
number = {1},
pages = {1},
title = {{Effects of cues to event segmentation on subsequent memory}},
url = {http://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-016-0043-2},
volume = {2},
year = {2017}
}
@article{Battaglia2013,
abstract = {In a glance, we can perceive whether a stack of dishes will topple, a branch will support a child's weight, a grocery bag is poorly packed and liable to tear or crush its contents, or a tool is firmly attached to a table or free to be lifted. Such rapid physical inferences are central to how people interact with the world and with each other, yet their computational underpinnings are poorly understood. We propose a model based on an "intuitive physics engine," a cognitive mechanism similar to computer engines that simulate rich physics in video games and graphics, but that uses approximate, probabilistic simulations to make robust and fast inferences in complex natural scenes where crucial information is unobserved. This single model fits data from five distinct psychophysical tasks, captures several illusions and biases, and explains core aspects of human mental models and common-sense reasoning that are instrumental to how humans understand their everyday world.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.2263v1},
author = {Battaglia, P. W. and Hamrick, J. B. and Tenenbaum, J. B.},
doi = {10.1073/pnas.1306572110},
eprint = {arXiv:1404.2263v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Battaglia, Hamrick, Tenenbaum - 2013 - Simulation as an engine of physical scene understanding.pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$n0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {45},
pages = {18327--18332},
pmid = {24145417},
title = {{Simulation as an engine of physical scene understanding}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1306572110},
volume = {110},
year = {2013}
}
@article{Gu2015,
abstract = {Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods, performance is critically dependent on the proposal distribution: a bad proposal can lead to arbitrarily inaccurate estimates of the target distribution. This paper presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods including the Extended Kalman and Unscented Particle Filters. Experiments also indicate that improved inference translates into improved parameter learning when NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to train a latent variable recurrent neural network (LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive SMC methods and the recent work in scalable, black-box variational inference.},
archivePrefix = {arXiv},
arxivId = {1506.03338},
author = {Gu, Shixiang and Ghahramani, Zoubin and Turner, Richard E.},
eprint = {1506.03338},
issn = {10495258},
title = {{Neural Adaptive Sequential Monte Carlo}},
year = {2015}
}
@article{Margossian2018,
abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of automatic differentiation however requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open problems include the extension of current packages to provide more specialized routines, and efficient methods to perform higher-order differentiation.},
archivePrefix = {arXiv},
arxivId = {1811.05031},
author = {Margossian, Charles C.},
eprint = {1811.05031},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Margossian - 2018 - A Review of automatic differentiation and its efficient implementation.pdf:pdf},
pages = {1--32},
title = {{A Review of automatic differentiation and its efficient implementation}},
url = {http://arxiv.org/abs/1811.05031},
year = {2018}
}
@article{Patacchiola2019,
abstract = {Humans tackle new problems by making inferences that go far beyond the information available, reusing what they have previously learned, and weighing different alternatives in the face of uncertainty. Incorporating these abilities in an artificial system is a major objective in machine learning. Towards this goal, we adapt Gaussian Processes (GPs) to tackle the problem of few-shot learning. We propose a simple, yet effective variant of deep kernel learning in which the kernel is transferred across tasks, which we call deep kernel transfer. This approach is straightforward to implement, provides uncertainty quantification, and does not require estimation of task-specific parameters. We empirically demonstrate that the proposed method outperforms several state-of-the-art algorithms in few-shot regression, classification, and cross-domain adaptation.},
archivePrefix = {arXiv},
arxivId = {1910.05199},
author = {Patacchiola, Massimiliano and Turner, Jack and Crowley, Elliot J. and O'Boyle, Michael and Storkey, Amos},
eprint = {1910.05199},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Patacchiola et al. - 2019 - Deep Kernel Transfer in Gaussian Processes for Few-shot Learning.pdf:pdf},
title = {{Deep Kernel Transfer in Gaussian Processes for Few-shot Learning}},
url = {http://arxiv.org/abs/1910.05199},
year = {2019}
}
@article{Song2018,
abstract = {Psychometric functions are typically estimated by fitting a parametric model to categorical subject responses. Procedures to estimate unidimensional psychometric functions (i.e., psychometric curves) have been subjected to the most research, with modern adaptive methods capable of quickly obtaining accurate estimates. These capabilities have been extended to some multidimensional psychometric functions (i.e., psychometric fields) that are easily parameterizable, but flexible procedures for general psychometric field estimation are lacking. This study introduces a nonparametric Bayesian psychometric field estimator operating on subject queries sequentially selected to improve the estimate in some targeted way. This estimator implements probabilistic classification using Gaussian processes trained by active learning. The accuracy and efficiency of two different actively sampled estimators were compared to two non-actively sampled estimators for simulations of one of the simplest psychometric fields in common use: the pure-tone audiogram. The actively sampled methods achieved estimate accuracy equivalent to the non-actively sampled methods with fewer observations. This trend held for a variety of audiogram phenotypes representative of the range of human auditory perception. Gaussian process classification is a general estimation procedure capable of extending to multiple input variables and response classes. Its success with a two-dimensional psychometric field informed by binary subject responses holds great promise for extension to complex perceptual models currently inaccessible to practical estimation.},
author = {Song, Xinyu D. and Sukesan, Kiron A. and Barbour, Dennis L.},
doi = {10.3758/s13414-017-1460-0},
file = {:Users/mshvarts/Downloads/Song2018{\_}Article{\_}BayesianActiveProbabilisticCla.pdf:pdf},
issn = {1943393X},
journal = {Attention, Perception, and Psychophysics},
keywords = {Audition,Psychoacoustics,Psychometrics/testing},
number = {3},
pages = {798--812},
pmid = {29256098},
publisher = {Attention, Perception, {\&} Psychophysics},
title = {{Bayesian active probabilistic classification for psychometric field estimation}},
volume = {80},
year = {2018}
}
@article{Tipping2012,
author = {Tipping, Michael E},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tipping - 2012 - Probabilistic principal component analysis.pdf:pdf},
journal = {Society},
number = {3},
pages = {611--622},
title = {{Probabilistic principal component analysis}},
volume = {61},
year = {2012}
}
@article{Allamanis2017,
abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarNaming, in which a network attempts to predict the name of a variable given its usage, and VarMisuse, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarMisuse identifies a number of bugs in mature open-source projects.},
archivePrefix = {arXiv},
arxivId = {1711.00740},
author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
doi = {arXiv:1711.00740v3},
eprint = {1711.00740},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Allamanis, Brockschmidt, Khademi - 2017 - Learning to Represent Programs with Graphs.pdf:pdf},
pages = {1--17},
title = {{Learning to Represent Programs with Graphs}},
url = {http://arxiv.org/abs/1711.00740},
year = {2017}
}
@article{Fernandes2006,
author = {Fernandes, Paulo and Plateau, Brigitte and Stewart, William J and Fernandes, Paulo and Plateau, Brigitte and Multipli-, William J Stewart Efficient Descriptor-vector},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Fernandes et al. - 2006 - Efficient Descriptor-Vector Multiplications in Stochastic Automata Networks To cite this version Efficient De.pdf:pdf},
keywords = {VECTOR-DESCRIPTOR MULTIPLICATION, GENERALIZED TENS},
title = {{Efficient Descriptor-Vector Multiplications in Stochastic Automata Networks To cite this version : Efficient Descriptor-Vector Multiplications in Stochastic Automata Networks}},
year = {2006}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
doi = {10.2507/daaam.scibook.2010.27},
eprint = {1701.07875},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Chintala, Bottou - 2017 - Wasserstein GAN.pdf:pdf},
isbn = {1406.2661},
issn = {1701.07875},
month = {jan},
pmid = {19963286},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{Fox2009,
abstract = {Many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our nonparametric Bayesian approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efficient joint sampling of the mode and state sequences. The utility and flexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, and the IBOVESPA stock index.},
author = {Fox, Emily B. and Sudderth, Erik B. and Jordan, Michael I. and Willsky, Alan S.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Fox et al. - 2009 - Nonparametric Bayesian learning of switching linear dynamical systems.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems 21 - Proceedings of the 2008 Conference},
pages = {457--464},
title = {{Nonparametric Bayesian learning of switching linear dynamical systems}},
year = {2009}
}
@article{Pohlmeyer2011,
abstract = {We describe a closed-loop brain-computer interface that re-ranks an image database by iterating between user generated 'interest' scores and computer vision generated visual similarity measures. The interest scores are based on decoding the electroencephalographic (EEG) correlates of target detection, attentional shifts and self-monitoring processes, which result from the user paying attention to target images interspersed in rapid serial visual presentation (RSVP) sequences. The highest scored images are passed to a semi-supervised computer vision system that reorganizes the image database accordingly, using a graph-based representation that captures visual similarity between images. The system can either query the user for more information, by adaptively resampling the database to create additional RSVP sequences, or it can converge to a 'done' state. The done state includes a final ranking of the image database and also a 'guess' of the user's chosen category of interest. We find that the closed-loop system's re-rankings can substantially expedite database searches for target image categories chosen by the subjects. Furthermore, better reorganizations are achieved than by relying on EEG interest rankings alone, or if the system were simply run in an open loop format without adaptive resampling.},
author = {Pohlmeyer, Eric A. and Wang, Jun and Jangraw, David C. and Lou, Bin and Chang, Shih Fu and Sajda, Paul},
doi = {10.1088/1741-2560/8/3/036025},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pohlmeyer et al. - 2011 - Closing the loop in cortically-coupled computer vision A brain-computer interface for searching image database.pdf:pdf},
isbn = {1741-2552 (Electronic)$\backslash$n1741-2552 (Linking)},
issn = {17412560},
journal = {Journal of Neural Engineering},
number = {3},
pmid = {21562364},
title = {{Closing the loop in cortically-coupled computer vision: A brain-computer interface for searching image databases}},
volume = {8},
year = {2011}
}
@article{Vrugt2009,
author = {Vrugt, J. A. and ter Braak, C.J.F. and Diks, C.G.H. and Robinson, B. A. and Hyman, J. M. and Higdon, D.},
doi = {10.1515/IJNSNS.2009.10.3.273},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Vrugt et al. - 2009 - Accelerating Markov Chain Monte Carlo Simulation by Differential Evolution with Self-Adaptive Randomized Subspace.pdf:pdf},
issn = {2191-0294},
journal = {International Journal of Nonlinear Sciences and Numerical Simulation},
month = {jan},
number = {3},
pages = {1--4},
title = {{Accelerating Markov Chain Monte Carlo Simulation by Differential Evolution with Self-Adaptive Randomized Subspace Sampling}},
url = {http://doi.wiley.com/10.1002/cncr.27633 https://linkinghub.elsevier.com/retrieve/pii/S105381191630057X https://www.degruyter.com/view/j/ijnsns.2009.10.3/ijnsns.2009.10.3.273/ijnsns.2009.10.3.273.xml},
volume = {10},
year = {2009}
}
@article{Keeley2019,
abstract = {Gaussian Process Factor Analysis (GPFA) has been broadly applied to the problem of identifying smooth, low-dimensional temporal structure underlying large-scale neural recordings. However, spike trains are non-Gaussian, which motivates combining GPFA with discrete observation models for binned spike count data. The drawback to this approach is that GPFA priors are not conjugate to count model likelihoods, which makes inference challenging. Here we address this obstacle by introducing a fast, approximate inference method for non-conjugate GPFA models. Our approach uses orthogonal second-order polynomials to approximate the nonlinear terms in the non-conjugate log-likelihood, resulting in a method we refer to as polynomial approximate log-likelihood (PAL) estimators. This approximation allows for accurate closed-form evaluation of marginal likelihood and fast numerical optimization for parameters and hyperparameters. We derive PAL estimators for GPFA models with binomial, Poisson, and negative binomial observations, and additionally show that the parameters obtained can be used to initialize black-box variational inference, which significantly speeds up and stabilizes the inference procedure for these factor analytic models. We apply these methods to data from mouse visual cortex and monkey higher-order visual and parietal cortices, and compare GPFA under three different spike count observation models to traditional GPFA. We demonstrate that PAL estimators achieve fast and accurate extraction of latent structure from multi-neuron spike train data.},
archivePrefix = {arXiv},
arxivId = {1906.03318},
author = {Keeley, Stephen L. and Zoltowski, David M. and Yu, Yiyi and Yates, Jacob L. and Smith, Spencer L. and Pillow, Jonathan W.},
eprint = {1906.03318},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Keeley et al. - 2019 - Efficient non-conjugate Gaussian process factor models for spike count data using polynomial approximations.pdf:pdf},
title = {{Efficient non-conjugate Gaussian process factor models for spike count data using polynomial approximations}},
url = {http://arxiv.org/abs/1906.03318},
year = {2019}
}
@article{Taylor1967,
abstract = {An adaptive procedure for rapid and efficient psychophysical testing is described. PEST (Parameter Estimation by Sequential Testing) was designed with maximally efficient trial-by-trial sequential decisions at each stimulus level, in a sequence which tends to converge on a selected target level. An appendix introduces an approach to measuring test efficiency as applied to psychophysical testing problems.},
author = {Taylor, M. M. and Creelman, C. Douglas},
doi = {10.1121/1.1910407},
file = {:Users/mshvarts/Downloads/1.1910407.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4A},
pages = {782--787},
title = {{PEST: Efficient Estimates on Probability Functions}},
volume = {41},
year = {1967}
}
@article{Tucker2017,
abstract = {Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al. 2016, Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, $\backslash$emph{\{}unbiased{\}} gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.},
archivePrefix = {arXiv},
arxivId = {1703.07370},
author = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson, Dieterich and Sohl-Dickstein, Jascha},
eprint = {1703.07370},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tucker et al. - 2017 - REBAR Low-variance, unbiased gradient estimates for discrete latent variable models.pdf:pdf},
issn = {10495258},
number = {Nips},
pages = {1--17},
title = {{REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models}},
url = {http://arxiv.org/abs/1703.07370},
year = {2017}
}
@article{Ohlson2013,
abstract = {In this paper, the multilinear normal distribution is introduced as an extension of the matrix-variate normal distribution. Basic properties such as marginal and conditional distributions, moments, and the characteristic function, are also presented. A trilinear example is used to explain the general contents at a simpler level. The estimation of parameters using a flip-flop algorithm is also briefly discussed. {\textcopyright} 2011 Elsevier Inc.},
author = {Ohlson, Martin and {Rauf Ahmad}, M. and von Rosen, Dietrich},
doi = {10.1016/j.jmva.2011.05.015},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ohlson, Rauf Ahmad, von Rosen - 2013 - The multilinear normal distribution Introduction and some basic properties.pdf:pdf},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Flip-flop algorithm,Marginal and conditional distributions,Matrix normal distribution,Maximum likelihood estimators,Moments,Tensor product},
pages = {37--47},
publisher = {Elsevier Inc.},
title = {{The multilinear normal distribution: Introduction and some basic properties}},
url = {http://dx.doi.org/10.1016/j.jmva.2011.05.015},
volume = {113},
year = {2013}
}
@article{Ranganath2016,
abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
archivePrefix = {arXiv},
arxivId = {1610.09033},
author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
eprint = {1610.09033},
issn = {10495258},
title = {{Operator Variational Inference}},
year = {2016}
}
@article{Letham2019,
abstract = {Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasi-Monte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.07094v2},
author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
doi = {10.1214/18-BA1110},
eprint = {arXiv:1706.07094v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Letham et al. - 2019 - Constrained Bayesian optimization with noisy experiments.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Bayesian optimization,Quasi-Monte Carlo methods,Randomized experiments},
number = {2},
pages = {495--519},
title = {{Constrained Bayesian optimization with noisy experiments}},
volume = {14},
year = {2019}
}
@article{Srivastava2014a,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
pmid = {23285570},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Hendrycks2019,
abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.},
archivePrefix = {arXiv},
arxivId = {1812.04606},
author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
eprint = {1812.04606},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hendrycks, Mazeika, Dietterich - 2019 - Deep anomaly detection with outlier exposure.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
pages = {1--18},
title = {{Deep anomaly detection with outlier exposure}},
year = {2019}
}
@phdthesis{Jarch2016,
author = {Jarch, Julian D},
doi = {10.18452/17641},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jarch - 2016 - A Machine Learning Perspective on Repeated Measures Gaussian Process Panel and Person-Specific EEG Modeling.pdf:pdf},
title = {{A Machine Learning Perspective on Repeated Measures: Gaussian Process Panel and Person-Specific EEG Modeling}},
year = {2016}
}
@article{Yger2013,
abstract = {Recently, covariance matrices have been shown to be interesting features for signal classification and object detection. In this paper, we review and compare the existing kernels on covariance matrices and explore their use for EEG classification in Brain-Computer Interfaces (BCI). This study adresses both experimental and theoretical aspects of the problem. Beside the apparent complexity of the kernels, we show that this approach simplifies the whole BCI system. Finally, we empirically demonstrate that this simpler approach obtains state-of-the-art results on the BCI competition IV dataset 2a. {\textcopyright} 2013 IEEE.},
author = {Yger, Florian},
doi = {10.1109/MLSP.2013.6661972},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yger - 2013 - A review of kernels on covariance matrices for BCI applications.pdf:pdf},
isbn = {9781479911806},
issn = {21610363},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {Brain Compute Interfaces,Covariance matrices,Kernel methods,Riemannian manifold},
pages = {1--6},
publisher = {IEEE},
title = {{A review of kernels on covariance matrices for BCI applications}},
year = {2013}
}
@article{Banerjee2018,
abstract = {The problem of detecting changes in firing patterns in neural data is studied. The problem is formulated as a quickest change detection problem. Important algorithms from the literature are reviewed. A new algorithmic technique is discussed to detect deviations from learned baseline behavior. The algorithms studied can be applied to both spike and local field potential data. The algorithms are applied to mice spike data to verify the presence of behavioral learning.},
archivePrefix = {arXiv},
arxivId = {1809.00358},
author = {Banerjee, Taposh and Allsop, Stephen and Tye, Kay M. and Ba, Demba and Tarokh, Vahid},
eprint = {1809.00358},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Banerjee et al. - 2018 - Sequential Detection of Regime Changes in Neural Data.pdf:pdf},
title = {{Sequential Detection of Regime Changes in Neural Data}},
url = {http://arxiv.org/abs/1809.00358},
year = {2018}
}
@article{Jiang2018,
abstract = {Multivariate testing has recently emerged as a promising technique in web interface design. In contrast to the standard A/B testing, multivariate approach aims at evaluating a large number of values in a few key variables systematically. The Taguchi method is a practical implementation of this idea, focusing on orthogonal combinations of values. This paper evaluates an alternative method: population-based search, i.e. evolutionary optimization. Its performance is compared to that of the Taguchi method in several simulated conditions, including an orthogonal one designed to favor the Taguchi method, and two realistic conditions with dependences between variables. Evolutionary optimization is found to perform significantly better especially in the realistic conditions, suggesting that it forms a good approach for web interface design in the future.},
archivePrefix = {arXiv},
arxivId = {1808.08347},
author = {Jiang, Jingbo and Legrand, Diego and Severn, Robert and Miikkulainen, Risto},
eprint = {1808.08347},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jiang et al. - 2018 - A Comparison of the Taguchi Method and Evolutionary Optimization in Multivariate Testing. (arXiv1808.08347v1 cs.NE.pdf:pdf},
keywords = {evolution algorithm,multivariate testing,taguchi,web interface design},
title = {{A Comparison of the Taguchi Method and Evolutionary Optimization in Multivariate Testing. (arXiv:1808.08347v1 [cs.NE])}},
url = {http://arxiv.org/abs/1808.08347},
year = {2018}
}
@article{Russell1991,
abstract = {In this paper we outline a general approach to the study of metareasoning, not in the sense of explicating the semantics of explicitly specified meta-level control policies, but in the sense of providing a basis for selecting and justifying computational actions. This research contributes to a developing attack on the problem of resource-bounded rationality, by providing a means for analyzing and generating optimal computational strategies. Because reasoning about a computation without doing it necessarily involves uncertainty as to its outcome, probability and decision theory will be our main tools. We develop a general formula for the utility of computations, this utility being derived directly from the ability of computations to affect an agent's external actions. We address some philosophical difficulties that arise in specifying this formula, given our assumption of limited rationality. We also describe a methodology for applying the theory to particular problem-solving systems, and provide a brief sketch of the resulting algorithms and their performance. {\textcopyright} 1991.},
author = {Russell, Stuart and Wefald, Eric},
doi = {10.1016/0004-3702(91)90015-C},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Russell, Wefald - 1991 - Principles of metareasoning.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-3},
pages = {361--395},
title = {{Principles of metareasoning}},
volume = {49},
year = {1991}
}
@article{Mnih2016,
abstract = {Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators.},
archivePrefix = {arXiv},
arxivId = {1602.06725},
author = {Mnih, Andriy and Rezende, Danilo J.},
eprint = {1602.06725},
isbn = {1602.06725},
issn = {1938-7228},
title = {{Variational inference for Monte Carlo objectives}},
year = {2016}
}
@article{Browder2019,
abstract = {In this study we investigated the use of simple vibrotactile signals to simulate contact with a virtual object. In particular we explored the relation between properties of the signal and the perceived hardness of the object. The space of stimuli is large, and we have no plausible a priori model for the relationship of parameters to percept. Thus we made use of non-parametric Bayesian methods, in particular utilizing Gaussian process priors. We show that this method both gives insight into the phenomenon of interest and well-predicts a second, separate data set collected via the method of constant stimuli. Thus we argue that it could be a fruitful approach for attacking a variety of perceptual problems.},
author = {Browder, Jonathan and Bochereau, S{\'{e}}r{\'{e}}na and {Van Beek}, Femke and King, Raymond},
doi = {10.1109/WHC.2019.8816102},
file = {:Users/mshvarts/Downloads/Stiffness-in-Virtual-Contact-Events-A-Non-Parametric-Bayesian-Approach.pdf:pdf},
isbn = {9781538694619},
journal = {2019 IEEE World Haptics Conference, WHC 2019},
pages = {515--520},
title = {{Stiffness in Virtual Contact Events: A Non-Parametric Bayesian Approach}},
year = {2019}
}
@article{Hudson2018,
abstract = {We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9{\%} accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.},
archivePrefix = {arXiv},
arxivId = {1803.03067},
author = {Hudson, Drew A and Manning, Christopher D},
eprint = {1803.03067},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hudson, Manning - 2018 - Compositional Attention Networks for Machine Reasoning.pdf:pdf},
month = {mar},
pages = {1--20},
title = {{Compositional Attention Networks for Machine Reasoning}},
url = {http://arxiv.org/abs/1803.03067},
year = {2018}
}
@article{Frazier2015,
abstract = {We introduce Bayesian optimization, a technique developed for optimizing time-consuming engineering simulations and for fitting machine learning models on large datasets. Bayesian optimization guides the choice of experiments during materials design and discovery to find good material designs in as few experiments as possible. We focus on the case when materials designs are parameterized by a low-dimensional vector. Bayesian optimization is built on a statistical technique called Gaussian process regression, which allows predicting the performance of a new design based on previously tested designs. After providing a detailed introduction to Gaussian process regression, we introduce two Bayesian optimization methods: expected improvement, for design problems with noise-free evaluations; and the knowledge-gradient method, which generalizes expected improvement and may be used in design problems with noisy evaluations. Both methods are derived using a value-of-information analysis, and enjoy one-step Bayes-optimality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01349v1},
author = {Frazier, Peter I. and Wang, Jialei},
doi = {10.1007/978-3-319-23871-5_3},
eprint = {arXiv:1506.01349v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Frazier, Wang - 2015 - Bayesian optimization for materials design.pdf:pdf},
isbn = {9783319238708},
issn = {0933033X},
journal = {Springer Series in Materials Science},
pages = {45--75},
title = {{Bayesian optimization for materials design}},
volume = {225},
year = {2015}
}
@article{Linderman2016,
abstract = {Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These "recurrent" switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable.},
archivePrefix = {arXiv},
arxivId = {1610.08466},
author = {Linderman, Scott W. and Miller, Andrew C. and Adams, Ryan P. and Blei, David M. and Paninski, Liam and Johnson, Matthew J.},
eprint = {1610.08466},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Linderman et al. - 2016 - Recurrent switching linear dynamical systems.pdf:pdf},
title = {{Recurrent switching linear dynamical systems}},
url = {http://arxiv.org/abs/1610.08466},
year = {2016}
}
@article{Arbabi2016,
abstract = {We establish the convergence of a class of numerical algorithms, known as Dynamic Mode Decomposition (DMD), for computation of the eigenvalues and eigenfunctions of the infinite-dimensional Koopman operator. The algorithms act on data coming from observables on a state space, arranged in Hankel-type matrices. The proofs utilize the assumption that the underlying dynamical system is ergodic. This includes the classical measure-preserving systems, as well as systems whose attractors support a physical measure. Our approach relies on the observation that vector projections in DMD can be used to approximate the function projections by the virtue of Birkhoff's ergodic theorem. Using this fact, we show that applying DMD to Hankel data matrices in the limit of infinite-time observations yields the true Koopman eigenfunctions and eigenvalues. We also show that the Singular Value Decomposition, which is the central part of most DMD algorithms, converges to the Proper Orthogonal Decomposition of observables. We use this result to obtain a representation of the dynamics of systems with continuous spectrum based on the lifting of the coordinates to the space of observables. The numerical application of these methods is demonstrated using well-known dynamical systems and examples from computational fluid dynamics.},
archivePrefix = {arXiv},
arxivId = {1611.06664},
author = {Arbabi, Hassan and Mezi{\'{c}}, Igor},
doi = {10.1137/17M1125236},
eprint = {1611.06664},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Arbabi, Mezi{\'{c}} - 2017 - Ergodic Theory, Dynamic Mode Decomposition, and Computation of Spectral Properties of the Koopman Operator.pdf:pdf},
issn = {1536-0040},
journal = {SIAM Journal on Applied Dynamical Systems},
keywords = {37a30,37m10,37n10,65p99,dmd,dynamic mode decomposition,ergodic theory,han-,kel matrix,koopman operator,mathematics subject classification,pod,proper orthogonal decomposition,singular value decomposition,svd},
month = {jan},
number = {4},
pages = {2096--2126},
title = {{Ergodic Theory, Dynamic Mode Decomposition, and Computation of Spectral Properties of the Koopman Operator}},
url = {http://arxiv.org/abs/1611.06664{\%}0Ahttp://dx.doi.org/10.1137/17M1125236 http://epubs.siam.org/doi/10.1137/17M1125236},
volume = {16},
year = {2017}
}
@article{Sharir2016,
abstract = {Casting neural networks in generative frameworks is a highly sought-after endeavor these days. Contemporary methods, such as Generative Adversarial Networks, capture some of the generative capabilities, but not all. In particular, they lack the ability of tractable marginalization, and thus are not suitable for many tasks. Other methods, based on arithmetic circuits and sum-product networks, do allow tractable marginalization, but their performance is challenged by the need to learn the structure of a circuit. Building on the tractability of arithmetic circuits, we leverage concepts from tensor analysis, and derive a family of generative models we call Tensorial Mixture Models (TMMs). TMMs assume a simple convolutional network structure, and in addition, lend themselves to theoretical analyses that allow comprehensive understanding of the relation between their structure and their expressive properties. We thus obtain a generative model that is tractable on one hand, and on the other hand, allows effective representation of rich distributions in an easily controlled manner. These two capabilities are brought together in the task of classification under missing data, where TMMs deliver state of the art accuracies with seamless implementation and design.},
archivePrefix = {arXiv},
arxivId = {1610.04167},
author = {Sharir, Or and Tamari, Ronen and Cohen, Nadav and Shashua, Amnon},
eprint = {1610.04167},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sharir et al. - 2016 - Tensorial Mixture Models.pdf:pdf},
title = {{Tensorial Mixture Models}},
url = {http://arxiv.org/abs/1610.04167},
year = {2016}
}
@article{Tiganj2018,
abstract = {Natural learners must compute an estimate of future outcomes that follow from a stimulus in continuous time. Widely used reinforcement learning algorithms discretize continuous time and estimate either transition functions from one step to the next (model-based algorithms) or a scalar value of exponentially-discounted future reward using the Bellman equation (model-free algorithms). An important drawback of model-based algorithms is that computational cost grows linearly with the amount of time to be simulated. On the other hand, an important drawback of model-free algorithms is the need to select a time-scale required for exponential discounting. We present a computational mechanism, developed based on work in psychology and neuroscience, for computing a scale-invariant timeline of future outcomes. This mechanism efficiently computes an estimate of inputs as a function of future time on a logarithmically-compressed scale, and can be used to generate a scale-invariant power-law-discounted estimate of expected future reward. The representation of future time retains information about what will happen when. The entire timeline can be constructed in a single parallel operation which generates concrete behavioral and neural predictions. This computational mechanism could be incorporated into future reinforcement learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1802.06426},
author = {Tiganj, Zoran and Gershman, Samuel J. and Sederberg, Per B. and Howard, Marc W.},
eprint = {1802.06426},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tiganj et al. - 2019 - Estimating Scale-Invariant Future in Continuous Time.pdf:pdf},
month = {feb},
title = {{Estimating scale-invariant future in continuous time}},
url = {http://arxiv.org/abs/1802.06426},
year = {2018}
}
@article{B.Schon2011,
abstract = {In these note we provide some important properties of the multivariate$\backslash$r$\backslash$nGaussian, which are important building blocks for more sophisticated$\backslash$r$\backslash$nprobabilistic models. We also illustrate how these properties can be used$\backslash$r$\backslash$nto derive the Kalman filter.},
author = {{B. Schon}, Thomas and Lindsten, Fredrik},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/B. Schon, Lindsten - 2011 - Manipulating the Multivariate Gaussian Density.pdf:pdf},
number = {3},
pages = {1--10},
title = {{Manipulating the Multivariate Gaussian Density}},
url = {http://user.it.uu.se/{~}thosc112/pubpdf/schonl2011.pdf},
volume = {2},
year = {2011}
}
@article{Guan2016,
abstract = {Depth constancy is the ability to perceive a fixed depth interval in the world as constant despite changes in viewing distance and the spatial scale of depth variation. It is well known that the spatial frequency of depth variation has a large effect on threshold. In the first experiment, we determined that the visual system compensates for this differential sensitivity when the change in disparity is suprathreshold, thereby attaining constancy similar to contrast constancy in the luminance domain. In a second experiment, we examined the ability to perceive constant depth when the spatial frequency and viewing distance both changed. To attain constancy in this situation, the visual system has to estimate distance. We investigated this ability when vergence, accommodation and vertical disparity are all presented accurately and therefore provided veridical information about viewing distance. We found that constancy is nearly complete across changes in viewing distance. Depth constancy is most complete when the scale of the depth relief is constant in the world rather than when it is constant in angular units at the retina. These results bear on the efficacy of algorithms for creating stereo content.},
author = {Guan, Phillip and Banks, Martin S.},
doi = {10.1098/rstb.2015.0253},
file = {:Users/mshvarts/Downloads/rstb20150253.pdf:pdf},
issn = {14712970},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {Depth perception,Human vision,Stereopsis},
number = {1697},
pmid = {27269596},
title = {{Stereoscopic depth constancy}},
volume = {371},
year = {2016}
}
@article{Leike2017,
abstract = {We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.},
archivePrefix = {arXiv},
arxivId = {1711.09883},
author = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A. and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
eprint = {1711.09883},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Leike et al. - 2017 - AI Safety Gridworlds.pdf:pdf},
title = {{AI Safety Gridworlds}},
url = {http://arxiv.org/abs/1711.09883},
year = {2017}
}
@article{Bronstein2017,
abstract = {Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
author = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
doi = {10.1109/MSP.2017.2693418},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bronstein et al. - 2017 - Geometric Deep Learning Going beyond Euclidean data.pdf:pdf},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {4},
pages = {18--42},
publisher = {IEEE},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
volume = {34},
year = {2017}
}
@article{Figurnov2018b,
abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
archivePrefix = {arXiv},
arxivId = {1805.08498},
author = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
eprint = {1805.08498},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Figurnov, Mohamed, Mnih - 2018 - Implicit Reparameterization Gradients.pdf:pdf},
number = {NeurIPS},
title = {{Implicit Reparameterization Gradients}},
url = {http://arxiv.org/abs/1805.08498},
year = {2018}
}
@article{Hensman2014,
abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.},
archivePrefix = {arXiv},
arxivId = {1411.2005},
author = {Hensman, James and Matthews, Alex and Ghahramani, Zoubin},
eprint = {1411.2005},
issn = {15337928},
title = {{Scalable Variational Gaussian Process Classification}},
year = {2014}
}
@article{Saab2019,
author = {Saab, Khaled and Dunnmon, Jared and Ratner, Alexander and Rubin, Daniel and Re, Christopher},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Saab et al. - 2019 - Improving Sample Complexity with Observational Supervision.pdf:pdf},
journal = {Iclr - Lld},
pages = {1--8},
title = {{Improving Sample Complexity with Observational Supervision}},
year = {2019}
}
@article{Milletari2018,
abstract = {We present a statistical mechanics model of deep feed forward neural networks (FFN). Our energy-based approach naturally explains several known results and heuristics, providing a solid theoretical framework and new instruments for a systematic development of FFN. We infer that FFN can be understood as performing three basic steps: encoding, representation validation and propagation. We obtain a set of natural activations -- such as sigmoid, {\$}\backslashtanh{\$} and ReLu -- together with a state-of-the-art one, recently obtained by Ramachandran et al.(arXiv:1710.05941) using an extensive search algorithm. We term this activation ESP (Expected Signal Propagation), explain its probabilistic meaning, and study the eigenvalue spectrum of the associated Hessian on classification tasks. We find that ESP allows for faster training and more consistent performances over a wide range of network architectures.},
archivePrefix = {arXiv},
arxivId = {1805.08786},
author = {Milletar{\'{i}}, Mirco and Chotibut, Thiparat and Trevisanutto, Paolo E.},
eprint = {1805.08786},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Milletar{\'{i}}, Chotibut, Trevisanutto - 2018 - Expectation propagation a probabilistic view of Deep Feed Forward Networks.pdf:pdf},
title = {{Expectation propagation: a probabilistic view of Deep Feed Forward Networks}},
url = {http://arxiv.org/abs/1805.08786},
year = {2018}
}
@article{Kingma2013,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
number = {Ml},
pages = {1--14},
pmid = {23459267},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@article{Zhang2017d,
abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully used in various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
archivePrefix = {arXiv},
arxivId = {1711.05597},
author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
doi = {10.1111/j.1365-2907.2011.00193.x},
eprint = {1711.05597},
isbn = {03051838},
issn = {03051838},
title = {{Advances in Variational Inference}},
year = {2017}
}
@article{Houthooft2016,
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1605.09674},
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
eprint = {1605.09674},
title = {{VIME: Variational Information Maximizing Exploration}},
year = {2016}
}
@article{Wu2018a,
abstract = {Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider two case studies, one of learning image transformations---edge detection, colorization, segmentation---as a set of modalities, followed by one of machine translation between two languages. We find appealing results across this range of tasks.},
archivePrefix = {arXiv},
arxivId = {1802.05335},
author = {Wu, Mike and Goodman, Noah},
eprint = {1802.05335},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Goodman - 2018 - Multimodal Generative Models for Scalable Weakly-Supervised Learning.pdf:pdf},
month = {feb},
number = {Nips},
title = {{Multimodal Generative Models for Scalable Weakly-Supervised Learning}},
url = {http://arxiv.org/abs/1802.05335},
year = {2018}
}
@article{Kuzin2018,
abstract = {Gaussian process regression is a machine learning approach which has been shown its power for estimation of unknown functions. However, Gaussian processes suffer from high computational complexity, as in a basic form they scale cubically with the number of observations. Several approaches based on inducing points were proposed to handle this problem in a static context. These methods though face challenges with real-time tasks and when the data is received sequentially over time. In this paper, a novel online algorithm for training sparse Gaussian process models is presented. It treats the mean and hyperparameters of the Gaussian process as the state and parameters of the ensemble Kalman filter, respectively. The online evaluation of the parameters and the state is performed on new upcoming samples of data. This procedure iteratively improves the accuracy of parameter estimates. The ensemble Kalman filter reduces the computational complexity required to obtain predictions with Gaussian processes preserving the accuracy level of these predictions. The performance of the proposed method is demonstrated on the synthetic dataset and real large dataset of UK house prices.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.03369v1},
author = {Kuzin, Danil and Yang, Le and Isupova, Olga and Mihaylova, Lyudmila},
doi = {10.23919/ICIF.2018.8455785},
eprint = {arXiv:1807.03369v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kuzin et al. - 2018 - Ensemble Kalman Filtering for Online Gaussian Process Regression and Learning.pdf:pdf},
isbn = {9780996452762},
journal = {2018 21st International Conference on Information Fusion, FUSION 2018},
pages = {39--46},
title = {{Ensemble Kalman Filtering for Online Gaussian Process Regression and Learning}},
year = {2018}
}
@article{VanGael2008,
abstract = {The infinite hidden Markov model is a non-parametric extension of the widely used hidden Markov model. Our paper introduces a new inference algorithm for the inﬁnite Hidden Markov model called beam sampling. Beam sampling combines slice sampling, which limits the number of states considered at each time step to a ﬁnite number, with dynamic programming, which samples whole state trajectories eﬃciently. Our algorithm typically outperforms the Gibbs sampler and is more robust. We present applications of iHMM inference using the beam sampler on changepoint detection and text prediction problems.},
author = {{Van Gael}, Jurgen and Saatci, Yunus and Teh, Yee Whye and Ghahramani, Zoubin},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Van Gael et al. - 2008 - Beam sampling for the infinite hidden Markov model.pdf:pdf},
isbn = {9781605582054},
journal = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1088--1095},
title = {{Beam sampling for the infinite hidden Markov model}},
year = {2008}
}
@article{Friston1994,
abstract = {Current approaches to detecting significantly activated regions of cerebral tissue use statistical parametric maps, which are thresholded to render the probability of one or more activated regions of one voxel, or larger, suitably small (e. g., 0.05). We present an approximate analysis giving the probability that one or more activated regions of a specified volume, or larger, could have occurred by chance. These results mean that detecting significant activations no longer depends on a fixed (and high) threshold, but can be effected at any (lower) threshold, in terms of the spatial extent of the activated region. The substantial improvement in sensitivity that ensues is illustrated using a power analysis and a simulated phantom activation study. {\textcopyright} 1994 Wiley-Liss, Inc.},
author = {Friston, K. J. and Worsley, K. J. and Frackowiak, R. S.J. and Mazziotta, J. C. and Evans, A. C.},
doi = {10.1002/hbm.460010306},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Friston et al. - 1994 - Assessing the significance of focal activations using their spatial extent.pdf:pdf},
isbn = {1097-0193},
issn = {10970193},
journal = {Human Brain Mapping},
keywords = {activation,excursion set,functional imaging,gaussian fields,statistical parametric mapping,thresholds},
number = {3},
pages = {210--220},
pmid = {24578041},
title = {{Assessing the significance of focal activations using their spatial extent}},
volume = {1},
year = {1994}
}
@article{Omar,
author = {Omar, Cyrus and Aldrich, Jonathan and Gerkin, Richard C},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Omar, Aldrich, Gerkin - Unknown - Collaborative Infrastructure for Test-Driven Scientific Model Validation.pdf:pdf},
isbn = {9781450327688},
keywords = {cyberinfrastructure,model validation,unit testing},
title = {{Collaborative Infrastructure for Test-Driven Scientific Model Validation}}
}
@article{Bogacz2006,
abstract = {In this article, the authors consider optimal decision making in two-alternative forced-choice (TAFC) tasks. They begin by analyzing 6 models of TAFC decision making and show that all but one can be reduced to the drift diffusion model, implementing the statistically optimal algorithm (most accurate for a given speed or fastest for a given accuracy). They prove further that there is always an optimal trade-off between speed and accuracy that maximizes various reward functions, including reward rate (percentage of correct responses per unit time), as well as several other objective functions, including ones weighted for accuracy. They use these findings to address empirical data and make novel predictions about performance under optimality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bogacz, Rafal and Brown, Eric and Moehlis, Jeff and Holmes, Philip and Cohen, Jonathan D.},
doi = {10.1037/0033-295X.113.4.700},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bogacz et al. - 2006 - The physics of optimal decision making A formal analysis of models of performance in two-alternative forced-choic.pdf:pdf},
isbn = {0033-295X},
issn = {0033295X},
journal = {Psychological Review},
keywords = {Drift diffusion model,Optimal performance,Perceptual choice,Reward rate,Speed-accuracy trade-off},
number = {4},
pages = {700--765},
pmid = {17014301},
title = {{The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks}},
volume = {113},
year = {2006}
}
@article{DeMerlis2014,
abstract = {Polyvinyl acetate phthalate (PVAP) was evaluated in a developmental toxicity study with Crl:CD(SD) rats. Female rats were provided continual access to the formulated diets on days 6 through 20 of presumed gestation (DGs 6 through 20) at concentrations of 0{\%}, 0.75{\%}, 1.5{\%} and 3{\%}. All surviving rats were sacrificed and Caesarean-sectioned on DG 21. The following parameters were evaluated: viability, clinical observations, body weights, feed consumption, necropsy observations, Caesarean-sectioning and litter observations, including gravid uterine weights, fetal body weights and sex, and fetal gross external, soft tissue and skeletal alterations. There were no treatment-related adverse effects reported in the developmental toxicity study. The maternal and developmental no-observable-adverse-effect level (NOAEL) of PVAP was the highest concentration administered, i.e., 3.0{\%} (equivalent to 2324. mg. PVAP/kg/day). {\textcopyright} 2014 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {1802.07810v2},
author = {DeMerlis, C. C. and Schoneker, D. R. and Borzelleca, J. F.},
doi = {10.1016/j.yrtph.2014.07.021},
eprint = {1802.07810v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/DeMerlis, Schoneker, Borzelleca - 2014 - Oral dietary developmental toxicity study with polyvinyl acetate phthalate (PVAP) in the rat.pdf:pdf},
issn = {10960295},
journal = {Regulatory Toxicology and Pharmacology},
keywords = {Developmental study,Enteric coating,Enteric polymer,Modified release coating,PVAP,PVAP-T,Pharmaceutical excipient,Polyvinyl acetate phthalate,Polyvinyl acetate phthalate and titanium dioxide},
number = {1},
pages = {325--332},
pmid = {25084367},
title = {{Oral dietary developmental toxicity study with polyvinyl acetate phthalate (PVAP) in the rat}},
volume = {70},
year = {2014}
}
@book{Iozzi,
author = {Iozzi, A.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Iozzi - Unknown - Multi-Linear Algebra and Applications.pdf:pdf},
keywords = {calculus in,mathematical physics with applications,ti-linear algebra and tensor,to special},
number = {2005},
pages = {2005--2006},
title = {{Multi-Linear Algebra and Applications}}
}
@article{Lawhern2016,
abstract = {Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the distinct characteristics of its expected EEG control signal, limiting its application to that specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision and speech recognition, have successfully been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms, while simultaneously being as compact as possible. In this work we introduce EEGNet, a compact convolutional network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). We show that EEGNet generalizes across paradigms better than the reference algorithms when only limited training data is available. We demonstrate three different approaches to visualize the contents of a trained EEGNet model to enable interpretation of the learned features. Our results suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range of BCI tasks, suggesting that the observed performances were not due to artifact or noise sources in the data.},
archivePrefix = {arXiv},
arxivId = {1611.08024},
author = {Lawhern, Vernon J. and Solon, Amelia J. and Waytowich, Nicholas R. and Gordon, Stephen M. and Hung, Chou P. and Lance, Brent J.},
doi = {10.1371/journal.pone.0138297},
eprint = {1611.08024},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lawhern et al. - 2016 - EEGNet A Compact Convolutional Network for EEG-based Brain-Computer Interfaces.pdf:pdf},
isbn = {1932-6203},
issn = {1932-6203},
pages = {1--30},
pmid = {26379232},
title = {{EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces}},
url = {http://arxiv.org/abs/1611.08024},
year = {2016}
}
@unpublished{Grigoryan2008,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Grigoryan, Alexander},
doi = {10.1007/978-1-4612-0779-5},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Grigoryan - 2008 - Measure Theory and Probability.pdf:pdf},
isbn = {978-1-4612-6899-4},
issn = {0023-5954},
number = {February},
pmid = {25246403},
title = {{Measure Theory and Probability}},
url = {http://link.springer.com/10.1007/978-1-4612-0779-5},
year = {2008}
}
@article{Rasmussen1996,
author = {Rasmussen, Carl Edward},
doi = {10.1.1.17.729},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rasmussen - 1996 - Evaluation of Gaussian processes and other methods for non-linear regression.pdf:pdf},
journal = {PhD thesis, Department of Computer Science, University of Toronto},
title = {{Evaluation of Gaussian processes and other methods for non-linear regression}},
url = {http://mlg.eng.cam.ac.uk/pub/pdf/Ras96b.pdf{\%}5Cnpapers2://publication/uuid/B41A30C1-E014-4C06-84CB-4DBC34ED92F4},
year = {1996}
}
@article{Yang2018a,
abstract = {This paper proposes an approach for online training of a sparse multi-output Gaussian process (GP) model using sequentially obtained data. The considered model combines linearly multiple latent sparse GPs to produce correlated output variables. Each latent GP has its own set of inducing points to achieve sparsity. We show that given the model hyperparameters, the posterior over the inducing points is Gaussian under Gaussian noise since they are linearly related to the model outputs. However , the inducing points from different latent GPs would become correlated, leading to a full covariance matrix cumbersome to handle. Variational inference is thus applied and an approximate regression technique is obtained, with which the posteriors over different inducing point sets can always factorize. As the model outputs are non-linearly dependent on the hyperparameters, a novel marginalized particle filer (MPF)-based algorithm is proposed for the online inference of the inducing point values and hyperparameters. The approximate regression technique is incorporated in the MPF and its distributed realization is presented. Algorithm validation using synthetic and real data is conducted, and promising results are obtained. Index Terms-Multi-output Gaussian Processes, Sparse approximation , online regression and learning, marginalized particle filter, Kullback-Leibler divergence.},
author = {Yang, Le and Wang, Ke and Mihaylova, Lyudmila S.},
doi = {10.1109/TSIPN.2018.2885925},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Wang, Mihaylova - 2018 - Online Sparse Multi-Output Gaussian Process Regression and Learning.pdf:pdf},
issn = {2373776X},
journal = {IEEE Transactions on Signal and Information Processing over Networks},
keywords = {Kullback-Leibler divergence,Multi-output Gaussian Processes,Sparse approximation,marginalized particle filter,online regression and learning},
title = {{Online Sparse Multi-Output Gaussian Process Regression and Learning}},
year = {2018}
}
@article{Ros2015a,
abstract = {In this paper we introduce a covariance framework for the analysis of single subject EEG and MEG data that takes into account observed temporal stationarity on small time scales and trial-to-trial variations. We formulate a model for the covariance matrix, which is a Kronecker product of three components that correspond to space, time and epochs/trials, and consider maximum likelihood estimation of the unknown parameter values. An iterative algorithm that finds approximations of the maximum likelihood estimates is proposed. Our covariance model is applicable in a variety of cases where spontaneous EEG or MEG acts as source of noise and realistic noise covariance estimates are needed, such as in evoked activity studies, or where the properties of spontaneous EEG or MEG are themselves the topic of interest, like in combined EEG-fMRI experiments in which the correlation between EEG and fMRI signals is investigated. We use a simulation study to assess the performance of the estimator and investigate the influence of different assumptions about the covariance factors on the estimated covariance matrix and on its components. We apply our method to real EEG and MEG data sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.2522v1},
author = {Ro{\'{s}}, Beata P. and Bijma, Fetsje and de Gunst, Mathisca C.M. and de Munck, Jan C.},
doi = {10.1016/j.neuroimage.2015.06.020},
eprint = {arXiv:1410.2522v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ro{\'{s}} et al. - 2015 - A three domain covariance framework for EEGMEG data(2).pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Covariance structure,EEG,Kronecker product structure,MEG,Maximum likelihood,fMRI},
pages = {305--315},
title = {{A three domain covariance framework for EEG/MEG data}},
volume = {119},
year = {2015}
}
@article{Lusch2017,
abstract = {Identifying coordinate transformations that make strongly nonlinear dynamics approximately linear is a central challenge in modern dynamical systems. These transformations have the potential to enable prediction, estimation, and control of nonlinear systems using standard linear theory. The Koopman operator has emerged as a leading data-driven embedding, as eigenfunctions of this operator provide intrinsic coordinates that globally linearize the dynamics. However, identifying and representing these eigenfunctions has proven to be mathematically and computationally challenging. This work leverages the power of deep learning to discover representations of Koopman eigenfunctions from trajectory data of dynamical systems. Our network is parsimonious and interpretable by construction, embedding the dynamics on a low-dimensional manifold that is of the intrinsic rank of the dynamics and parameterized by the Koopman eigenfunctions. In particular, we identify nonlinear coordinates on which the dynamics are globally linear using a modified auto-encoder. We also generalize Koopman representations to include a ubiquitous class of systems that exhibit continuous spectra, ranging from the simple pendulum to nonlinear optics and broadband turbulence. Our framework parametrizes the continuous frequency using an auxiliary network, enabling a compact and efficient embedding at the intrinsic rank, while connecting our models to half a century of asymptotics. In this way, we benefit from the power and generality of deep learning, while retaining the physical interpretability of Koopman embeddings.},
archivePrefix = {arXiv},
arxivId = {1712.09707},
author = {Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
doi = {10.1073/pnas.XXXXXXXXXX},
eprint = {1712.09707},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lusch, Kutz, Brunton - 2017 - Deep learning for universal linear embeddings of nonlinear dynamics.pdf:pdf},
isbn = {1216578109},
issn = {0027-8424},
keywords = {deep learning,dynamical systems,koopman theory,learning,machine},
pmid = {28811373},
title = {{Deep learning for universal linear embeddings of nonlinear dynamics}},
url = {http://arxiv.org/abs/1712.09707},
year = {2017}
}
@techreport{Oh,
abstract = {This paper focuses on Bayesian Optimization (BO) for objectives on combinatorial search spaces, including ordinal and categorical variables. Despite the abundance of potential applications of Combinatorial BO, including chipset configuration search and neural architecture search, only a handful of methods have been proposed. We introduce COMBO, a new Gaussian Process (GP) BO. COMBO quantifies "smoothness" of functions on combinatorial search spaces by utilizing a combinatorial graph. The vertex set of the combinatorial graph consists of all possible joint assignments of the variables, while edges are constructed using the graph Cartesian product of the sub-graphs that represent the individual variables. On this combinatorial graph, we propose an ARD diffusion kernel with which the GP is able to model high-order interactions between variables leading to better performance. Moreover, using the Horseshoe prior for the scale parameter in the ARD diffusion kernel results in an effective variable selection procedure, making COMBO suitable for high dimensional problems. Computationally, in COMBO the graph Cartesian product allows the Graph Fourier Transform calculation to scale linearly instead of exponentially.We validate COMBO in a wide array of realistic benchmarks, including weighted maximum satisfiability problems and neural architecture search. COMBO outperforms consistently the latest state-of-the-art while maintaining computational and statistical efficiency.},
archivePrefix = {arXiv},
arxivId = {1902.00448v2},
author = {Oh, Changyong and Tomczak, Jakub M and Gavves, Efstratios and Welling, Max},
eprint = {1902.00448v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Oh et al. - Unknown - Combinatorial Bayesian Optimization using the Graph Cartesian Product.pdf:pdf},
title = {{Combinatorial Bayesian Optimization using the Graph Cartesian Product}}
}
@article{Molchanov2017,
abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
archivePrefix = {arXiv},
arxivId = {1701.05369},
author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
doi = {10.1021/ct2009208},
eprint = {1701.05369},
isbn = {9781937284275},
issn = {1938-7228},
pmid = {25246403},
title = {{Variational Dropout Sparsifies Deep Neural Networks}},
year = {2017}
}
@article{Kulkarni2016,
abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.},
archivePrefix = {arXiv},
arxivId = {1604.06057},
author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
doi = {10.1023/A:1025696116075},
eprint = {1604.06057},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kulkarni et al. - 2016 - Hierarchical Deep Reinforcement Learning Integrating Temporal Abstraction and Intrinsic Motivation.pdf:pdf},
isbn = {0924-6703},
issn = {1573-7594},
number = {Nips},
title = {{Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation}},
url = {http://arxiv.org/abs/1604.06057},
year = {2016}
}
@article{Mihali2017,
abstract = {Microsaccades are high-velocity fixational eye movements, with special roles in perception and cognition. The default microsaccade detection method is to determine when the smoothed eye velocity exceeds a threshold. We have developed a new method, Bayesian microsaccade detection (BMD), which performs inference based on a simple statistical model of eye positions. In this model, a hidden state variable changes between drift and microsaccade states at random times. The eye position is a biased random walk with different velocity distributions for each state. BMD generates samples from the posterior probability distribution over the eye state time series given the eye position time series. Applied to simulated data, BMD recovers the “true” microsaccades with fewer errors than alternative algorithms, especially at high noise. Applied to EyeLink eye tracker data, BMD detects almost all the microsaccades detected by the default method, but also apparent microsaccades embedded in high noise—although these can also be interpreted as false positives. Next we apply the algorithms to data collected with a Dual Purkinje Image eye tracker, whose higher precision justifies defining the inferred microsaccades as ground truth. When we add artificial measurement noise, the inferences of all algorithms degrade; however, at noise levels comparable to EyeLink data, BMD recovers the “true” microsaccades with 54{\%} fewer errors than the default algorithm. Though unsuitable for online detection, BMD has other advantages: It returns probabilities rather than binary judgments, and it can be straightforwardly adapted as the generative model is refined. We make our algorithm available as a software package.},
author = {Mihali, Andra and van Opheusden, Bas and Ma, Wei Ji},
doi = {10.1167/17.1.13},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mihali, van Opheusden, Ma - 2017 - Bayesian microsaccade detection.pdf:pdf},
issn = {1534-7362},
journal = {Journal of Vision},
number = {1},
pages = {13},
pmid = {28114483},
title = {{Bayesian microsaccade detection}},
url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/17.1.13},
volume = {17},
year = {2017}
}
@article{Sutin2016,
abstract = {Physiological monitoring of oxygen delivery to the brain has great significance for improving the management of patients at risk for brain injury. Diffuse correlation spectroscopy (DCS) is a rapidly growing optical technology able to non-invasively assess the blood flow index (BFi) at the bedside. The current limitations of DCS are the contami- nation introduced by extracerebral tissue and the need to know the tissue's optical properties to correctly quantify the BFi. To overcome these limitations, we have developed a new technology for time-resolved diffuse correlation spectros- copy. By operatingDCS in the time domain (TD-DCS),we are able to simultaneously acquire the temporal point-spread function to quantify tissue optical properties and the autocorrelation function to quantify the BFi.More importantly, by applying time-gated strategies to theDCS autocorrelation functions, we are able to differentiate between short and long photon paths through the tissue and determine the BFi for different depths. Here, we present the novel device and we report the first experiments in tissue-like phantoms and in rodents. The TD-DCS method opens many possibilities for improved non-invasive monitoring of oxygen delivery in humans.},
author = {Sutin, Jason and Zimmerman, Bernhard and Tyulmankov, Danil and Tamborini, Davide and Wu, Kuan Cheng and Selb, Juliette and Gulinatti, Angelo and Rech, Ivan and Tosi, Alberto and Boas, David A. and Franceschini, Maria Angela},
doi = {10.1364/OPTICA.3.001006},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sutin et al. - 2016 - Time-domain diffuse correlation spectroscopy.pdf:pdf},
issn = {2334-2536},
journal = {Optica},
number = {9},
pages = {1006},
title = {{Time-domain diffuse correlation spectroscopy}},
url = {https://www.osapublishing.org/abstract.cfm?URI=optica-3-9-1006},
volume = {3},
year = {2016}
}
@article{Wilson2016,
abstract = {Copyright {\textcopyright} 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Much existing work examining the ethical behaviors of robots does not consider the impact and effects of longterm human-robot interactions. A robot teammate, collaborator or helper is often expected to increase task performance, individually or of the team, but little discussion is usually devoted to how such a robot should balance the task requirements with building and maintaining a "working relationship" with a human partner, much less appropriate social relations outside that team. We propose the "Relational Enhancement" framework for the design and evaluation of long-term interactions, which composed of interrelated concepts of efficiency, solidarity, and prosocial concern. We discuss how this framework can be used to evaluate common existing approaches in cognitive architectures for robots and then examine how social norms and mental simulation may contribute to each of the components of the framework.},
author = {Wilson, J.R. and Arnold, T. and Scheutz, M.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilson, Arnold, Scheutz - 2016 - Relational enhancement A framework for evaluating and designing human-robot relationships.pdf:pdf},
isbn = {9781577357599},
journal = {AAAI Workshop - Technical Report},
keywords = {AI, Ethics, and Society: Technical Report WS-16-02},
pages = {132--142},
title = {{Relational enhancement: A framework for evaluating and designing human-robot relationships}},
volume = {WS-16-01 -},
year = {2016}
}
@inproceedings{Pandey2017,
abstract = {In this paper, we address the problem of conditional modality learning, whereby one is interested in generating one modality given the other. While it is straightforward to learn a joint distribution over multiple modalities using a deep multimodal architecture, we observe that such models aren't very effective at conditional generation. Hence, we address the problem by learning conditional distributions between the modalities. We use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from a single modality alone to be `close' to the joint representation obtained from multiple modalities. We use the proposed model to generate faces from attributes. We show that the faces generated from attributes using the proposed model, are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.01801v2},
author = {Pandey, Gaurav and Dukkipati, Ambedkar},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2017.7965870},
eprint = {arXiv:1603.01801v2},
isbn = {9781509061815},
title = {{Variational methods for conditional multimodal deep learning}},
volume = {2017-May},
year = {2017}
}
@article{Wu2010,
abstract = {In numerous neuroscience studies, multichannel EEG data are often recorded over multiple trial periods under the same experimental condition. To date, little effort is aimed to learn spatial patterns from EEG data to account for trial-to-trial variability. In this paper, a hierarchical Bayesian framework is introduced to model inter-trial source variability while extracting common spatial patterns under multiple experimental conditions in a supervised manner. We also present a variational Bayesian algorithm for model inference, by which the number of sources can be determined effectively via automatic relevance determination (ARD). The efficacy of the proposed learning algorithm is validated with both synthetic and real EEG data. Using two brain-computer interface (BCI) motor imagery data sets we show the proposed algorithm consistently outperforms the common spatial patterns (CSP) algorithm while attaining comparable performance with a recently proposed discriminative approach.},
author = {Wu, Wei and Chen, Zhe and Gao, Shangkai and Brown, Emery N.},
doi = {10.1109/ICASSP.2010.5495663},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2010 - Hierarchical bayesian modeling of inter-trial variability and variational Bayesian learning of common spatial pattern.pdf:pdf},
isbn = {9781424442966},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
pages = {501--504},
title = {{Hierarchical bayesian modeling of inter-trial variability and variational Bayesian learning of common spatial patterns from multichannel EEG}},
year = {2010}
}
@article{Kim2019,
abstract = {Recurrent neural network grammars (RNNG) are generative models of language which jointly model syntax and surface structure by incrementally generating a syntax tree and sentence in a top-down, left-to-right order. Supervised RNNGs achieve strong language modeling and parsing performance, but require an annotated corpus of parse trees. In this work, we experiment with unsupervised learning of RNNGs. Since directly marginalizing over the space of latent trees is intractable, we instead apply amortized variational inference. To maximize the evidence lower bound, we develop an inference network parameterized as a neural CRF constituency parser. On language modeling, unsupervised RNNGs perform as well their supervised counterparts on benchmarks in English and Chinese. On constituency grammar induction, they are competitive with recent neural language models that induce tree structures from words through attention mechanisms.},
author = {Kim, Yoon and Rush, Alexander and Yu, Lei and Kuncoro, Adhiguna and Dyer, Chris and Melis, G{\'{a}}bor},
doi = {10.18653/v1/n19-1114},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2019 - Unsupervised Recurrent Neural Network Grammars.pdf:pdf},
pages = {1105--1117},
title = {{Unsupervised Recurrent Neural Network Grammars}},
year = {2019}
}
@misc{Vehtari2016,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
archivePrefix = {arXiv},
arxivId = {1507.04544},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
booktitle = {Statistics and Computing},
doi = {10.1007/s11222-016-9696-4},
eprint = {1507.04544},
isbn = {1507.04544},
issn = {15731375},
title = {{Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
year = {2016}
}
@article{Collins2018,
abstract = {Relationships between neuroimaging measures and behavior provide important clues about brain function and cognition in healthy and clinical populations. While electroencephalography (EEG) provides a portable, low cost measure of brain dynamics, it has been somewhat underrepresented in the emerging field of model-based inference. We seek to address this gap in this article by highlighting the utility of linking EEG and behavior, with an emphasis on approaches for EEG analysis that move beyond focusing on peaks or "components" derived from averaging EEG responses across trials and subjects (generating the event-related potential, ERP). First, we review methods for deriving features from EEG in order to enhance the signal within single-trials. These methods include filtering based on user-defined features (i.e., frequency decomposition, time-frequency decomposition), filtering based on data-driven properties (i.e., blind source separation, BSS), and generating more abstract representations of data (e.g., using deep learning). We then review cognitive models which extract latent variables from experimental tasks, including the drift diffusion model (DDM) and reinforcement learning (RL) approaches. Next, we discuss ways to access associations among these measures, including statistical models, data-driven joint models and cognitive joint modeling using hierarchical Bayesian models (HBMs). We think that these methodological tools are likely to contribute to theoretical advancements, and will help inform our understandings of brain dynamics that contribute to moment-to-moment cognitive function.},
author = {Collins, Anne G. E. and Bridwell, David A. and Srinivasan, Ramesh and Nunez, Michael D. and Stober, Sebastian and Calhoun, Vince D. and Cavanagh, James F.},
doi = {10.3389/fnhum.2018.00106},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Collins et al. - 2018 - Moving Beyond ERP Components A Selective Review of Approaches to Integrate EEG and Behavior.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {EEG, ERP, blind source separation, partial least s,blind source separation,canonical correlations analysis,deep learning,eeg,erp,hierarchical bayesian model,partial least squares,representational similarity analysis},
number = {March},
pages = {1--17},
pmid = {29632480},
title = {{Moving Beyond ERP Components: A Selective Review of Approaches to Integrate EEG and Behavior}},
volume = {12},
year = {2018}
}
@article{Bui2014a,
author = {Bui, Thang and Turner, Richard},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bui, Turner - 2014 - Sparse Approximations for Non-Conjugate Gaussian Process Regression Review of GPs for regression.pdf:pdf},
pages = {1--7},
title = {{Sparse Approximations for Non-Conjugate Gaussian Process Regression Review of GPs for regression}},
year = {2014}
}
@article{Ulrich2015,
abstract = {Multi-output Gaussian processes provide a convenient framework for multi-task problems. An illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data, where experimentalists are in-terested in both power and phase coherence between channels. Recently, Wilson and Adams (2013) proposed the spectral mixture (SM) kernel to model the spec-tral density of a single task in a Gaussian process framework. In this paper, we develop a novel covariance kernel for multiple outputs, called the cross-spectral mixture (CSM) kernel. This new, flexible kernel represents both the power and phase relationship between multiple observation channels. We demonstrate the expressive capabilities of the CSM kernel through implementation of a Bayesian hidden Markov model, where the emission distribution is a multi-output Gaus-sian process with a CSM covariance kernel. Results are presented for measured multi-region electrophysiological data.},
author = {Ulrich, Kyle R. and Carlson, David E. and Dzirasa, Kafui and Carin, Lawrence},
doi = {10.2307/1913455?ref=search-gateway:ab2880c18b5ababa9ad9b12afb065215},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ulrich et al. - 2015 - GP Kernels for Cross-Spectrum Analysis.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1990--1998},
title = {{GP Kernels for Cross-Spectrum Analysis}},
url = {http://papers.nips.cc/paper/5966-gp-kernels-for-cross-spectrum-analysis},
year = {2015}
}
@inproceedings{Ngeo2014,
abstract = {—Surface electromyographic (EMG) signals have of- ten been used in estimating upper and lower limb dynamics and kinematics for the purpose of controlling robotic devices such as robot prosthesis and finger exoskeletons. However, in estimating multiple and a high number of degrees-of-freedom (DOF) kinematics from EMG, output DOFs are usually estimated independently. In this study, we estimate finger joint kinematics from EMG signals using a multi-output convolved Gaussian Process (Multi-output Full GP) that considers dependencies between outputs. We show that estimation of finger joints from muscle activation inputs can be improved by using a regression model that considers inherent coupling or correlation within the hand and finger joints. We also provide a comparison of estimation performance between different regression methods, such as Artificial Neural Networks (ANN) which is used by many of the related studies. We show that using a multi-output GP gives improved estimation compared to multi-output ANN and},
author = {Ngeo, Jimson and Tamei, Tomoya and Shibata, Tomohiro},
booktitle = {2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
doi = {10.1109/EMBC.2014.6944386},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ngeo, Tamei, Shibata - 2014 - Estimation of continuous multi-DOF finger joint kinematics from surface EMG using a multi-output Gaussian.pdf:pdf},
isbn = {978-1-4244-7929-0},
month = {aug},
pages = {3537--3540},
publisher = {IEEE},
title = {{Estimation of continuous multi-DOF finger joint kinematics from surface EMG using a multi-output Gaussian Process}},
url = {http://ieeexplore.ieee.org/document/6944386/},
year = {2014}
}
@article{Niso2018,
abstract = {We present a significant extension of the Brain Imaging Data Structure (BIDS) to support the specific aspects of magnetoencephalography (MEG) data. MEG measures brain activity with millisecond temporal resolution and unique source imaging capabilities. So far, BIDS was a solution to organise magnetic resonance imaging (MRI) data. The nature and acquisition parameters of MRI and MEG data are strongly dissimilar. Although there is no standard data format for MEG, we propose MEG-BIDS as a principled solution to store, organise, process and share the multidimensional data volumes produced by the modality. The standard also includes well-defined metadata, to facilitate future data harmonisation and sharing efforts. This responds to unmet needs from the multimodal neuroimaging community and paves the way to further integration of other techniques in electrophysiology. MEG-BIDS builds on MRI-BIDS, extending BIDS to a multimodal data structure. We feature several data-analytics software that have adopted MEG-BIDS, and a diverse sample of open MEG-BIDS data resources available to everyone.},
author = {Niso, Guiomar and Gorgolewski, Krzysztof J. and Bock, Elizabeth and Brooks, Teon L. and Flandin, Guillaume and Gramfort, Alexandre and Henson, Richard N. and Jas, Mainak and Litvak, Vladimir and Moreau, Jeremy T. and Oostenveld, Robert and Schoffelen, Jan Mathijs and Tadel, Francois and Wexler, Joseph and Baillet, Sylvain},
doi = {10.1038/sdata.2018.110},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Niso et al. - 2018 - MEG-BIDS, the brain imaging data structure extended to magnetoencephalography.pdf:pdf},
issn = {20524463},
journal = {Scientific Data},
pages = {1--5},
title = {{MEG-BIDS, the brain imaging data structure extended to magnetoencephalography}},
volume = {5},
year = {2018}
}
@article{Frigola-alcalde2015,
author = {Frigola-alcalde, Roger},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Frigola-alcalde - 2015 - Bayesian Time Series Learning with Gaussian Processes.pdf:pdf},
number = {August},
title = {{Bayesian Time Series Learning with Gaussian Processes}},
year = {2015}
}
@article{Hernandez-Lobato2015,
abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
archivePrefix = {arXiv},
arxivId = {1502.05336},
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Adams, Ryan P.},
eprint = {1502.05336},
isbn = {9781510810587},
issn = {1938-7228},
title = {{Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks}},
year = {2015}
}
@article{Wu2016,
abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural network architectures in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm --- the parallel knowledge gradient method. By construction, this method provides the one-step Bayes-optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.},
archivePrefix = {arXiv},
arxivId = {1606.04414},
author = {Wu, Jian and Frazier, Peter I.},
eprint = {1606.04414},
issn = {10495258},
title = {{The Parallel Knowledge Gradient Method for Batch Bayesian Optimization}},
year = {2016}
}
@inproceedings{Garriga-Alonso2019,
abstract = {We show that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike “deep kernels”, has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84{\%} classification error on MNIST, a new record for GPs with a comparable number of parameters.},
archivePrefix = {arXiv},
arxivId = {1808.05587},
author = {Garriga-Alonso, Adri{\`{a}} and Aitchison, Laurence and Rasmussen, Carl Edward},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1808.05587},
title = {{Deep convolutional networks as shallow Gaussian processes}},
year = {2019}
}
@article{Yao2018,
abstract = {While it's always possible to compute a variational approximation to a posterior distribution, it can be difficult to discover problems with this approximation. We propose two diagnostic algorithms to alleviate this problem. The Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of fit measurement for joint distributions, while simultaneously improving the error in the estimate. The variational simulation-based calibration (VSBC) assesses the average performance of point estimates.},
archivePrefix = {arXiv},
arxivId = {1802.02538},
author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
eprint = {1802.02538},
title = {{Yes, but Did It Work?: Evaluating Variational Inference}},
year = {2018}
}
@article{Sun2018,
abstract = {The generalization properties of Gaussian processes depend heavily on the choice of kernel, and this choice remains a dark art. We present the Neural Kernel Network (NKN), a flexible family of kernels represented by a neural network. The NKN architecture is based on the composition rules for kernels, so that each unit of the network corresponds to a valid kernel. It can compactly approximate compositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient-based optimization. We show that the NKN is universal for the class of stationary kernels. Empirically we demonstrate pattern discovery and extrapolation abilities of NKN on several tasks that depend crucially on identifying the underlying structure, including time series and texture extrapolation, as well as Bayesian optimization.},
archivePrefix = {arXiv},
arxivId = {1806.04326},
author = {Sun, Shengyang and Zhang, Guodong and Wang, Chaoqi and Zeng, Wenyuan and Li, Jiaman and Grosse, Roger},
doi = {10.1109/IRPS.2014.6860679},
eprint = {1806.04326},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2018 - Differentiable Compositional Kernel Learning for Gaussian Processes.pdf:pdf},
isbn = {9781479933167},
issn = {1938-7228},
title = {{Differentiable Compositional Kernel Learning for Gaussian Processes}},
year = {2018}
}
@article{Suris2019,
abstract = {When we travel, we often encounter new scenarios we have never experienced before, with new sights and new words that describe them. We can use our language-learning ability to quickly learn these new words and correlate them with the visual world. In contrast, language models often do not robustly generalize to novel words and compositions. We propose a framework that learns how to learn text representations from visual context. Experiments show that our approach significantly outperforms the state-of-the-art in visual language modeling for acquiring new words and predicting new compositions. Model ablations and visualizations suggest that the visual modality helps our approach more robustly generalize at these tasks. Project webpage is available at https://expert.cs.columbia.edu/},
archivePrefix = {arXiv},
arxivId = {1911.11237},
author = {Sur{\'{i}}s, D{\'{i}}dac and Epstein, Dave and Ji, Heng and Chang, Shih-Fu and Vondrick, Carl},
eprint = {1911.11237},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sur{\'{i}}s et al. - 2019 - Learning to Learn Words from Narrated Video.pdf:pdf},
title = {{Learning to Learn Words from Narrated Video}},
url = {http://arxiv.org/abs/1911.11237},
year = {2019}
}
@inproceedings{Wilson2013,
abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that it is possible to reconstruct several popular standard covariances within our framework.},
address = {Atlanta, Georgia, USA},
archivePrefix = {arXiv},
arxivId = {1302.4245},
author = {Wilson, Andrew Gordon and Adams, Ryan},
booktitle = {Proceedings of the 30th International Conference on Machine Learning},
editor = {Dasgupta, Sanjoy and McAllester, David},
eprint = {1302.4245},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilson, Adams - 2013 - Gaussian Process Kernels for Pattern Discovery and Extrapolation.pdf:pdf},
month = {may},
number = {3},
pages = {1067--1075},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Gaussian Process Kernels for Pattern Discovery and Extrapolation}},
url = {http://proceedings.mlr.press/v28/wilson13.html http://arxiv.org/abs/1302.4245},
volume = {28},
year = {2013}
}
@article{Matthews2014,
abstract = {This paper details a novel probabilistic method for automatic neural spike sorting which uses stochastic point process models of neural spike trains and parameterized action potential waveforms. A novel likelihood model for
observed firing times as the aggregation of hidden neural spike trains is derived, as well as an iterative procedure for clustering the data and finding the parameters that maximize the likelihood. The method is executed and evaluated on both a fully labeled semiartificial dataset and a partially labeled real dataset of extracellular electric traces from rat hippocampus. In conditions of relatively high difficulty (i.e., with additive noise and with similar action potential waveform shapes for distinct neurons) the method achieves significant improvements in clustering performance over a baseline waveform-only Gaussian mixture model (GMM) clustering on the semiartificial set (1.98{\&}{\#}x25; reduction in error rate) and outperforms both the GMM and a state-of-the-art method on the real dataset (5.04{\&}{\#}x25; reduction in false positive + false negative errors). Finally, an empirical study of two free parameters for our method is performed on the semiartificial dataset.},
author = {Matthews, Brett A and Clements, Mark A},
doi = {10.1155/2014/643059},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Matthews, Clements - 2014 - Spike Sorting by Joint Probabilistic Modeling of Neural Spike Trains and Waveforms.pdf:pdf},
publisher = {Hindawi Publishing Corporation},
title = {{Spike Sorting by Joint Probabilistic Modeling of Neural Spike Trains and Waveforms}},
volume = {2014},
year = {2014}
}
@article{Brand1997,
abstract = {We address the problem of visually detecting causal events and fitting them together into a coherent story of the action witnessed by the camera. We show that this can be done by reasoning about the motions and collisions of surfaces, using high-level causal constraints derived from psychological studies of infant visual behavior. These constraints are naive forms of basic physical laws governing substantiality, contiguity, momentum, and acceleration. We describe two implementations. One system parses instructional videos, extracting plans of action and key frames suitable for storyboarding. Since learning will play a role in making such systems robust, we introduce a new framework for higher-order hidden Markov models and demonstrate its use in a second system that segments stereo video into actions in near real-time. Rather than attempt accurate low-level vision, both systems use high-level causal analysis to integrate fast but sloppy pixel-based representations over time. The output is suitable for summary, indexing, and automated editing.},
author = {Brand, Matthew},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Brand - 1997 - The Inverse Hollywood Problem From video to scripts and storyboards via causal analysis.pdf:pdf},
isbn = {0-262-51095-2},
journal = {AAAI Conference on Artificial Intelligence},
keywords = {Activity recognition,HMM},
pages = {12--96},
title = {{The "Inverse Hollywood Problem": From video to scripts and storyboards via causal analysis}},
year = {1997}
}
@article{Wang2008,
abstract = {We introduce models for density estimation with multiple, hidden, continuous factors. In particular, we propose a generalization of multilinear models using nonlinear basis functions. By marginalizing over the weights, we obtain a multifactor form of the Gaussian process latent variable model. In this model, each factor is kernelized independently, allowing nonlinear mappings from any particular factor to the data. We learn models for human locomotion data, in which each pose is generated by factors representing the persons identity, gait, and the current state of motion. We demonstrate our approach using time-series prediction, and by synthesizing novel animation from the model.},
author = {Wang, Jack M. and Fleet, David J. and Hertzmann, Aaron},
doi = {10.1145/1273496.1273619},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Fleet, Hertzmann - 2008 - Multifactor Gaussian process models for style-content separation.pdf:pdf},
pages = {975--982},
title = {{Multifactor Gaussian process models for style-content separation}},
year = {2008}
}
@article{Parra2017,
abstract = {Early approaches to multiple-output Gaussian processes (MOGPs) relied on linear combinations of independent, latent, single-output Gaussian processes (GPs). This resulted in cross-covariance functions with limited parametric interpretation, thus conflicting with the ability of single-output GPs to understand lengthscales, frequencies and magnitudes to name a few. On the contrary, current approaches to MOGP are able to better interpret the relationship between different channels by directly modelling the cross-covariances as a spectral mixture kernel with a phase shift. We extend this rationale and propose a parametric family of complex-valued cross-spectral densities and then build on Cram$\backslash$'er's Theorem (the multivariate version of Bochner's Theorem) to provide a principled approach to design multivariate covariance functions. The so-constructed kernels are able to model delays among channels in addition to phase differences and are thus more expressive than previous methods, while also providing full parametric interpretation of the relationship across channels. The proposed method is first validated on synthetic data and then compared to existing MOGP methods on two real-world examples.},
archivePrefix = {arXiv},
arxivId = {1709.01298},
author = {Parra, Gabriel and Tobar, Felipe},
eprint = {1709.01298},
issn = {10495258},
title = {{Spectral Mixture Kernels for Multi-Output Gaussian Processes}},
year = {2017}
}
@article{Obermeyer2019,
abstract = {It is a significant challenge to design probabilistic programming systems that can accommodate a wide variety of inference strategies within a unified framework. Noting that the versatility of modern automatic differentiation frameworks is based in large part on the unifying concept of tensors, we describe a software abstraction --functional tensors-- that captures many of the benefits of tensors, while also being able to describe continuous probability distributions. Moreover, functional tensors are a natural candidate for generalized variable elimination and parallel-scan filtering algorithms that enable parallel exact inference for a large family of tractable modeling motifs. We demonstrate the versatility of functional tensors by integrating them into the modeling frontend and inference backend of the Pyro programming language. In experiments we show that the resulting framework enables a large variety of inference strategies, including those that mix exact and approximate inference.},
archivePrefix = {arXiv},
arxivId = {1910.10775},
author = {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Phan, Du and Chen, Jonathan P.},
eprint = {1910.10775},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Obermeyer et al. - 2019 - Functional Tensors for Probabilistic Programming.pdf:pdf},
title = {{Functional Tensors for Probabilistic Programming}},
url = {http://arxiv.org/abs/1910.10775},
year = {2019}
}
@article{Li2018,
archivePrefix = {arXiv},
arxivId = {1811.05016},
author = {Li, Shuang and Xiao, Shuai and Zhu, Shixiang and Du, Nan and Xie, Yao and Song, Le},
eprint = {1811.05016},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2018 - Learning Temporal Point Processes via Reinforcement Learning.pdf:pdf},
number = {Nips},
pages = {1--16},
title = {{Learning Temporal Point Processes via Reinforcement Learning}},
year = {2018}
}
@article{Lewis2017,
abstract = {Future robotic agents may be required to reason about a given situation and decide whether it is appropriate to lie to or deceive humans. One type of deception, known formally as strategic deception, is the act of influencing others toward a specific goal through non-truths. To demonstrate and test for the kind of reasoning required in strategic deception, we use a modified form of the social strategy game "Mafia" as a testing ground. In the game, the townsfolk, who can be seen as an uninformed majority, must determine who amongst themselves are members of the informed minority (the Mafia) via social cues before the Mafia eliminate all the townsfolk. First, we talk about how strategic deception applies to Mafia. We then present simplified rules for the game which can be formalized into a logic-based language. Once formalized, the rules can be provided to an automated theorem prover, which can carry out the necessary reasoning. By using this automated theorem prover we discuss how one can demonstrate automated strategic deception.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.05027v1},
author = {Lewis, Brad and Smith, Isaac and Fowler, Max and Licato, John},
doi = {10.1145/1235},
eprint = {arXiv:1807.05027v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lewis et al. - 2017 - The robot mafia A test environment for deceptive robots.pdf:pdf},
isbn = {9781450321389},
journal = {28th Modern Artificial Intelligence and Cognitive Science Conference, MAICS 2017},
keywords = {Robots,Social Games,Strategic Deception},
pages = {189--190},
title = {{The robot mafia: A test environment for deceptive robots}},
year = {2017}
}
@article{Rolfs2009,
abstract = {Contrary to common wisdom, fixations are a dynamically rich behavior, composed of continual, miniature eye movements, of which microsaccades are the most salient component. Over the last few years, interest in these small movements has risen dramatically, driven by both neurophysiological and psychophysical results and by advances in techniques, analysis, and modeling of eye movements. The field has a long history but a significant portion of the earlier work has gone missing in the current literature, in part, as a result of the collapse of the field in the 1980s that followed a series of discouraging results. The present review compiles 60 years of work demonstrating the unique contribution of microsaccades to visual and oculomotor function. Specifically, the review covers the contribution of microsaccades to (1) the control of fixation position, (2) the reduction of perceptual fading and the continuity of perception, (3) the generation of synchronized visual transients, (4) visual acuity, (5) scanning of small spatial regions, (6) shifts of spatial attention, (7) resolving perceptual ambiguities in the face of multistable perception, as well as several other functions. The accumulated evidence demonstrates that microsaccades serve both perceptual and oculomotor goals and although in some cases their contribution is neither necessary nor unique, microsaccades are a malleable tool conveniently employed by the visual system. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Rolfs, Martin},
doi = {10.1016/j.visres.2009.08.010},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rolfs - 2009 - Microsaccades Small steps on a long way.pdf:pdf},
isbn = {1878-5646 (Electronic)},
issn = {00426989},
journal = {Vision Research},
keywords = {Attention,Drift,Fixation control,Fixational eye movements,Microsaccade,Multistable perception,Oculomotor control,Perceptual fading,Tremor,Visual acuity,Visual perception,Visual search,Visual transients},
number = {20},
pages = {2415--2441},
pmid = {19683016},
publisher = {Elsevier Ltd},
title = {{Microsaccades: Small steps on a long way}},
url = {http://dx.doi.org/10.1016/j.visres.2009.08.010},
volume = {49},
year = {2009}
}
@article{Liu1994,
abstract = {his paper proposes a new procedure for quickest detection of an abrupt change in a random sequence, where a change is known to occur with probability one. Applications include on-line speech segmentation, edge detection in image processing, and communications channel monitoring. In con- trast to Shiryayev's problem formulation, prior knowledge of a change-time distribution is not required. The optimality crite- rion considered is similar to Shiryayev's, except that the ex- pected delay is to be minimized subject to both overall false alarm probability and false alarm average run length con- straints. Under this criterion, theoretical study shows that the new procedure approximates an optimal but unrealizable Bayesian procedure, particularly for small signal changes or for low probability of false alarm. Simulations confirm that under such conditions, the new procedure compares favorably with Page's CUSUM, an optimized moving-window fixed-sample-size (FSS) procedure, and a special case of the Girshick-Rubin- Shiryayev (GRS) procedure.},
archivePrefix = {arXiv},
arxivId = {1401.6044},
author = {Liu, Yong and Blostein, Steven D.},
doi = {10.1109/18.340471},
eprint = {1401.6044},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Blostein - 1994 - Quickest Detection of an Abrupt Change in a Random Sequence with Finite Change-Time.pdf:pdf},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
keywords = {Sequential detection,cumulative sum tests,signal change detection},
number = {6},
pages = {1985--1993},
title = {{Quickest Detection of an Abrupt Change in a Random Sequence with Finite Change-Time}},
volume = {40},
year = {1994}
}
@article{Geyer2011,
author = {Geyer, Charles},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Geyer - 2011 - Introduction to MCMC methods.pdf:pdf},
journal = {Handbook of Markov Chain Monte Carlo},
number = {March},
pages = {C},
title = {{Introduction to MCMC methods}},
year = {2011}
}
@article{Watson1983,
abstract = {QUEST is a Bayesian adaptive psychometric testing method that allows an arbitrary number of stimulus dimensions, psychometric function parameters, and trial outcomes. It is a generalization and extension of the original QUEST procedure and incorporates many subsequent developments in the area of parametric adaptive testing. With a single procedure, it is possible to implement a wide variety of experimental designs, including conventional threshold measurement; measurement of psychometric function parameters, such as slope and lapse; estimation of the contrast sensitivity function; measurement of increment threshold functions; measurement of noise-masking functions; Thurstone scale estimation using pair comparisons; and categorical ratings on linear and circular stimulus dimensions. QUEST provides a general method to accelerate data collection in many areas of cognitive and perceptual science.},
author = {Watson, Andrew B. and Pelli, Denis G.},
doi = {10.3758/BF03202828},
file = {:Users/mshvarts/Downloads/Watson-Pelli1983{\_}Article{\_}QuestABayesianAdaptivePsychome.pdf:pdf},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
month = {mar},
number = {2},
pages = {113--120},
pmid = {28355623},
title = {{Quest: A Bayesian adaptive psychometric method}},
url = {http://link.springer.com/10.3758/BF03202828},
volume = {33},
year = {1983}
}
@article{Turner2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.07182v4},
author = {Turner, Richard E},
eprint = {arXiv:1802.07182v4},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Turner - 2019 - The Gaussian Process Autoregressive Regression Model (GPAR).pdf:pdf},
title = {{The Gaussian Process Autoregressive Regression Model (GPAR)}},
volume = {89},
year = {2019}
}
@article{Lloyd2014,
abstract = {This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural-language text. Our approach treats unknown regression functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.},
archivePrefix = {arXiv},
arxivId = {1402.4304},
author = {Lloyd, James Robert and Duvenaud, David and Grosse, Roger and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
eprint = {1402.4304},
isbn = {9781577356783},
title = {{Automatic Construction and Natural-Language Description of Nonparametric Regression Models}},
year = {2014}
}
@article{Popescu-belis,
author = {Popescu-belis, Andrei},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Popescu-belis - Unknown - Managing Multimodal Data , Metadata and Annotations Challenges and Solutions.pdf:pdf},
pages = {187--199},
title = {{Managing Multimodal Data , Metadata and Annotations : Challenges and Solutions}}
}
@techreport{Wua,
abstract = {A central challenge faced by memory systems is the robust retrieval of a stored pattern in the presence of interference due to other stored patterns and noise. A theoretically well-founded solution to robust retrieval is given by attractor dynamics, which iteratively clean up patterns during recall. However, incorporating attractor dynamics into modern deep learning systems poses difficulties: attractor basins are characterised by vanishing gradients, which are known to make training neural networks difficult. In this work, we avoid the vanishing gradient problem by training a generative distributed memory without simulating the attractor dynamics. Based on the idea of memory writing as inference, as proposed in the Kanerva Machine, we show that a likelihood-based Lyapunov function emerges from maximising the variational lower-bound of a generative memory. Experiments shows it converges to correct patterns upon iterative retrieval and achieves competitive performance as both a memory model and a generative model.},
author = {Wu, Yan and Wayne, Greg and Gregor, Karol and Deepmind, Timothy Lillicrap},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - Learning Attractor Dynamics for Generative Memory.pdf:pdf},
title = {{Learning Attractor Dynamics for Generative Memory}}
}
@article{Russo2017,
abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
archivePrefix = {arXiv},
arxivId = {1707.02038},
author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
doi = {10.1561/2200000070},
eprint = {1707.02038},
isbn = {9781680833683},
issn = {1935-8237},
title = {{A Tutorial on Thompson Sampling}},
year = {2017}
}
@article{Zoltowski2020,
abstract = {An open question in systems and computational neuroscience is how neural circuits accumulate evidence towards a decision. Fitting models of decision-making theory to neural activity helps answer this question, but current approaches limit the number of these models that we can fit to neural data. Here we propose a unifying framework for modeling neural activity during decision-making tasks. The framework includes the canonical drift-diffusion model and enables extensions such as multi-dimensional accumulators, variable and collapsing boundaries, and discrete jumps. Our framework is based on constraining the parameters of recurrent state-space models, for which we introduce a scalable variational Laplace-EM inference algorithm. We applied the modeling approach to spiking responses recorded from monkey parietal cortex during two decision-making tasks. We found that a two-dimensional accumulator better captured the trial-averaged responses of a set of parietal neurons than a single accumulator model. Next, we identified a variable lower boundary in the responses of an LIP neuron during a random dot motion task.},
archivePrefix = {arXiv},
arxivId = {2001.04571},
author = {Zoltowski, David M. and Pillow, Jonathan W. and Linderman, Scott W.},
eprint = {2001.04571},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zoltowski, Pillow, Linderman - 2020 - Unifying and generalizing models of neural dynamics during decision-making.pdf:pdf},
title = {{Unifying and generalizing models of neural dynamics during decision-making}},
url = {http://arxiv.org/abs/2001.04571},
year = {2020}
}
@article{Garnelo2018,
abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
archivePrefix = {arXiv},
arxivId = {1807.01622},
author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
doi = {00130832-200512000-00011 [pii]},
eprint = {1807.01622},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Garnelo et al. - 2018 - Neural Processes.pdf:pdf},
isbn = {1528-4050 (Print)$\backslash$r1473-6322 (Linking)},
issn = {1938-7228},
pmid = {16264335},
title = {{Neural Processes}},
url = {http://arxiv.org/abs/1807.01622},
year = {2018}
}
@book{Green1966,
author = {Green, D. M. and Swets, J. A.},
publisher = {Wiley},
title = {{Signal detection theory and psychophysics.}},
year = {1966}
}
@article{Wang2012,
abstract = {Multi-task learning models using Gaussian processes (GP) have been developed and suc-cessfully applied in various applications. The main difficulty with this approach is the computational cost of inference using the union of examples from all tasks. Therefore sparse solutions, that avoid using the entire data directly and instead use a set of informa-tive " representatives " are desirable. The paper investigates this problem for the grouped mixed-effect GP model where each individual response is given by a fixed-effect, taken from one of a set of unknown groups, plus a random individual effect function that cap-tures variations among individuals. Such models have been widely used in previous work but no sparse solutions have been developed. The paper presents the first sparse solution for such problems, showing how the sparse approximation can be obtained by maximizing a variational lower bound on the marginal likelihood, generalizing ideas from single-task Gaussian processes to handle the mixed-effect model as well as grouping. Experiments using artificial and real data validate the approach showing that it can recover the perfor-mance of inference with the full sample, that it outperforms baseline methods, and that it outperforms state of the art sparse solutions for other multi-task GP formulations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.6653v1},
author = {Wang, Yuyang and Khardon, Roni},
doi = {10.1007/978-3-642-33460-3_51},
eprint = {arXiv:1211.6653v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Khardon - 2012 - Nonparametric Bayesian Mixed-effect Model a Sparse Gaussian Process Approach.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {gaussian processes,mixed-effect model,multi-task learning,sparse model},
pages = {1--27},
title = {{Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process Approach}},
year = {2012}
}
@article{Li2015a,
abstract = {This paper presents a kernel-based framework for classification of sparse and irregularly sampled time series. The properties of such time series can result in substantial uncertainty about the val-ues of the underlying temporal processes, while making the data difficult to deal with using stan-dard classification methods that assume fixed-dimensional feature spaces. To address these challenges, we propose to first re-represent each time series through the Gaussian process (GP) posterior it induces under a GP regression model. We then define kernels over the space of GP pos-teriors and apply standard kernel-based classifi-cation. Our primary contributions are (i) the de-velopment of a kernel between GPs based on the mixture of kernels between their finite marginals, (ii) the development and analysis of extensions of random Fourier features for scaling the pro-posed kernel to large-scale data, and (iii) an ex-tensive empirical analysis of both the classifica-tion performance and scalability of our proposed approach.},
author = {Li, Steven Cheng-xian and Marlin, Benjamin},
isbn = {9780000000002},
journal = {UAI},
title = {{Classification of Sparse and Irregularly Sampled Time Series with Mixtures of Expected Gaussian Kernels and Random Features}},
year = {2015}
}
@article{Lake2015,
abstract = {People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches.We also present several “visual Turing tests” probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
archivePrefix = {arXiv},
arxivId = {10.1126/science.aab3050},
author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
doi = {10.1126/science.aab3050},
eprint = {science.aab3050},
isbn = {0036-8075},
issn = {10959203},
journal = {Science},
number = {6266},
pmid = {26659050},
primaryClass = {10.1126},
title = {{Human-level concept learning through probabilistic program induction}},
volume = {350},
year = {2015}
}
@article{Vilalta2017,
abstract = {The current state-of-the-art for image annotation and image retrieval tasks is obtained through deep neural networks, which combine an image representation and a text representation into a shared embedding space. In this paper we evaluate the impact of using the Full-Network embedding in this setting, replacing the original image representation in a competitive multimodal embedding generation scheme. Unlike the one-layer image embeddings typically used by most approaches, the Full-Network embedding provides a multi-scale representation of images, which results in richer characterizations. To measure the influence of the Full-Network embedding, we evaluate its performance on three different datasets, and compare the results with the original multimodal embedding generation scheme when using a one-layer image embedding, and with the rest of the state-of-the-art. Results for image annotation and image retrieval tasks indicate that the Full-Network embedding is consistently superior to the one-layer embedding. These results motivate the integration of the Full-Network embedding on any multimodal embedding generation scheme, something feasible thanks to the flexibility of the approach.},
archivePrefix = {arXiv},
arxivId = {1707.09872},
author = {Vilalta, Armand and Garcia-Gasulla, Dario and Par{\'{e}}s, Ferran and Ayguad{\'{e}}, Eduard and Labarta, Jesus and Cort{\'{e}}s, Ulises and Suzumura, Toyotaro},
eprint = {1707.09872},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Vilalta et al. - 2017 - Full-Network Embedding in a Multimodal Embedding Pipeline.pdf:pdf},
title = {{Full-Network Embedding in a Multimodal Embedding Pipeline}},
url = {http://arxiv.org/abs/1707.09872},
year = {2017}
}
@inproceedings{Milli2017,
abstract = {Intuitively, obedience -- following the order that a human gives -- seems like a good property for a robot to have. But, we humans are not perfect and we may give orders that are not best aligned to our preferences. We show that when a human is not perfectly rational then a robot that tries to infer and act according to the human's underlying preferences can always perform better than a robot that simply follows the human's literal order. Thus, there is a tradeoff between the obedience of a robot and the value it can attain for its owner. We investigate how this tradeoff is impacted by the way the robot infers the human's preferences, showing that some methods err more on the side of obedience than others. We then analyze how performance degrades when the robot has a misspecified model of the features that the human cares about or the level of rationality of the human. Finally, we study how robots can start detecting such model misspecification. Overall, our work suggests that there might be a middle ground in which robots intelligently decide when to obey human orders, but err on the side of obedience.},
address = {California},
archivePrefix = {arXiv},
arxivId = {1705.09990},
author = {Milli, Smitha and Hadfield-Menell, Dylan and Dragan, Anca and Russell, Stuart},
booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/662},
eprint = {1705.09990},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Milli et al. - 2017 - Should Robots be Obedient.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
keywords = {Social: Human-machine Interaction,Technical: Models,Technical: Techniques},
month = {aug},
number = {Section 2},
pages = {4754--4760},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
title = {{Should Robots be Obedient?}},
url = {https://www.ijcai.org/proceedings/2017/662},
year = {2017}
}
@article{Zhou2009,
abstract = {Alignment of time series is an important problem to solve in many scientific dis- ciplines. In particular, temporal alignment of two or more subjects performing similar activities is a challenging problem due to the large temporal scale differ- ence between human actions as well as the inter/intra subject variability. In this paper we present canonical time warping (CTW), an extension of canonical cor- relation analysis (CCA) for spatio-temporal alignment of human motion between two subjects. CTW extends previous work on CCA in two ways: (i) it combines CCA with dynamic time warping (DTW), and (ii) it extends CCA by allowing local spatial deformations. We show CTW's effectiveness in three experiments: alignment of synthetic data, alignment of motion capture data of two subjects per- forming similar actions, and alignment of similar facial expressions made by two people. Our results demonstrate thatCTWprovides both visually and qualitatively better alignment than state-of-the-art techniques based on DTW.},
author = {Zhou, Feng and de la Torre, Fernando},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, de la Torre - 2009 - Canonical time warping for alignment of human behavior.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 22 (Proceedings of NIPS)},
keywords = {Canonical Correlation Analysis,Dynamic Time Warping},
pages = {1--9},
title = {{Canonical time warping for alignment of human behavior}},
url = {http://f-zhou.com/pdf/2009{\_}nips{\_}ctw.pdf},
year = {2009}
}
@article{Tian2013,
abstract = {In this paper, a new nonparametric Bayesian model called Sticky Multimodal Dual Hierarchical Dirichlet Process Hidden Markov Model (SMD-HDP-HMM) is proposed for mining activities from a collection of time series. An activity is modeled as an HMM where each state corresponds to an atomic activity. By extensively using Dirichlet Process (DP), multiple HMMs sharing a common set of states are learned and the numbers of HMMs and states are both automatically determined. Each time series is modeled to be generated by one of the HMMs such that all time series are clustered into activities. Simultaneously state sequences for time series are learned and each of them is decomposed into a sequence of atomic activities. Experimental results on KTH activity dataset demonstrate the advantage of our method. {\textcopyright} 2013 IEEE.},
author = {Tian, Guodong and Yuan, Chunfeng and Hu, Weiming and Cai, Zhaoquan},
doi = {10.1109/ICIP.2013.6738021},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tian et al. - 2013 - Mining activities using sticky multimodal dual hierarchical Dirichlet process hidden Markov model.pdf:pdf},
isbn = {9781479923410},
journal = {2013 IEEE International Conference on Image Processing, ICIP 2013 - Proceedings},
keywords = {Dirichlet process,HDP,HMM,activity mining,time series},
number = {2012},
pages = {98--102},
publisher = {IEEE},
title = {{Mining activities using sticky multimodal dual hierarchical Dirichlet process hidden Markov model}},
year = {2013}
}
@article{Bui2017,
abstract = {Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseudo-input locations. The proposed framework is assessed using synthetic and real-world datasets.},
archivePrefix = {arXiv},
arxivId = {1705.07131},
author = {Bui, Thang D. and Nguyen, Cuong V. and Turner, Richard E.},
eprint = {1705.07131},
issn = {10495258},
title = {{Streaming Sparse Gaussian Process Approximations}},
year = {2017}
}
@inproceedings{Goyal2017,
abstract = {The recently developed variational autoencoders (VAEs) have proved to be an effective confluence of the rich representational power of neural networks with Bayesian methods. However, most work on VAEs use a rather simple prior over the latent variables such as standard normal distribution, thereby restricting its applications to relatively simple phenomena. In this work, we propose hierarchical nonparametric variational autoencoders, which combines tree-structured Bayesian nonparametric priors with VAEs, to enable infinite flexibility of the latent representation space. Both the neural parameters and Bayesian priors are learned jointly using tailored variational inference. The resulting model induces a hierarchical structure of latent semantic concepts underlying the data corpus, and infers accurate representations of data instances. We apply our model in video representation learning. Our method is able to discover highly interpretable activity hierarchies, and obtain improved clustering accuracy and generalization capacity based on the learned rich representations.},
author = {Goyal, Prasoon and Hu, Zhiting and Liang, Xiaodan and Wang, Chenyu and Xing, Eric P.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.545},
isbn = {9781538610329},
issn = {15505499},
title = {{Nonparametric Variational Auto-Encoders for Hierarchical Representation Learning}},
volume = {2017-Octob},
year = {2017}
}
@unpublished{Hagen,
author = {Hagen, David R.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hagen - Unknown - Array Calculus.pdf:pdf},
title = {{Array Calculus}}
}
@misc{Shahriari2016,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {De Freitas}, Nando},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2015.2494218},
eprint = {arXiv:1011.1669v3},
isbn = {0018-9219},
issn = {00189219},
number = {1},
pmid = {25246403},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
volume = {104},
year = {2016}
}
@article{Wei2000,
author = {Wei, G. W},
doi = {10.1088/0305-4470/33/27/311},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2000 - A unified approach for the solution of the Fokker-Planck equation.pdf:pdf},
issn = {0305-4470},
journal = {Journal of Physics A: Mathematical and General},
keywords = {differential equation,diffusion equation,discrete poisson equation,equation solving,fokker planck equation,mathematical analysis,mathematical optimization,mathematics,partial differential equation,stiff equation,summation equation},
month = {jul},
number = {27},
pages = {4935--4953},
title = {{A unified approach for the solution of the Fokker-Planck equation}},
url = {https://iopscience.iop.org/article/10.1088/0305-4470/33/27/311},
volume = {33},
year = {2000}
}
@article{Morais2018,
abstract = {Recent work in theoretical neuroscience has shown that efficient neural codes, which allocate neural resources to maximize the mutual information between stimuli and neural responses, give rise to a lawful relationship between perceptual bias and discriminability in psychophysical measurements (Wei {\&} Stocker 2017, [1]). Here we generalize these results to show that the same law arises under a much larger family of optimal neural codes, which we call power-law efficient codes. These codes provide a unifying framework for understanding the relationship between perceptual bias and discriminability, and how it depends on the allocation of neural resources. Specifically, we show that the same lawful relationship between bias and discriminability arises whenever Fisher information is allocated proportional to any power of the prior distribution. This family includes neural codes that are optimal for minimizing Lp error for any p, indicating that the lawful relationship observed in human psychophysical data does not require information-theoretically optimal neural codes. Furthermore, we derive the exact constant of proportionality governing the relationship between bias and discriminability for different choices of power law exponent q, which includes information-theoretic (q = 2) as well as “discrimax” (q = 1/2) neural codes, and different choices of decoder. As a bonus, our framework provides new insights into “anti-Bayesian” perceptual biases, in which percepts are biased away from the center of mass of the prior. We derive an explicit formula that clarifies precisely which combinations of neural encoder and decoder can give rise to such biases.},
author = {Morais, Michael J. and Pillow, Jonathan W.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Morais, Pillow - 2018 - Power-law efficient neural codes provide general link between perceptual bias and discriminability.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {5071--5080},
title = {{Power-law efficient neural codes provide general link between perceptual bias and discriminability}},
volume = {2018-Decem},
year = {2018}
}
@article{Atkinson2019,
abstract = {We introduce a methodology for nonlinear inverse problems using a variational Bayesian approach where the unknown quantity is a spatial field. A structured Bayesian Gaussian process latent variable model is used both to construct a low-dimensional generative model of the sample-based stochastic prior as well as a surrogate for the forward evaluation. Its Bayesian formulation captures epistemic uncertainty introduced by the limited number of input and output examples, automatically selects an appropriate dimensionality for the learned latent representation of the data, and rigorously propagates the uncertainty of the data-driven dimensionality reduction of the stochastic space through the forward model surrogate. The structured Gaussian process model explicitly leverages spatial information for an informative generative prior to improve sample efficiency while achieving computational tractability through Kronecker product decompositions of the relevant kernel matrices. Importantly, the Bayesian inversion is carried out by solving a variational optimization problem, replacing traditional computationally-expensive Monte Carlo sampling. The methodology is demonstrated on an elliptic PDE and is shown to return well-calibrated posteriors and is tractable with latent spaces with over 100 dimensions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.04302v1},
author = {Atkinson, Steven and Zabaras, Nicholas},
doi = {10.1016/j.jcp.2018.12.037},
eprint = {arXiv:1807.04302v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Atkinson, Zabaras - 2019 - Structured Bayesian Gaussian process latent variable model Applications to data-driven dimensionality reducti.pdf:pdf},
issn = {00219991},
journal = {Journal of Computational Physics},
keywords = {Bayesian inference,Gaussian processes,Stochastic partial differential equations,Surrogate models,Uncertainty quantification,Variational inference},
month = {apr},
pages = {166--195},
title = {{Structured Bayesian Gaussian process latent variable model: Applications to data-driven dimensionality reduction and high-dimensional inversion}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999119300397},
volume = {383},
year = {2019}
}
@article{Ranganath2014,
abstract = {We describe $\backslash$textit{\{}deep exponential families{\}} (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent "black box" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {1411.2581},
author = {Ranganath, Rajesh and Tang, Linpeng and Charlin, Laurent and Blei, David M.},
doi = {10.1016/j.nimb.2008.03.150},
eprint = {1411.2581},
isbn = {1411.2581},
issn = {15337928},
title = {{Deep Exponential Families}},
year = {2014}
}
@inproceedings{Burda2016,
abstract = {The variational autoencoder (VAE; Kingma {\&} Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1509.00519},
author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1509.00519},
title = {{Importance weighted autoencoders}},
year = {2016}
}
@article{Chua2018,
abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
archivePrefix = {arXiv},
arxivId = {1805.12114},
author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
doi = {arXiv:1805.12114v1},
eprint = {1805.12114},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chua et al. - 2018 - Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models.pdf:pdf},
isbn = {1471-2458},
number = {Nips},
title = {{Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}},
url = {http://arxiv.org/abs/1805.12114},
year = {2018}
}
@article{Elhoseiny2015,
abstract = {There has been a growing interest in mutual information measures due to their wide range of applications in machine learning and computer vision. In this paper, we present a generalized structured regression framework based on Sharma–Mittal (SM) divergence, a relative entropy measure, which is introduced to in the machine learning community in this work. SM divergence is a generalized mutual information measure for the widely used R{\'{e}}nyi, Tsallis, Bhattacharyya, and Kullback–Leibler (KL) relative entropies. Specifically, we study SM divergence as a cost function in the context of the Twin Gaussian processes (TGP) (Bo and Sminchisescu 2010), which generalizes over the KL-divergence without computational penalty. We show interesting properties of Sharma–Mittal TGP (SMTGP) through a theoretical analysis, which covers missing insights in the traditional TGP formulation. However, we generalize this theory based on SM-divergence instead of KL-divergence which is a special case. Experimentally, we evaluated the proposed SMTGP framework on several datasets. The results show that SMTGP reaches better predictions than KL-based TGP, since it offers a bigger class of models through its parameters that we learn from the data.},
archivePrefix = {arXiv},
arxivId = {1409.7480},
author = {Elhoseiny, Mohamed and Elgammal, Ahmed},
doi = {10.1007/s10994-015-5497-9},
eprint = {1409.7480},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Elhoseiny, Elgammal - 2015 - Generalized Twin Gaussian processes using Sharma–Mittal divergence.pdf:pdf},
issn = {15730565},
journal = {Machine Learning},
keywords = {Image reconstruction,Pose estimation,Sharma–Mittal entropy,Structured regression,Twin Gaussian processes},
number = {2-3},
pages = {399--424},
title = {{Generalized Twin Gaussian processes using Sharma–Mittal divergence}},
volume = {100},
year = {2015}
}
@article{Durrande2013,
abstract = {We consider the problem of detecting and quantifying the periodic component of a function given noise-corrupted observations of a limited number of input/output tuples. Our approach is based on Gaussian process regression which provides a flexible non-parametric framework for modelling periodic data. We introduce a novel decomposition of the covariance function as the sum of periodic and aperiodic kernels. This decomposition allows for the creation of sub-models which capture the periodic nature of the signal and its complement. To quantify the periodicity of the signal, we derive a periodicity ratio which reflects the uncertainty in the fitted sub-models. Although the method can be applied to many kernels, we give a special emphasis to the Mat$\backslash$'ern family, from the expression of the reproducing kernel Hilbert space inner product to the implementation of the associated periodic kernels in a Gaussian process toolkit. The proposed method is illustrated by considering the detection of periodically expressed genes in the arabidopsis genome.},
archivePrefix = {arXiv},
arxivId = {1303.7090},
author = {Durrande, Nicolas and Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
eprint = {1303.7090},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Durrande et al. - 2013 - Gaussian process models for periodicity detection.pdf:pdf},
keywords = {harmonic analysis,kriging,mat,rkhs},
month = {mar},
pages = {1--25},
title = {{Gaussian process models for periodicity detection}},
url = {http://arxiv.org/abs/1303.7090},
year = {2013}
}
@article{Zhang2017b,
author = {Zhang, Yu and Zhou, Guoxu and Jin, Jing and Zhang, Yangsong and Wang, Xingyu and Cichocki, Andrzej},
doi = {10.1016/j.neucom.2016.11.008},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Sparse Bayesian multiway canonical correlation analysis for EEG pattern recognition.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Brain–computer interface (BCI),Electroencephalogram (EEG),Multiway canonical correlation analysis (MCCA),Sparse Bayesian learning,Steady-state visual evoked potential (SSVEP),bci,brain,computer interface,eeg,electroencephalogram,mcca,multiway canonical correlation analysis},
month = {feb},
number = {November 2016},
pages = {103--110},
publisher = {Elsevier},
title = {{Sparse Bayesian multiway canonical correlation analysis for EEG pattern recognition}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.11.008 https://linkinghub.elsevier.com/retrieve/pii/S0925231216313376},
volume = {225},
year = {2017}
}
@article{Huys2017,
author = {Huys, Quentin J M and Maia, Tiago V and Frank, Michael J and Unit, Neuromodeling and Behavior, Human},
doi = {10.1038/nn.4238},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Huys et al. - 2017 - clinical applications.pdf:pdf},
number = {3},
pages = {404--413},
title = {clinical applications},
volume = {19},
year = {2017}
}
@article{Gelbart2014,
abstract = {Recent work on Bayesian optimization has shown its effectiveness in global optimization of difficult black-box objective functions. Many real-world optimization problems of interest also have constraints which are unknown a priori. In this paper, we study Bayesian optimization for constrained problems in the general case that noise may be present in the constraint functions, and the objective and constraints may be evaluated independently. We provide motivating practical examples, and present a general framework to solve such problems. We demonstrate the effectiveness of our approach on optimizing the performance of online latent Dirichlet allocation subject to topic sparsity constraints, tuning a neural network given test-time memory constraints, and optimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed time, subject to passing standard convergence diagnostics.},
author = {Gelbart, Michael A. and Snoek, Jasper and Adams, Ryan P.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gelbart, Snoek, Adams - 2014 - Bayesian optimization with unknown constraints.pdf:pdf},
isbn = {9780974903910},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014},
pages = {250--259},
title = {{Bayesian optimization with unknown constraints}},
year = {2014}
}
@article{Matthews2018,
abstract = {Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.},
archivePrefix = {arXiv},
arxivId = {1804.11271},
author = {Matthews, Alexander G. de G. and Rowland, Mark and Hron, Jiri and Turner, Richard E. and Ghahramani, Zoubin},
doi = {10.1039/f19868202801},
eprint = {1804.11271},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Matthews et al. - 2018 - Gaussian Process Behaviour in Wide Deep Neural Networks.pdf:pdf},
isbn = {1804.11271v2},
issn = {0300-9589},
month = {apr},
pages = {1--36},
title = {{Gaussian Process Behaviour in Wide Deep Neural Networks}},
url = {http://arxiv.org/abs/1804.11271},
year = {2018}
}
@article{Hielscher2002,
author = {Hielscher, A H and Bluestone, A Y and Abdoulaev, G S and Klose, A D and Lasker, J and Stewart, M},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hielscher et al. - 2002 - Near-infrared diffuse optical tomography.pdf:pdf},
keywords = {brain and joint imaging,image reconstruction,instrumentation,scattering media,tomography},
pages = {313--337},
title = {{Near-infrared diffuse optical tomography}},
volume = {18},
year = {2002}
}
@article{Alon2016,
abstract = {A key feature of neural network architectures is their ability to support the simultaneous interaction among large numbers of units in the learning and processing of representations. However, how the richness of such interactions trades off against the ability of a network to simultaneously carry out multiple independent processes -- a salient limitation in many domains of human cognition -- remains largely unexplored. In this paper we use a graph-theoretic analysis of network architecture to address this question, where tasks are represented as edges in a bipartite graph {\$}G=(A \backslashcup B, E){\$}. We define a new measure of multitasking capacity of such networks, based on the assumptions that tasks that $\backslash$emph{\{}need{\}} to be multitasked rely on independent resources, i.e., form a matching, and that tasks $\backslash$emph{\{}can{\}} be multitasked without interference if they form an induced matching. Our main result is an inherent tradeoff between the multitasking capacity and the average degree of the network that holds $\backslash$emph{\{}regardless of the network architecture{\}}. These results are also extended to networks of depth greater than {\$}2{\$}. On the positive side, we demonstrate that networks that are random-like (e.g., locally sparse) can have desirable multitasking properties. Our results shed light into the parallel-processing limitations of neural systems and provide insights that may be useful for the analysis and design of parallel architectures.},
archivePrefix = {arXiv},
arxivId = {1611.02400},
author = {Alon, Noga and Cohen, Jonathan D. and Dey, Biswadip and Griffiths, Thomas L. and Musslick, Sebastian and Ozcimder, Kayhan and Reichman, Daniel and Shinkar, Igor and Wagner, Tal},
eprint = {1611.02400},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Alon et al. - 2016 - A Graph-Theoretic Approach to Multitasking.pdf:pdf},
number = {November},
title = {{A Graph-Theoretic Approach to Multitasking}},
url = {http://arxiv.org/abs/1611.02400},
year = {2016}
}
@article{Zhong,
abstract = {Classifying electroencephalography (EEG) signals is an important step for proceeding EEG-based brain computer interfaces (BCI). Currently, kernel based methods such as sup- port vector machine (SVM) are the state-of-the-art methods for this problem. In this paper, we apply Gaussian process (GP) classification to binary classification problems of motor imagery EEG data. Comparing with SVM, GP based methods naturally provide probabil- ity outputs for identifying a trusted prediction which can be used for post-processing in a BCI. Experimental results show that the classification methods based on Gaussian process perform similar with kernel logistic regression and probabilistic SVM in terms of predictive likelihood, but outperform SVM and K-Nearest Neighbor (KNN) in terms of 0-1 loss class prediction error},
author = {Zhong, Mingjun and Lotte, Fabien and Girolami, Mark and L, Anatole},
doi = {10.1016/j.patrec.2007.10.009},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhong et al. - Unknown - Classifying EEG for Brain Computer Interfaces Using Gaussian Process Gaussian Process for Binary Classification.pdf:pdf},
journal = {Computing},
keywords = {brain computer interfaces,eeg,gaussian process,support vector machine},
number = {0},
pages = {1--8},
title = {{Classifying EEG for Brain Computer Interfaces Using Gaussian Process Gaussian Process for Binary Classification}},
volume = {33}
}
@article{Duvenaud2013,
author = {Duvenaud, David and Lloyd, James Robert and Grosse, Roger and Tenenbaum, Joshua B and Lin, Lin},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Duvenaud et al. - 2013 - Structure Discovery in Nonparametric Regression through Compositional Kernel Search.pdf:pdf},
title = {{Structure Discovery in Nonparametric Regression through Compositional Kernel Search}},
volume = {28},
year = {2013}
}
@article{Solin2018,
abstract = {Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension {\$}m{\$} which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension {\$}m{\$} to {\$}\backslashmathcal{\{}O{\}}(m{\^{}}2){\$} per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100{\~{}}Hz.},
archivePrefix = {arXiv},
arxivId = {1811.06588},
author = {Solin, Arno and Hensman, James and Turner, Richard E.},
eprint = {1811.06588},
month = {nov},
title = {{Infinite-Horizon Gaussian Processes}},
url = {http://arxiv.org/abs/1811.06588},
year = {2018}
}
@article{Gebhardt2019,
author = {Gebhardt, Christoph and Hecox, Brian and Opheusden, Bas Van and Wigdor, Daniel and Hillis, James and Hilliges, Otmar and Benko, Hrvoje},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gebhardt et al. - 2019 - Learning Cooperative Personalized Policies from Gaze Data.pdf:pdf},
isbn = {9781450368162},
title = {{Learning Cooperative Personalized Policies from Gaze Data}},
year = {2019}
}
@article{Letham2017,
abstract = {Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasi-Monte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.},
archivePrefix = {arXiv},
arxivId = {1706.07094},
author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
doi = {10.1214/18-BA1110},
eprint = {1706.07094},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Letham et al. - 2017 - Constrained Bayesian Optimization with Noisy Experiments.pdf:pdf},
issn = {1936-0975},
keywords = {bayesian optimization,quasi-monte carlo,randomized experiments},
number = {0},
title = {{Constrained Bayesian Optimization with Noisy Experiments}},
url = {http://arxiv.org/abs/1706.07094},
year = {2017}
}
@article{Tiwari2018,
abstract = {The recently proposed option-critic architecture Bacon et al. provide a stochastic policy gradient approach to hierarchical reinforcement learning. Specifically, they provide a way to estimate the gradient of the expected discounted return with respect to parameters that define a finite number of temporally extended actions, called $\backslash$textit{\{}options{\}}. In this paper we show how the option-critic architecture can be extended to estimate the natural gradient of the expected discounted return. To this end, the central questions that we consider in this paper are: 1) what is the definition of the natural gradient in this context, 2) what is the Fisher information matrix associated with an option's parameterized policy, 3) what is the Fisher information matrix associated with an option's parameterized termination function, and 4) how can a compatible function approximation approach be leveraged to obtain natural gradient estimates for both the parameterized policy and parameterized termination functions of an option with per-time-step time and space complexity linear in the total number of parameters. Based on answers to these questions we introduce the natural option critic algorithm. Experimental results showcase improvement over the vanilla gradient approach.},
archivePrefix = {arXiv},
arxivId = {1812.01488},
author = {Tiwari, Saket and Thomas, Philip S},
doi = {arXiv:1812.01488v1},
eprint = {1812.01488},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tiwari, Thomas - 2018 - Natural Option Critic.pdf:pdf},
title = {{Natural Option Critic}},
url = {http://arxiv.org/abs/1812.01488},
year = {2018}
}
@article{Doerr2018,
abstract = {State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. To overcome this limitation, we propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. In contrast to existing work, the proposed variational approximation allows one to fully capture the latent state temporal correlations. These correlations are the key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.},
archivePrefix = {arXiv},
arxivId = {1801.10395},
author = {Doerr, Andreas and Daniel, Christian and Schiegg, Martin and Nguyen-Tuong, Duy and Schaal, Stefan and Toussaint, Marc and Trimpe, Sebastian},
eprint = {1801.10395},
title = {{Probabilistic Recurrent State-Space Models}},
year = {2018}
}
@article{Tran2017,
abstract = {Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.},
archivePrefix = {arXiv},
arxivId = {1702.08896},
author = {Tran, Dustin and Ranganath, Rajesh and Blei, David M.},
eprint = {1702.08896},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tran, Ranganath, Blei - 2017 - Hierarchical Implicit Models and Likelihood-Free Variational Inference.pdf:pdf},
issn = {1702.08896},
number = {Nips},
title = {{Hierarchical Implicit Models and Likelihood-Free Variational Inference}},
url = {http://arxiv.org/abs/1702.08896},
year = {2017}
}
@article{Shenhav2017,
abstract = {In spite of its familiar phenomenology, the mechanistic basis for mental effort remains poorly understood. Although most researchers agree that mental effort is aversive and stems from limitations in our capacity to exercise cognitive control, it is unclear what gives rise to those limitations and why they result in an experience of control as costly. The presence of these control costs also raises further questions regarding how best to allocate mental effort to minimize those costs and maximize the attendant benefits. This review explores recent advances in computational modeling and empirical research aimed at addressing these questions at the level of psychological process and neural mechanism, examining both the limitations to mental effort exertion and how we manage those limited cognitive resources. We conclude by identifying remaining challenges for theoretical accounts of mental effort as well as possible applications of the available findings to understanding the causes of and potential solutions for apparent failures to exert the mental effort required of us. Expected final online publication date for the Annual Review of Neuroscience Volume 40 is July 8, 2017. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
author = {Shenhav, Amitai and Musslick, Sebastian and Lieder, Falk and Kool, Wouter and Griffiths, Thomas L. and Cohen, Jonathan D. and Botvinick, Matthew M.},
doi = {10.1146/annurev-neuro-072116-031526},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shenhav et al. - 2017 - Toward a Rational and Mechanistic Account of Mental Effort.pdf:pdf},
isbn = {0718160649},
issn = {0147-006X},
journal = {Annual Review of Neuroscience},
number = {1},
pages = {99--124},
pmid = {28375769},
title = {{Toward a Rational and Mechanistic Account of Mental Effort}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-neuro-072116-031526},
volume = {40},
year = {2017}
}
@article{Cutajar2016a,
abstract = {The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget.},
archivePrefix = {arXiv},
arxivId = {1602.06693},
author = {Cutajar, Kurt and Osborne, Michael A. and Cunningham, John P. and Filippone, Maurizio},
eprint = {1602.06693},
isbn = {9781510829008},
title = {{Preconditioning Kernel Matrices}},
year = {2016}
}
@article{Lupetti2014,
abstract = {We propose an experimental arrangement to image, with attosecond resolution, transient surface plasmonic excitations. The required modifications to state-of-the-art setups used for attosecond streaking experiments from solid surfaces only involve available technology. Buildup and life times of surface plasmon polaritons can be extracted and local modulations of the exciting optical pulse can be diagnosed {\{}$\backslash$it in situ{\}}.},
archivePrefix = {arXiv},
arxivId = {1401.4290},
author = {Lupetti, Mattia and Hengster, Julia and Uphues, Thorsten and Scrinzi, Armin},
doi = {10.1103/PhysRevLett.113.113903},
eprint = {1401.4290},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lupetti et al. - 2014 - Attosecond Photoscopy of Plasmonic Excitations.pdf:pdf},
isbn = {1469-8986},
issn = {1469-8986},
journal = {Psychophysiology},
keywords = {Artifact removal,EEG,EOG,ICA,Independent component analysis},
month = {jan},
number = {02},
pages = {163--178},
pmid = {10731767},
title = {{Attosecond Photoscopy of Plasmonic Excitations}},
url = {http://journals.cambridge.org/abstract{\_}S0048577200980259 http://arxiv.org/abs/1401.4290 http://dx.doi.org/10.1103/PhysRevLett.113.113903},
volume = {37},
year = {2014}
}
@article{Jayaram2018,
abstract = {Objective. Brain-computer interface (BCI) algorithm development has long been hampered by two major issues: small sample sets and a lack of reproducibility. We offer a solution to both of these problems via a software suite that streamlines both the issues of finding and preprocessing data in a reliable manner, as well as that of using a consistent interface for machine learning methods. Approach. By building on recent advances in software for signal analysis implemented in the MNE toolkit, and the unified framework for machine learning offered by the scikit-learn project, we offer a system that can improve BCI algorithm development. This system is fully open-source under the BSD licence and available at https://github.com/NeuroTechX/moabb. Main results. We analyze a set of state-of-the-art decoding algorithms across 12 open access datasets, including over 250 subjects. Our results show that even for the best methods, there are datasets which do not show significant improvements, and further that many previously validated methods do not generalize well outside the datasets they were tested on. Significance. Our analysis confirms that BCI algorithms validated on single datasets are not representative, highlighting the need for more robust validation in the machine learning for BCIs community.},
archivePrefix = {arXiv},
arxivId = {1805.06427},
author = {Jayaram, Vinay and Barachant, Alexandre},
doi = {10.1088/1741-2552/aadea0},
eprint = {1805.06427},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jayaram, Barachant - 2018 - MOABB Trustworthy algorithm benchmarking for BCIs.pdf:pdf},
issn = {17412552},
journal = {Journal of Neural Engineering},
keywords = {BCI,CSP,EEG,brain-computer interfacing,machine learning,software,spatial filtering},
number = {6},
publisher = {IOP Publishing},
title = {{MOABB: Trustworthy algorithm benchmarking for BCIs}},
volume = {15},
year = {2018}
}
@article{Hadfield-Menell2017,
abstract = {Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.},
archivePrefix = {arXiv},
arxivId = {1711.02827},
author = {Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca},
eprint = {1711.02827},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hadfield-Menell et al. - 2017 - Inverse Reward Design.pdf:pdf},
issn = {10495258},
month = {nov},
number = {Nips},
title = {{Inverse Reward Design}},
url = {http://arxiv.org/abs/1711.02827},
year = {2017}
}
@article{Teng2015,
abstract = {Sequential hypothesis testing is a desirable decision making strategy in any time sensitive scenario. Compared with fixed sample-size testing, sequential testing is capable of achieving identical probability of error requirements using less samples in average. For a binary detection problem, it is well known that for known density functions accumulating the likelihood ratio statistics is time optimal under a fixed error rate constraint. This paper considers the problem of learning a binary sequential detector from training samples when density functions are unavailable. We formulate the problem as a constrained likelihood ratio estimation which can be solved efficiently through convex optimization by imposing Reproducing Kernel Hilbert Space (RKHS) structure on the log-likelihood ratio function. In addition, we provide a computationally efficient approximated solution for large scale data set. The proposed algorithm, namely Wald-Kernel, is tested on a synthetic data set and two real world data sets, together with previous approaches for likelihood ratio estimation. Our empirical results show that the classifier trained through the proposed technique achieves smaller average sampling cost than previous approaches proposed in the literature for the same error rate.},
archivePrefix = {arXiv},
arxivId = {1508.07964},
author = {Teng, Diyan and Ertin, Emre},
eprint = {1508.07964},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Teng, Ertin - 2015 - Wald-Kernel Learning to Aggregate Information for Sequential Inference.pdf:pdf},
pages = {1--14},
title = {{Wald-Kernel: Learning to Aggregate Information for Sequential Inference}},
url = {http://arxiv.org/abs/1508.07964},
year = {2015}
}
@article{Matthews2016,
abstract = {GPflow is a Gaussian process library that uses TensorFlow for its core computations and Python for its front end. The distinguishing features of GPflow are that it uses variational inference as the primary approximation method, provides concise code through the use of automatic differentiation, has been engineered with a particular emphasis on software testing and is able to exploit GPU hardware.},
archivePrefix = {arXiv},
arxivId = {1610.08733},
author = {Matthews, Alexander G. de G. and van der Wilk, Mark and Nickson, Tom and Fujii, Keisuke and Boukouvalas, Alexis and Le{\'{o}}n-Villagr{\'{a}}, Pablo and Ghahramani, Zoubin and Hensman, James},
eprint = {1610.08733},
issn = {15337928},
title = {{GPflow: A Gaussian process library using TensorFlow}},
year = {2016}
}
@inproceedings{Smith2008,
author = {Smith, Ryan J and Tenore, Francesco and Huberdeau, David and Etienne-Cummings, Ralph and Thakor, Nitish V},
booktitle = {2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
doi = {10.1109/IEMBS.2008.4649124},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2008 - Continuous decoding of finger position from surface EMG signals for the control of powered prostheses.pdf:pdf},
isbn = {978-1-4244-1814-5},
keywords = {EMG processing {\&} applications,Prosthetics {\&} orthotics,Robotics},
month = {aug},
pages = {197--200},
publisher = {IEEE},
title = {{Continuous decoding of finger position from surface EMG signals for the control of powered prostheses}},
url = {http://ieeexplore.ieee.org/document/4649124/},
year = {2008}
}
@article{Ranganath2015,
abstract = {Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior.},
archivePrefix = {arXiv},
arxivId = {1511.02386},
author = {Ranganath, Rajesh and Tran, Dustin and Blei, David M.},
eprint = {1511.02386},
isbn = {1511.02386},
title = {{Hierarchical Variational Models}},
year = {2015}
}
@article{Luo2013,
abstract = {Gaussian process regression (GPR) is a powerful non-linear technique for Bayesian inference and prediction. One drawback is its O(N 3) computational complexity for both prediction and hyperparameter estimation for N input points which has led to much work in sparse GPR methods. In case that the covariance function is expressible as a tensor product kernel (TPK) and the inputs form a multidimensional grid, it was shown that the costs for exact GPR can be reduced to a sub-quadratic function of N. We extend these exact fast algorithms to sparse GPR and remark on a connection to Gaussian process latent variable models (GPLVMs). In practice, the inputs may also violate the mul-tidimensional grid constraints so we pose and efficiently solve missing and extra data problems for both exact and sparse grid GPR. We demonstrate our method on synthetic, text scan, and magnetic resonance imaging (MRI) data reconstructions.},
author = {Luo, Yuancheng and Duraiswami, Ramani},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Luo, Duraiswami - 2013 - Fast near-GRID Gaussian process regression.pdf:pdf},
issn = {1533-7928},
journal = {Journal of Machine Learning Research},
number = {1952},
pages = {424--432},
title = {{Fast near-GRID Gaussian process regression}},
volume = {31},
year = {2013}
}
@article{Angelino2016,
abstract = {Datasets are growing not just in size but in complexity, creating a demand for rich models and quantification of uncertainty. Bayesian methods are an excellent fit for this demand, but scaling Bayesian inference is a challenge. In response to this challenge, there has been considerable recent work based on varying assumptions about model structure, underlying computational resources, and the importance of asymptotic correctness. As a result, there is a zoo of ideas with few clear overarching principles. In this paper, we seek to identify unifying principles, patterns, and intuitions for scaling Bayesian inference. We review existing work on utilizing modern computing resources with both MCMC and variational approximation techniques. From this taxonomy of ideas, we characterize the general principles that have proven successful for designing scalable inference procedures and comment on the path forward.},
archivePrefix = {arXiv},
arxivId = {1602.05221},
author = {Angelino, Elaine and Johnson, Matthew James and Adams, Ryan P.},
doi = {10.1561/2200000052},
eprint = {1602.05221},
isbn = {1602.05221},
issn = {1935-8237},
title = {{Patterns of Scalable Bayesian Inference}},
year = {2016}
}
@article{Zhang2016,
abstract = {In this paper, we propose a unified co-salient$\backslash$r$\backslash$nobject detection framework by introducing two novel$\backslash$r$\backslash$ninsights: (1) looking deep to transfer higher-level representations$\backslash$r$\backslash$nby using the convolutional neural network with$\backslash$r$\backslash$nadditional adaptive layers could better reflect the sematic$\backslash$r$\backslash$nproperties of the co-salient objects; (2) looking wide to$\backslash$r$\backslash$ntake advantage of the visually similar neighbors from other$\backslash$r$\backslash$nimage groups could effectively suppress the influence of the$\backslash$r$\backslash$ncommon background regions. The wide and deep information$\backslash$r$\backslash$nare explored for the object proposal windows extracted$\backslash$r$\backslash$nin each image. The window-level co-saliency scores are$\backslash$r$\backslash$ncalculated by integrating the intra-image contrast, the intragroup$\backslash$r$\backslash$nconsistency, and the inter-group separability via a$\backslash$r$\backslash$nprincipled Bayesian formulation and are then converted to$\backslash$r$\backslash$nthe superpixel-level co-saliency maps through a foreground$\backslash$r$\backslash$nregion agreement strategy. Comprehensive experiments on$\backslash$r$\backslash$ntwo existing and one newly established datasets have demonstrated$\backslash$r$\backslash$nthe consistent performance gain of the proposed$\backslash$r$\backslash$napproach.$\backslash$r$\backslash$n},
author = {Zhang, Dingwen and Han, Junwei and Li, Chao and Wang, Jingdong and Li, Xuelong},
doi = {10.1007/s11263-016-0907-4},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2016 - Detection of Co-salient Objects by Looking Deep and Wide.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Bayesian framework,Co-saliency detection,Domain adaptive convolutional neural network},
number = {2},
pages = {215--232},
publisher = {Springer US},
title = {{Detection of Co-salient Objects by Looking Deep and Wide}},
volume = {120},
year = {2016}
}
@article{Lund2014,
abstract = {Diffusion processes superimposed upon deterministic motion play a key role in understanding and controlling the transport of matter, energy, momentum, and even information in physics, chemistry, material science, biology, and communications technology. Given functions defining these random and deterministic components, the Fokker-Planck (FP) equation is often used to model these diffusive systems. Many methods exist for estimating the drift and diffusion profiles from one or more identifiable diffusive trajectories; however, when many identical entities diffuse simultaneously, it may not be possible to identify individual trajectories. Here we present a method capable of simultaneously providing nonparametric estimates for both drift and diffusion profiles from evolving density profiles, requiring only the validity of Langevin/FP dynamics. This algebraic FP manipulation provides a flexible and robust framework for estimating stationary drift and diffusion coefficient profiles, is not based on fluctuation theory or solved diffusion equations, and may facilitate predictions for many experimental systems. We illustrate this approach on experimental data obtained from a model lipid bilayer system exhibiting free diffusion and electric field induced drift. The wide range over which this approach provides accurate estimates for drift and diffusion profiles is demonstrated through simulation.},
author = {Lund, Steven P. and Hubbard, Joseph B. and Halter, Michael},
doi = {10.1021/jp5084357},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lund, Hubbard, Halter - 2014 - Nonparametric estimates of drift and diffusion profiles via Fokker-Planck algebra.pdf:pdf},
issn = {15205207},
journal = {Journal of Physical Chemistry B},
number = {44},
pages = {12743--12749},
title = {{Nonparametric estimates of drift and diffusion profiles via Fokker-Planck algebra}},
volume = {118},
year = {2014}
}
@article{Astudillo2019,
abstract = {We consider black-box global optimization of time-consuming-to-evaluate functions on behalf of a decision-maker whose preferences must be learned. Each feasible design is associated with a time-consuming-to-evaluate vector of attributes, each vector of attributes is assigned a utility by the decision-maker's utility function, and this utility function may be learned approximately using preferences expressed by the decision-maker over pairs of attribute vectors. Past work has used this estimated utility function as if it were error-free within single-objective optimization. However, errors in utility estimation may yield a poor suggested decision. Furthermore, this approach produces a single suggested "best" design, whereas decision-makers often prefer to choose among a menu of designs. We propose a novel Bayesian optimization algorithm that acknowledges the uncertainty in preference estimation and implicitly chooses designs to evaluate using the time-consuming function that are good not just for a single estimated utility function but a range of likely utility functions. Our algorithm then shows a menu of designs and evaluated attributes to the decision-maker who makes a final selection. We demonstrate the value of our algorithm in a variety of numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1911.05934},
author = {Astudillo, Raul and Frazier, Peter I.},
eprint = {1911.05934},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Astudillo, Frazier - 2019 - Bayesian Optimization with Uncertain Preferences over Attributes(2).pdf:pdf},
title = {{Bayesian Optimization with Uncertain Preferences over Attributes}},
url = {http://arxiv.org/abs/1911.05934},
year = {2019}
}
@article{Gardner2015a,
abstract = {Psychophysical detection tests are ubiquitous in the study of human sensation and the diagnosis and treatment of virtually all sensory impairments. In many of these settings, the goal is to recover, from a series of binary observations from a human subject, the latent function that describes the discriminability of a sensory stimulus over some relevant domain. The auditory detection test, for example, seeks to understand a subject's likelihood of hearing sounds as a function of frequency and amplitude. Conventional methods for performing these tests involve testing stimuli on a pre-determined grid. This approach not only samples at very uninformative locations, but also fails to learn critical features of a subject's latent discriminability function. Here we advance active learning with Gaussian processes to the setting of psychophysical testing. We develop a model that incorporates strong prior knowledge about the class of stimuli, we derive a sensible method for choosing sample points, and we demonstrate how to evaluate this model efficiently. Finally, we develop a novel likelihood that enables testing of multiple stimuli simultaneously. We evaluate our method in both simulated and real auditory detection tests, demonstrating the merit of our approach.},
author = {Gardner, Jacob R. and Song, Xinyu D. and Weinberger, Kilian Q. and Barbour, Dennis and Cunningham, John P.},
file = {:Users/mshvarts/Downloads/213.pdf:pdf},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 31st Conference, UAI 2015},
pages = {286--295},
title = {{Psychophysical detection testing with Bayesian active learning}},
year = {2015}
}
@article{Ma2015a,
abstract = {Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient MCMC samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing MCMC samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed SGRHMC sampler inherits the benefits of Riemann HMC, with the scalability of stochastic gradient methods.},
archivePrefix = {arXiv},
arxivId = {1506.04696},
author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
eprint = {1506.04696},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ma, Chen, Fox - 2015 - A Complete Recipe for Stochastic Gradient MCMC(2).pdf:pdf},
number = {Mcmc},
pages = {1--9},
title = {{A Complete Recipe for Stochastic Gradient MCMC}},
url = {http://arxiv.org/abs/1506.04696},
year = {2015}
}
@unpublished{Kolecki2002,
abstract = {Tensor analysis is the type of subject that can make even the best of students shudder. My own post-graduate instructor in the subject took away much of the fear by speaking of an implicit rhythm in the peculiar notation traditionally used, and helped us to see how this rhythm plays its way throughout the various formalisms. Prior to taking that class, I had spent many years playing on my own with tensors. I found the going to be tremendously difficult but was able, over time, to back out some physical and geometrical consider- ations that helped to make the subject a little more transparent. Today, it is sometimes hard not to think in terms of tensors and their associated concepts. This article, prompted and greatly enhanced by Marlos Jacob, whom Ive met only by e-mail, is an attempt to record those early notions concerning tensors. It is intended to serve as a bridge from the point where most undergraduate students leave off in their studies of mathematics to the place where most texts on tensor analysis begin. A basic knowledge of vectors, matrices, and physics is assumed. A semi-intuitive approach to those notions underlying tensor analysis is given via scalars, vectors, dyads, triads, and higher vector products. The reader must be prepared to do some mathematics and to think. For those students who wish to go beyond this humble start, I can only recommend my professors wisdom: find the rhythm in the mathematics and you will fare pretty well.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kolecki, Joseph C},
doi = {10.1049/sqj.1936.0070},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kolecki - 2002 - An Introduction to Tensors for Students of Physics and Engineering.pdf:pdf},
isbn = {3016210134},
issn = {18733514},
number = {September},
pages = {29},
pmid = {21439990},
title = {{An Introduction to Tensors for Students of Physics and Engineering}},
url = {http://unix.eng.ua.edu/{~}checlass/che304/che304/{\_}Media/tensors{\_}tm2002211716.pdf},
volume = {7},
year = {2002}
}
@article{Ryzhov2018,
abstract = {We propose a framework for targeting and selection (T{\&}S), a new problem class in simulation optimization where the objective is to select a simulation alternative whose mean performance matches a prespecified target as closely as possible. T{\&}S resembles the more well-known problem of ranking and selection but presents unexpected challenges: for example, a one-step look-ahead method may produce statistically inconsistent estimates of the values, even under very standard normality assumptions. We create a new and fundamentally different approach, based on a Brownian local time model, that exhibits characteristics of two widely studied methodologies, namely expected value of information and optimal computing budget allocation. We characterize the asymptotic sampling rates of this method and relate them to the convergence rates of metrics of interest. The local time method outperforms benchmarks in experiments, including problems where the modeling assumptions of T{\&}S are violated.},
author = {Ryzhov, Ilya O.},
doi = {10.1287/opre.2018.1731},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ryzhov - 2018 - The local time method for targeting and selection.pdf:pdf},
issn = {15265463},
journal = {Operations Research},
keywords = {Brownian local time,Optimal learning,Simulation calibration,Simulation optimization},
number = {5},
pages = {1406--1422},
title = {{The local time method for targeting and selection}},
volume = {66},
year = {2018}
}
@article{Wu2017b,
abstract = {A large body of recent work focuses on methods for extracting low-dimensional latent structure from multi-neuron spike train data. Most such methods employ either linear latent dynamics or linear mappings from latent space to log spike rates. Here we propose a doubly nonlinear latent variable model that can identify low-dimensional structure underlying apparently high-dimensional spike train data. We introduce the Poisson Gaussian-Process Latent Variable Model (P-GPLVM), which consists of Poisson spiking observations and two underlying Gaussian processes—one governing a temporal latent variable and another governing a set of nonlinear tuning curves. The use of nonlinear tuning curves enables discovery of low-dimensional latent structure even when spike responses exhibit high linear dimensionality (e.g., as found in hippocampal place cell codes). To learn the model from data, we introduce the decoupled Laplace approximation, a fast approxi-mate inference method that allows us to efficiently optimize the latent path while marginalizing over tuning curves. We show that this method outperforms previous Laplace-approximation-based inference methods in both the speed of convergence and accuracy. We apply the model to spike trains recorded from hippocampal place cells and show that it compares favorably to a variety of previous methods for latent structure discovery, including variational auto-encoder (VAE) based methods that parametrize the nonlinear mapping from latent space to spike rates with a deep neural network.},
author = {Wu, Anqi and Roy, Nicholas A and Keeley, Stephen and Pillow, Jonathan W},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2017 - Gaussian process based nonlinear latent structure discovery in multivariate spike train data.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems},
number = {Nips},
pages = {5--8},
title = {{Gaussian process based nonlinear latent structure discovery in multivariate spike train data}},
url = {http://papers.nips.cc/paper/6941-gaussian-process-based-nonlinear-latent-structure-discovery-in-multivariate-spike-train-data.pdf},
year = {2017}
}
@article{Chung2013,
author = {Chung, Yeojin and Rabe-Hesketh, Sophia and Dorie, Vincent and Gelman, Andrew and Liu, Jingchen},
doi = {10.1007/s11336-013-9328-2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2013 - A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models.pdf:pdf},
issn = {0033-3123},
journal = {Psychometrika},
keywords = {hierarchical linear model,multilevel model,penalized likelihood,variance},
month = {oct},
number = {4},
pages = {685--709},
title = {{A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models}},
url = {http://www.stat.columbia.edu/{~}gelman/research/unpublished/avoid.pdf http://link.springer.com/10.1007/s11336-013-9328-2},
volume = {78},
year = {2013}
}
@article{Laptev,
abstract = {Validating and testing a machine learning model is a critical stage in model development. For time-series anomaly detection , validation and testing is challenging because of the lack of labeled data and the difficulty of generating a realistic time-series with anomalies. Motivated by the continued success of Variational Auto-Encoders (VAE) and Generative Adversarial Networks (GANs) to produce realistic-looking data we provide a platform to generate a realistic time-series with anomalies called AnoGen. Our contribution includes a sampling technique that allows us to sample from the latent z space of a trained variational auto-encoder to determinis-tically generate a realistic time-series with anomalies.},
author = {Laptev, Nikolay},
doi = {10.475/123},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Laptev - Unknown - AnoGen Deep Anomaly Generator.pdf:pdf},
isbn = {1234567245},
pages = {4--6},
title = {{AnoGen: Deep Anomaly Generator}},
url = {http://www.andrew.cmu.edu/user/lakoglu/odd/accepted{\_}papers/ODD{\_}v50{\_}paper{\_}1.pdf}
}
@article{Song2017,
abstract = {{\textcopyright} 2017 IEEE. Data from real applications involve multiple modalities representing content with the same semantics from complementary aspects. However, relations among heterogeneous modalities are simply treated as observation-to-fit by existing work, and the parameterized modality specific mapping functions lack flexibility in directly adapting to the content divergence and semantic complicacy in multimodal data. In this paper, we build our work based on the Gaussian process latent variable model (GPLVM) to learn the non-parametric mapping functions and transform heterogeneous modalities into a shared latent space. We propose multimodal Similarity Gaussian Process latent variable model (m-SimGP), which learns the mapping functions between the intra-modal similarities and latent representation. We further propose multimodal distance-preserved similarity GPLVM (m-DSimGP) to preserve the intra-modal global similarity structure, and multimodal regularized similarity GPLVM (m-RSimGP) by encouraging similar/dissimilar points to be similar/dissimilar in the latent space. We propose m-DRSimGP, which combines the distance preservation in m-DSimGP and semantic preservation in m-RSimGP to learn the latent representation. The overall objective functions of the four models are solved by simple and scalable gradient decent techniques. They can be applied to various tasks to discover the nonlinear correlations and to obtain the comparable low-dimensional representation for heterogeneous modalities. On five widely used real-world data sets, our approaches outperform existing models on cross-modal content retrieval and multimodal classification.},
author = {Song, Guoli and Wang, Shuhui and Huang, Qingming and Tian, Qi},
doi = {10.1109/TIP.2017.2713045},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Song et al. - 2017 - Multimodal Similarity Gaussian Process Latent Variable Model.pdf:pdf},
isbn = {6162010600},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Gaussian processes,Multimodal learning,similarity preservation},
number = {9},
pages = {4168--4181},
title = {{Multimodal Similarity Gaussian Process Latent Variable Model}},
volume = {26},
year = {2017}
}
@book{Hackbush2014,
abstract = {Complete book, all about tensors},
author = {Hackbush, Wolfgang - (Max-Planck-Institute for Mathematics in the Sciences)},
booktitle = {Springer},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hackbush - 2014 - Tensor spaces and numerical Tensor calculus.pdf:pdf},
keywords = {high resolution images,research,risks management,sustainable reconstruction},
number = {1},
pages = {1--5},
title = {{Tensor spaces and numerical Tensor calculus}},
year = {2014}
}
@article{Lancewicki,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.06156v1},
author = {Lancewicki, Tomer},
eprint = {arXiv:1707.06156v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lancewicki - Unknown - Regularization of the Kernel Matrix via Covariance Matrix Shrinkage Estimation.pdf:pdf},
title = {{Regularization of the Kernel Matrix via Covariance Matrix Shrinkage Estimation}}
}
@article{Dong2017,
abstract = {For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an {\$}n \backslashtimes n{\$} positive definite matrix, and its derivatives - leading to prohibitive {\$}\backslashmathcal{\{}O{\}}(n{\^{}}3){\$} computations. We propose novel {\$}\backslashmathcal{\{}O{\}}(n){\$} approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra. We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels.},
archivePrefix = {arXiv},
arxivId = {1711.03481},
author = {Dong, Kun and Eriksson, David and Nickisch, Hannes and Bindel, David and Wilson, Andrew Gordon},
eprint = {1711.03481},
issn = {10495258},
title = {{Scalable Log Determinants for Gaussian Process Kernel Learning}},
year = {2017}
}
@article{Paisley2015,
abstract = {We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP is a generalization of the nested Chinese restaurant process (nCRP) that allows each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation of the nCRP, allowing a document to more easily express thematic borrowings as a random effect. We derive a stochastic variational inference algorithm for the model, in addition to a greedy subtree selection method for each document, which allows for efficient inference using massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 3.3 million documents from Wikipedia.},
author = {Paisley, John and Wang, Chong and Blei, David M. and Jordan, Michael I.},
doi = {10.1109/TPAMI.2014.2318728},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
title = {{Nested hierarchical dirichlet processes}},
volume = {37},
year = {2015}
}
@article{Rabinowitz2018,
abstract = {Theory of mind (ToM; Premack {\&} Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer {\&} Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
archivePrefix = {arXiv},
arxivId = {1802.07740},
author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
eprint = {1802.07740},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf:pdf},
issn = {1938-7228},
title = {{Machine Theory of Mind}},
url = {http://arxiv.org/abs/1802.07740},
year = {2018}
}
@article{Chen2018b,
abstract = {In transfer learning, one seeks to transfer related infor-mation from source tasks with sufficient data to help with the learning of target task with only limited data. In this paper, we propose a novel Coupled End-to-end Transfer Learning (CETL) framework, which mainly consists of two convolu-tional neural networks (source and target) that connect to a shared decoder. A novel loss function, the coupled loss, is used for CETL training. From a theoretical perspective, we demonstrate the rationale of the coupled loss by establish-ing a learning bound for CETL. Moreover, we introduce the generalized Fisher information to improve multi-task opti-mization in CETL. From a practical aspect, CETL provides a unified and highly flexible solution for various learning tasks such as domain adaption and knowledge distillation. Empirical result shows the superior performance of CETL on cross-domain and cross-task image classification.},
author = {Chen, Shixing and Zhang, Caojin and Dong, Ming},
doi = {10.1109/CVPR.2018.00455},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Zhang, Dong - 2018 - Coupled End-to-End Transfer Learning with Generalized Fisher Information.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4329--4338},
title = {{Coupled End-to-End Transfer Learning with Generalized Fisher Information}},
year = {2018}
}
@article{Geisler1989,
abstract = {Visual stimuli contain a limited amount of information that could potentially be used to perform a given visual task. At successive stages of visual processing, some of this information is lost and some is transmitted to higher stages. This article describes a new analysis, based on the concept of the ideal observer in signal detection theory, that allows one to trace the flow of discrimination information through the initial physiological stages of visual processing, for arbitrary spatio-chromatic stimuli. This ideal-observer analysis provides a rigorous means of measuring the information content of visual stimuli and of assessing the contribution of specific physiological mechanisms to discrimination performance. Here, the analysis is developed for the physiological mechanisms up to the level of the photoreceptor. It is shown that many psychophysical phenomena previously attributed to neural mechanisms may be explained by variations in the information content of the stimuli and by preneural mechanisms.},
author = {Geisler, Wilson S.},
doi = {10.1037/0033-295X.96.2.267},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Geisler - 1989 - Sequential Ideal-Observer Analysis of Visual Discriminations.pdf:pdf},
issn = {0033295X},
journal = {Psychological Review},
number = {2},
pages = {267--314},
pmid = {2652171},
title = {{Sequential Ideal-Observer Analysis of Visual Discriminations}},
volume = {96},
year = {1989}
}
@article{Hensman2015,
abstract = {In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference.},
archivePrefix = {arXiv},
arxivId = {1401.1605},
author = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
doi = {10.1109/TPAMI.2014.2318711},
eprint = {1401.1605},
isbn = {0162-8828 VO - 37},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pmid = {26353249},
title = {{Fast nonparametric clustering of structured time-series}},
volume = {37},
year = {2015}
}
@article{Ranganathan2011,
abstract = {We present a new Gaussian process (GP) inference algorithm, called online sparse matrix Gaussian processes (OSMGP), and demonstrate its merits by applying it to the problems of head pose estimation and visual tracking. The OSMGP is based upon the observation that for kernels with local support, the Gram matrix is typically sparse. Maintaining and updating the sparse Cholesky factor of the Gram matrix can be done efficiently using Givens rotations. This leads to an exact, online algorithm whose update time scales linearly with the size of the Gram matrix. Further, we provide a method for constant time operation of the OSMGP using matrix downdates. The downdates maintain the Cholesky factor at a constant size by removing certain rows and columns corresponding to discarded training examples. We demonstrate that, using these matrix downdates, online hyperparameter estimation can be included at cost linear in the number of total training examples. We describe a robust appearance-based head pose estimation system based upon the OSMGP. Numerous experiments and comparisons with existing methods using a large dataset system demonstrate the efficiency and accuracy of our system. Further, to showcase the applicability of OSMGP to a wide variety of problems, we also describe a regression-based visual tracking method. Experiments show that our OSMGP algorithm generalizes well using online learning.},
author = {Ranganathan, Ananth and Yang, Ming Hsuan and Ho, Jeffrey},
doi = {10.1109/TIP.2010.2066984},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ranganathan, Yang, Ho - 2011 - Online sparse gaussian process regression and its applications.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Gaussian process,matrix algebra,online algorithm,pose estimation,visual tracking},
number = {2},
pages = {391--404},
title = {{Online sparse gaussian process regression and its applications}},
volume = {20},
year = {2011}
}
@article{Wahlstrom2015,
abstract = {Modeling dynamical systems is important in many disciplines, such as control, robotics, or neurotechnology. Commonly the state of these systems is not directly observed, but only available through noisy and potentially high-dimensional observations. In these cases, system identification, i.e., finding the measurement mapping and the transition mapping (system dynamics) in latent space can be challenging. For linear system dynamics and measurement mappings efficient solutions for system identification are available. However, in practical applications, the linearity assumptions does not hold, requiring nonlinear system identification techniques. If additionally the observations are high-dimensional (e.g., images), nonlinear system identification is inherently hard. To address the problem of nonlinear system identification from high-dimensional observations, we combine recent advances in deep learning and system identification. In particular, we jointly learn a low-dimensional embedding of the observation by means of deep auto-encoders and a predictive transition model in this low-dimensional space. We demonstrate that our model enables learning good predictive models of dynamical systems from pixel information only.},
archivePrefix = {arXiv},
arxivId = {1410.7550},
author = {Wahlstr{\"{o}}m, Niklas and Sch{\"{o}}n, Thomas B. and Deisenroth, Marc Peter},
doi = {10.1016/j.ifacol.2015.12.271},
eprint = {1410.7550},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wahlstr{\"{o}}m, Sch{\"{o}}n, Deisenroth - 2015 - Learning deep dynamical models from image pixels.pdf:pdf},
isbn = {0309-1317},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Deep neural networks,auto-encoder,low-dimensional embedding,nonlinear systems,system identification},
number = {28},
pages = {1059--1064},
title = {{Learning deep dynamical models from image pixels}},
volume = {48},
year = {2015}
}
@article{Yang2017b,
abstract = {Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
archivePrefix = {arXiv},
arxivId = {1702.08139},
author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
doi = {10.1016/j.jamcollsurg.2017.01.041},
eprint = {1702.08139},
isbn = {9781510855144},
issn = {10727515},
pmid = {28137536},
title = {{Improved Variational Autoencoders for Text Modeling using Dilated Convolutions}},
year = {2017}
}
@article{Srivastava2014,
abstract = {We study collective decision-making in a model of human groups, with network interactions, performing two alternative choice tasks. We focus on the speed-accuracy tradeoff, i.e., the tradeoff between a quick decision and a reliable decision, for individuals in the network. We model the evidence aggregation process across the network using a coupled drift diffusion model (DDM) and consider the free response paradigm in which individuals take their time to make the decision. We develop reduced DDMs as decoupled approximations to the coupled DDM and characterize their efficiency. We determine high probability bounds on the error rate and the expected decision time for the reduced DDM. We show the effect of the decision-maker's location in the network on their decision-making performance under several threshold selection criteria. Finally, we extend the coupled DDM to the coupled Ornstein-Uhlenbeck model for decision-making in two alternative choice tasks with recency effects, and to the coupled race model for decision-making in multiple alternative choice tasks.},
archivePrefix = {arXiv},
arxivId = {1402.3634},
author = {Srivastava, Vaibhav and Leonard, Naomi Ehrich},
eprint = {1402.3634},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava, Leonard - 2014 - Collective Decision-Making in Ideal Networks The Speed-Accuracy Tradeoff.pdf:pdf},
month = {feb},
pages = {1--12},
title = {{Collective Decision-Making in Ideal Networks: The Speed-Accuracy Tradeoff}},
url = {http://arxiv.org/abs/1402.3634},
year = {2014}
}
@article{Piantadosi2016,
abstract = {It is well-known in numerical cognition that higher numbers are represented with less absolute fidelity than lower numbers, often formalized as a logarithmic mapping. Previous derivations of this psychological law have worked by assuming that relative change in physical magnitude is the key psychologically-relevant measure (Fechner, 1860; Sun et al., 2012; Portugal {\&} Svaiter, Minds and Machines, 21(1), 73–81, 2011). Ideally, however, this property of psychological scales would be derived from more general, independent principles. This paper shows that a logarithmic number line is the one which minimizes the error between input and representation relative to the probability that subjects would need to represent each number. This need probability is measured here through natural language and matches the form of need probabilities found in other literatures. The derivation does not presuppose anything like Weber's law and makes minimal assumptions about both the nature of internal representations and the form of the mapping. More generally, the results prove in a general setting that the optimal psychological scale will change with the square root of the probability of each input. For stimuli that follow a power-law need distribution this approach recovers either a logarithmic or power-law psychophysical mapping (Stevens, 1957, 1961, 1975).},
author = {Piantadosi, Steven T.},
doi = {10.3758/s13423-015-0963-8},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Piantadosi - 2016 - A rational analysis of the approximate number system.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Number frequency,Psychophysical law,Weber's law},
number = {3},
pages = {877--886},
title = {{A rational analysis of the approximate number system}},
volume = {23},
year = {2016}
}
@article{Veksler2016,
abstract = {A driver for achieving human-level AI and high-fidelity cognitive architectures is the ability to easily test and compare the performance and behavior of computational agents/models to humans and to one another. One major difficulty in setting up and getting participation in large-scale cognitive decathlon and grand challenge competitions, or even smaller scale cross-framework evaluation and Turing testing, is that there is no standard interface protocol that enables and facilitates human and computational agent "plug-and-play" participation across various tasks. We identify three major issues. First, human-readable task interfaces aren't often translated into machine-readable form. Second, in the cases where a task interface is made available in a machine-readable protocol, the protocol is often task-specific, and differs from other task protocols. Finally, where both human and machine-readable versions of the task interface exist, the two versions often differ in content. This makes the bar of entry extremely high for comparison of humans and multiple computational frameworks across multiple tasks. This paper proposes a standard approach to task design where all task interactions adhere to a standard API. We provide examples of how this method can be employed to gather human and computational simulation data in text-and-button tasks, visual and animated tasks, and in real-time robotics tasks.},
author = {Veksler, Vladislav D. and Buchler, Norbou and Lebiere, Christian and Morrison, Don and Kelley, Troy},
doi = {10.1016/j.bica.2016.10.003},
issn = {2212683X},
journal = {Biologically Inspired Cognitive Architectures},
title = {{The performance comparison problem: Universal task access for cross-framework evaluation, Turing tests, grand challenges, and cognitive decathlons}},
volume = {18},
year = {2016}
}
@inproceedings{Jiang2017,
abstract = {Clustering is among the most fundamental tasks in computer vision and machine learning. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Variational Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into observables. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameterization trick. Quantitative comparisons with strong baselines are included in this paper, and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 4 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training. Lastly, VaDE is a flexible and extensible framework for unsupervised generative clustering, more general mixture models than GMM can be easily plugged in.},
archivePrefix = {arXiv},
arxivId = {1611.05148},
author = {Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/273},
eprint = {1611.05148},
isbn = {9780999241103},
issn = {10450823},
pmid = {25275695},
title = {{Variational deep embedding: An unsupervised generative approach to Clustering}},
year = {2017}
}
@article{Stanislaw1999,
author = {Stanislaw, Harold and Todorov, Natasha},
doi = {10.3758/BF03207704},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Stanislaw, Todorov - 1999 - Calculation of signal detection theory measures.pdf:pdf},
isbn = {0743-3808 (Print) 0743-3808 (Linking)},
issn = {0743-3808},
journal = {Behavior Research Methods, Instruments, {\&} Computers},
month = {mar},
number = {1},
pages = {137--149},
pmid = {10495845},
title = {{Calculation of signal detection theory measures}},
url = {http://link.springer.com/10.3758/BF03207704},
volume = {31},
year = {1999}
}
@article{Savva2017,
abstract = {We present MINOS, a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. The simulator leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites. We use MINOS to benchmark deep-learning-based navigation methods, to analyze the influence of environmental complexity on navigation performance, and to carry out a controlled study of multimodality in sensorimotor learning. The experiments show that current deep reinforcement learning approaches fail in large realistic environments. The experiments also indicate that multimodality is beneficial in learning to navigate cluttered scenes. MINOS is released open-source to the research community at http://minosworld.org . A video that shows MINOS can be found at https://youtu.be/c0mL9K64q84},
archivePrefix = {arXiv},
arxivId = {1712.03931},
author = {Savva, Manolis and Chang, Angel X. and Dosovitskiy, Alexey and Funkhouser, Thomas and Koltun, Vladlen},
eprint = {1712.03931},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Savva et al. - 2017 - MINOS Multimodal Indoor Simulator for Navigation in Complex Environments.pdf:pdf},
pages = {1--14},
title = {{MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments}},
url = {http://arxiv.org/abs/1712.03931},
year = {2017}
}
@article{McCoy2018,
abstract = {Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag-of-words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations.},
archivePrefix = {arXiv},
arxivId = {1812.08718},
author = {McCoy, R. Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
eprint = {1812.08718},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/McCoy et al. - 2018 - RNNs Implicitly Implement Tensor Product Representations.pdf:pdf},
pages = {1--21},
title = {{RNNs Implicitly Implement Tensor Product Representations}},
url = {http://arxiv.org/abs/1812.08718},
year = {2018}
}
@book{Fechner1966,
address = {New York},
author = {Fechner, G},
edition = {Translated},
publisher = {Rinehart and Winston},
title = {{Elements of Psychophysics, Vol. 1}},
year = {1966}
}
@article{Wilson2012,
abstract = {I define a process over all stationary covariance kernels. I show how one might be able to perform inference that scales as O(nm 2) in a GP regression model using this process as a prior over the covariance kernel, with n datapoints and m {\textless} n. I also show how the stationarity assumption can be relaxed.},
author = {Wilson, Andrew Gordon},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilson - 2012 - A Process over all Stationary Covariance Kernels.pdf:pdf},
pages = {1--6},
title = {{A Process over all Stationary Covariance Kernels}},
year = {2012}
}
@article{Singh2004,
abstract = {Modeling dynamical systems, both for con-trol purposes and to make predictions about their behavior, is ubiquitous in science and engineering. Predictive state representations (PSRs) are a recently introduced class of models for discrete-time dynamical systems. The key idea behind PSRs and the closely related OOMs (Jaeger's observable operator models) is to represent the state of the sys-tem as a set of predictions of observable outcomes of experiments one can do in the system. This makes PSRs rather different from history-based models such as n th -order Markov models and hidden-state-based mod-els such as HMMs and POMDPs. We in-troduce an interesting construct, the system-dynamics matrix, and show how PSRs can be derived simply from it. We also use this con-struct to show formally that PSRs are more general than both n th -order Markov models and HMMs/POMDPs. Finally, we discuss the main difference between PSRs and OOMs and conclude with directions for future work.},
archivePrefix = {arXiv},
arxivId = {1207.4167},
author = {Singh, Satinder P and James, Michael R and Rudary, Matthew R},
eprint = {1207.4167},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Singh, James, Rudary - 2004 - Predictive State Pepresentations a New Theory for Modeling Dynamical Systems.pdf:pdf},
isbn = {0974903906},
journal = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI)},
pages = {512--519},
title = {{Predictive State Pepresentations: a New Theory for Modeling Dynamical Systems}},
url = {http://www.cc.gatech.edu/{~}isbell/reading/papers/uai2004psr.pdf},
year = {2004}
}
@article{Chen2018a,
abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
archivePrefix = {arXiv},
arxivId = {1806.07366},
author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
doi = {arXiv:1806.07366v3},
eprint = {1806.07366},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf:pdf},
isbn = {9781139108188},
issn = {0140-525X},
number = {NeurIPS},
pmid = {1000303116},
title = {{Neural Ordinary Differential Equations}},
url = {http://arxiv.org/abs/1806.07366},
year = {2018}
}
@inproceedings{Hansen2004,
author = {Hansen, Eric and Bernstein, Daniel and Zilberstein, Shlomo},
booktitle = {Nineteenth Conference on Artificial Conference (AAAI)},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hansen, Bernstein, Zilberstein - 2004 - Dynamic Programming for Partially Observable Stochastic Games.pdf:pdf},
pages = {709--715},
title = {{Dynamic Programming for Partially Observable Stochastic Games}},
year = {2004}
}
@article{Kontsevich1999,
abstract = {We introduce a new Bayesian adaptive method for acquisition of both threshold and slope of the psychometric function. The method updates posterior probabilities in the two-dimensional parameter space of psychometric functions and makes predictions based on the expected mean threshold and slope values. On each trial it sets the stimulus intensity that maximizes the expected information to be gained by completion of that trial. The method was evaluated in computer simulations and in a psychophysical experiment using the two-alternative forced-choice (2AFC) paradigm. Threshold estimation within 2 dB (23{\%}) precision requires less than 30 trials for a typical 2AFC detection task. To get the slope estimate with the same precision takes about 300 trials.},
author = {Kontsevich, Leonid L. and Tyler, Christopher W.},
doi = {10.1016/S0042-6989(98)00285-5},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kontsevich, Tyler - 1999 - Bayesian adaptive estimation of psychometric slope and threshold.pdf:pdf},
issn = {00426989},
journal = {Vision Research},
keywords = {Bayesian adaptive method,Psychometric function,Two-alternative forced-choice},
number = {16},
pages = {2729--2737},
pmid = {10492833},
title = {{Bayesian adaptive estimation of psychometric slope and threshold}},
volume = {39},
year = {1999}
}
@article{Frund2011,
abstract = {Measuring sensitivity is at the heart of psychophysics. Often, sensitivity is derived from estimates of the psychometric function. This function relates response probability to stimulus intensity. In estimating these response probabilities, most studies assume stationary observers: Responses are expected to be dependent only on the intensity of a presented stimulus and not on other factors such as stimulus sequence, duration of the experiment, or the responses on previous trials. Unfortunately, a number of factors such as learning, fatigue, or fluctuations in attention and motivation will typically result in violations of this assumption. The severity of these violations is yet unknown. We use Monte Carlo simulations to show that violations of these assumptions can result in underestimation of confidence intervals for parameters of the psychometric function. Even worse, collecting more trials does not eliminate this misestimation of confidence intervals. We present a simple adjustment of the confidence intervals that corrects for the underestimation almost independently of the number of trials and the particular type of violation. {\textcopyright} ARVO.},
author = {Fr{\"{u}}nd, Ingo and Haenel, N. Valentin and Wichmann, Felix A.},
doi = {10.1167/11.6.16},
file = {:Users/mshvarts/Downloads/jov-11-6-16.pdf:pdf},
issn = {15347362},
journal = {Journal of Vision},
keywords = {Inference,Nonstationary behavior,Psychometric function},
number = {6},
pages = {1--19},
pmid = {21606382},
title = {{Inference for psychometric functions in the presence of nonstationary behavior}},
volume = {11},
year = {2011}
}
@article{Sinha2015,
author = {Sinha, Subhrajit and Vaidya, Umesh},
doi = {10.1109/CDC.2015.7403119},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sinha, Vaidya - 2015 - Formalism for information transfer in dynamical network.pdf:pdf},
isbn = {9781479978861},
issn = {07431546},
journal = {Proceedings of the IEEE Conference on Decision and Control},
keywords = {Covariance matrices,Density functional theory,Entropy,Mathematical model,Probability density function,Random variables,Uncertainty},
number = {Cdc},
pages = {5731--5736},
title = {{Formalism for information transfer in dynamical network}},
volume = {54rd IEEE },
year = {2015}
}
@inproceedings{Schlichtkrull2018,
abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8{\%} on FB15k-237 over a decoder-only baseline.},
author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van den Berg, Rianne and Titov, Ivan and Welling, Max},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-93417-4_38},
isbn = {9783319934167},
issn = {16113349},
title = {{Modeling Relational Data with Graph Convolutional Networks}},
volume = {10843 LNCS},
year = {2018}
}
@article{Ezzyat2018,
abstract = {{\textcopyright} 2018 The Author(s). Memory failures are frustrating and often the result of ineffective encoding. One approach to improving memory outcomes is through direct modulation of brain activity with electrical stimulation. Previous efforts, however, have reported inconsistent effects when using open-loop stimulation and often target the hippocampus and medial temporal lobes. Here we use a closed-loop system to monitor and decode neural activity from direct brain recordings in humans. We apply targeted stimulation to lateral temporal cortex and report that this stimulation rescues periods of poor memory encoding. This system also improves later recall, revealing that the lateral temporal cortex is a reliable target for memory enhancement. Taken together, our results suggest that such systems may provide a therapeutic approach for treating memory dysfunction.},
author = {Ezzyat, Youssef and Wanda, Paul A. and Levy, Deborah F. and Kadel, Allison and Aka, Ada and Pedisich, Isaac and Sperling, Michael R. and Sharan, Ashwini D. and Lega, Bradley C. and Burks, Alexis and Gross, Robert E. and Inman, Cory S. and Jobst, Barbara C. and Gorenstein, Mark A. and Davis, Kathryn A. and Worrell, Gregory A. and Kucewicz, Michal T. and Stein, Joel M. and Gorniak, Richard and Das, Sandhitsu R. and Rizzuto, Daniel S. and Kahana, Michael J.},
doi = {10.1038/s41467-017-02753-0},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ezzyat et al. - 2018 - Closed-loop stimulation of temporal cortex rescues functional networks and improves memory.pdf:pdf},
issn = {20411723},
journal = {Nature Communications},
number = {1},
title = {{Closed-loop stimulation of temporal cortex rescues functional networks and improves memory}},
volume = {9},
year = {2018}
}
@phdthesis{Chow2017,
author = {Chow, Michael Anthony},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chow - 2017 - Decoding Three Categories of Conventional Wisdom in Pattern Similarity Analyses.pdf:pdf},
number = {June},
title = {{Decoding Three Categories of Conventional Wisdom in Pattern Similarity Analyses}},
year = {2017}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and de Freitas, Nando},
doi = {10.1007/s10115-008-0151-5},
eprint = {1606.04474},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient descent.pdf:pdf},
isbn = {1011500801515},
issn = {0219-1377},
number = {Nips},
pages = {1--17},
pmid = {207591},
title = {{Learning to learn by gradient descent by gradient descent}},
url = {http://arxiv.org/abs/1606.04474},
year = {2016}
}
@incollection{Fruehwirt2017,
address = {Cham},
author = {Fruehwirt, Wolfgang and Zhang, Pengfei and Gerstgrasser, Matthias and Grossegger, Dieter and Schmidt, Reinhold and Benke, Thomas and Dal-Bianco, Peter and Ransmayr, Gerhard and Weydemann, Leonard and Garn, Heinrich and Waser, Markus and Osborne, Michael and Dorffner, Georg},
doi = {10.1007/978-3-319-59758-4_7},
editor = {ten Teije, Annette and Popow, Christian and Holmes, John H. and Sacchi, Lucia},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Fruehwirt et al. - 2017 - Bayesian Gaussian Process Classification from Event-Related Brain Potentials in Alzheimer's Disease.pdf:pdf},
isbn = {978-3-319-59757-7},
pages = {65--75},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Bayesian Gaussian Process Classification from Event-Related Brain Potentials in Alzheimer's Disease}},
url = {http://link.springer.com/10.1007/978-3-319-59758-4 http://link.springer.com/10.1007/978-3-319-59758-4{\_}7},
volume = {10259},
year = {2017}
}
@article{Turner2017,
abstract = {A growing number of researchers have advocated for the advancement of cognitive neuroscience by blending cognitive models with neurophysiology. The recently proposed joint modeling framework is one way to bridge the gap between the abstractions assumed by cognitive models and the neurophysiology obtained by modern methods in neuroscience. Despite this advancement, the current method for linking the two domains is hindered by the dimensionality of the neural data. In this article, we present a new linking function based on factor analysis that allows joint models to grow linearly in complexity with increases in the number of neural features. The new linking function is then evaluated in two simulation studies. The first simulation study shows how the model parameters can be accurately recovered when there are many neural features, that mimics real-world applications. The second simulation shows how the new linking function can (1) properly recover a representation of the data generating model, even in the case of model misspecification, and (2) outperform the previous linking function in a cross-validation test. We close by applying a model equipped with the new linking function to real-world data from a perceptual decision making task. The model allows us to understand how differences in the model parameters emerge as a function of differences in brain function across speed and accuracy instruction.},
author = {Turner, Brandon M. and Wang, Ting and Merkle, Edgar C.},
doi = {10.1016/j.neuroimage.2017.03.044},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Turner, Wang, Merkle - 2017 - Factor analysis linking functions for simultaneously modeling neural and behavioral data.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Factor analysis,Joint modeling,Neural drift diffusion model},
number = {July 2016},
pages = {28--48},
publisher = {Elsevier},
title = {{Factor analysis linking functions for simultaneously modeling neural and behavioral data}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2017.03.044},
volume = {153},
year = {2017}
}
@article{Stoudenmire2016,
abstract = {Tensor networks are approximations of high-order tensors which are efficient to work with and have been very successful for physics and mathematics applications. We demonstrate how algorithms for optimizing tensor networks can be adapted to supervised learning tasks by using matrix product states (tensor trains) to parameterize non-linear kernel learning models. For the MNIST data set we obtain less than 1{\%} test set classification error. We discuss an interpretation of the additional structure imparted by the tensor network to the learned model.},
author = {Stoudenmire, E. M. and Schwab, David J.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Stoudenmire, Schwab - 2016 - Supervised learning with tensor networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {4806--4814},
title = {{Supervised learning with tensor networks}},
year = {2016}
}
@article{Jones2016,
author = {Jones, Matt and Zhang, Jun},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jones, Zhang - 2016 - Duality Between Feature and Similarity Models , Based on the Reproducing-Kernel Hilbert Space.pdf:pdf},
pages = {1--28},
title = {{Duality Between Feature and Similarity Models , Based on the Reproducing-Kernel Hilbert Space}},
year = {2016}
}
@article{Amodei2016,
abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
archivePrefix = {arXiv},
arxivId = {1606.06565},
author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'{e}}, Dan},
doi = {1606.06565},
eprint = {1606.06565},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:pdf},
isbn = {0387310738},
issn = {00210552},
month = {jun},
pages = {1--29},
title = {{Concrete Problems in AI Safety}},
url = {http://arxiv.org/abs/1606.06565},
year = {2016}
}
@inproceedings{Liang2018,
abstract = {Autonomous urban driving navigation with complex multi-agent dynamics is under-explored due to the difficulty of learning an optimal driving policy. The traditional modular pipeline heavily relies on hand-designed rules and the pre-processing perception system while the supervised learning-based models are limited by the accessibility of extensive human experience. We present a general and principled Controllable Imitative Reinforcement Learning (CIRL) approach which successfully makes the driving agent achieve higher success rates based on only vision inputs in a high-fidelity car simulator. To alleviate the low exploration efficiency for large continuous action space that often prohibits the use of classical RL on challenging real tasks, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover, we propose to specialize adaptive policies and steering-angle reward designs for different control signals (i.e. follow, straight, turn right, turn left) based on the shared representations to improve the model capability in tackling with diverse cases. Extensive experiments on CARLA driving benchmark demonstrate that CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on a variety of goal-directed driving tasks. We also show its superior generalization capability in unseen environments. To our knowledge, this is the first successful case of the learned driving policy through reinforcement learning in the high-fidelity simulator, which performs better-than supervised imitation learning.},
archivePrefix = {arXiv},
arxivId = {1807.03776},
author = {Liang, Xiaodan and Wang, Tairui and Yang, Luona and Xing, Eric},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01234-2_36},
eprint = {1807.03776},
isbn = {9783030012335},
issn = {16113349},
keywords = {Autonomous driving,Imitative reinforcement learning},
title = {{CIRL: Controllable imitative reinforcement learning for vision-based self-driving}},
year = {2018}
}
@article{Sense2018,
abstract = {Technological developments have spawned a range of educational software that strives to enhance learning through personalized adaptation. The success of these systems depends on how accurate the knowledge state of individual learners is modeled over time. Computer scientists have been at the forefront of development for these kinds of distributed learning systems and have primarily relied on data-driven algorithms to trace knowledge acquisition in noisy and complex learning domains. Meanwhile, research psychologists have primarily relied on data collected in controlled laboratory settings to develop and validate theory-driven computational models, but have not devoted much exploration to learning in naturalistic environments. The two fields have largely operated in parallel despite considerable overlap in goals. We argue that mutual benefits would result from identifying and implementing more accurate methods to model the temporal dynamics of learning and forgetting for individual learners. Here we discuss recent efforts in developing adaptive learning technologies to highlight the strengths and weaknesses inherent in the typical approaches of both fields. We argue that a closer collaboration between the educational machine learning/data mining and cognitive psychology communities would be a productive and exciting direction for adaptive learning system application to move in.},
author = {Sense, Florian and Jastrzembski, Tiffany S and Mozer, Michael C and Krusmark, Michael and {Van Rijn}, Hedderik and Van, H Rijn@rug},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sense et al. - 2018 - Perspectives on Computational Models of Learning and Forgetting.pdf:pdf},
keywords = {computational models,educational application,forgetting,learning,naturalistic data,process models,recurrent neural networks,seriously is a productive,taking this hypothetical endeavor},
title = {{Perspectives on Computational Models of Learning and Forgetting}},
volume = {3},
year = {2018}
}
@article{Cichocki2015,
abstract = {The widespread use of multi-sensor technology and the emergence of big datasets has highlighted the limitations of standard flat-view matrix models and the necessity to move towards more versatile data analysis tools. We show that higher-order tensors (i.e., multiway arrays) enable such a fundamental paradigm shift towards models that are essentially polynomial and whose uniqueness, unlike the matrix methods, is guaranteed under verymild and natural conditions. Benefiting fromthe power ofmultilinear algebra as theirmathematical backbone, data analysis techniques using tensor decompositions are shown to have great flexibility in the choice of constraints that match data properties, and to find more general latent components in the data than matrix-based methods. A comprehensive introduction to tensor decompositions is provided from a signal processing perspective, starting from the algebraic foundations, via basic Canonical Polyadic and Tucker models, through to advanced cause-effect and multi-view data analysis schemes. We show that tensor decompositions enable natural generalizations of some commonly used signal processing paradigms, such as canonical correlation and subspace techniques, signal separation, linear regression, feature extraction and classification. We also cover computational aspects, and point out how ideas from compressed sensing and scientific computing may be used for addressing the otherwise unmanageable storage and manipulation problems associated with big datasets. The concepts are supported by illustrative real world case studies illuminating the benefits of the tensor framework, as efficient and promising tools for modern signal processing, data analysis and machine learning applications; these benefits also extend to vector/matrix data through tensorization. Keywords: ICA, NMF, CPD, Tucker decomposition, HOSVD, tensor networks, Tensor Train.},
archivePrefix = {arXiv},
arxivId = {1403.4462},
author = {Cichocki, Andrzej and Mandic, Danilo and {De Lathauwer}, Lieven and Zhou, Guoxu and Zhao, Qibin and Caiafa, Cesar and Phan, Huy Anh},
doi = {10.1109/MSP.2013.2297439},
eprint = {1403.4462},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cichocki et al. - 2015 - Tensor decompositions for signal processing applications From two-way to multiway component analysis.pdf:pdf},
isbn = {9781605585062},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
keywords = {cpd,hosvd,ica,nmf,tensor networks,tensor train,tucker decomposition},
number = {2},
pages = {145--163},
title = {{Tensor decompositions for signal processing applications: From two-way to multiway component analysis}},
volume = {32},
year = {2015}
}
@article{Chen2015,
abstract = {Multi-subject fMRI data is critical for evaluating the generality and validity of findings across subjects, and its effective utilization helps improve analysis sensitivity. We develop a shared response model for aggregating multi-subject fMRI data that accounts for different functional topographies among anatomically aligned datasets. Our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest. Furthermore, by removing the identified shared response, it allows improved de- tection of group differences. The ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fMRI studies. 1},
author = {Chen, Po-Hsuan (Cameron) and Chen, Janice and Yeshurun, Yaara and Hasson, Uri and Haxby, James and Ramadge, Peter J},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2015 - A Reduced-Dimension fMRI Shared Response Model.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems Conference (NIPS)},
number = {3},
pages = {460--468},
title = {{A Reduced-Dimension fMRI Shared Response Model}},
url = {http://papers.nips.cc/paper/5855-a-reduced-dimension-fmri-shared-response-model.pdf{\%}0Ahttp://papers.nips.cc/paper/5855-a-reduced-dimension-fmri-shared-response-model},
volume = {20},
year = {2015}
}
@article{Gneiting2011,
abstract = {Isotropic positive definite functions on spheres play important roles in spatial statistics, where they occur as the correlation functions of homogeneous random fields and star-shaped random particles. In approximation theory, strictly positive definite functions serve as radial basis functions for interpolating scattered data on spherical domains. We review characterizations of positive definite functions on spheres in terms of Gegenbauer expansions and apply them to dimension walks, where monotonicity properties of the Gegenbauer coefficients guarantee positive definiteness in higher dimensions. Subject to a natural support condition, isotropic positive definite functions on the Euclidean space {\$}\backslashmathbb{\{}R{\}}{\^{}}3{\$}, such as Askey's and Wendland's functions, allow for the direct substitution of the Euclidean distance by the great circle distance on a one-, two- or three-dimensional sphere, as opposed to the traditional approach, where the distances are transformed into each other. Completely monotone functions are positive definite on spheres of any dimension and provide rich parametric classes of such functions, including members of the powered exponential, Mat$\backslash$'{\{}e{\}}rn, generalized Cauchy and Dagum families. The sine power family permits a continuous parameterization of the roughness of the sample paths of a Gaussian process. A collection of research problems provides challenges for future work in mathematical analysis, probability theory and spatial statistics.},
archivePrefix = {arXiv},
arxivId = {1111.7077},
author = {Gneiting, Tilmann},
doi = {10.3150/12-BEJSP06},
eprint = {1111.7077},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gneiting - 2011 - Strictly and non-strictly positive definite functions on spheres.pdf:pdf},
issn = {1350-7265},
keywords = {completely monotone,completely monotone, covariance localization, frac,covariance localization,fractal index,interpolation of scattered data,isotropic,locally supported,multiquadric,p{\'{o}}lya criterion,radial basis function,schoenberg coefficients},
number = {4},
pages = {1327--1349},
title = {{Strictly and non-strictly positive definite functions on spheres}},
url = {http://arxiv.org/abs/1111.7077{\%}0Ahttp://dx.doi.org/10.3150/12-BEJSP06},
volume = {19},
year = {2011}
}
@article{Urtasun2007,
abstract = {Supervised learning is difficult with high dimensional input spaces and very small training sets, but accurate classification may be possible if the data lie on a low-dimensional manifold. Gaussian Process Latent Variable Models can discover low dimensional manifolds given only a small number of examples, but learn a latent space without regard for class labels. Existing methods for discriminative manifold learning (e.g., LDA, GDA) do constrain the class distribution in the latent space, but are generally deterministic and may not generalize well with limited training data. We introduce a method for Gaussian Process Classification using latent variable models trained with discriminative priors over the latent space, which can learn a discriminative latent space from a small training set.},
author = {Urtasun, Raquel and Darrell, Trevor},
doi = {10.1145/1273496.1273613},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Urtasun, Darrell - 2007 - Discriminative Gaussian process latent variable model for classification.pdf:pdf},
isbn = {978-1-59593-793-3},
journal = {ICML '07: Proceedings of the 24th international conference on Machine learning},
keywords = {dim{\_}reduction,gplvm},
pages = {927--934},
title = {{Discriminative Gaussian process latent variable model for classification}},
url = {citeulike-article-id:6539604{\%}5Cnhttp://dx.doi.org/10.1145/1273496.1273613{\%}5Cnhttp://portal.acm.org/citation.cfm?id=1273613},
year = {2007}
}
@article{Yin2010,
abstract = {OBJECTIVE To explore the characteristics of inorganic elements in Erodium stephanianum. METHOD The content of elements such as Li, B, Na, Mg, Al, Si, K, Ca, Ti, V, Cr, Mn, Fe, Ni, Cu, Zn, Ga, Br, Rb, Sr, Ba, La, Ce and Rb in ten E. stephanianum samples were determined by means of ICP/MS. The results were used for the development of element distribution diagram. The principal component analysis of SPSS and Q-type cluster analysis were applied for the study of characteristic elements in E. stephanianum. RESULT Five principal components which accounted for over 91{\%} of the total variance were extracted from the original data. The analysis results showed that Al, Ti, V, Fe, La, Ce, Li, Ga and Ba may be the characteristic elements in E. stephanianum; The results of Q-type cluster analysis showed that the samples could be clustered reasonably into two groups, and the elemental distribution characteristics were related to the ecology and origins of E. stephanianum. CONCLUSION The principal component analysis and Q-type cluster analysis could be used in data processing in inorganic elements.},
author = {Yin, Haibo and Zhang, Nan and Luo, Hong and Han, Rongchun and Li, Guoyu},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yin et al. - 2010 - Principal component analysis and cluster analysis of inorganic elements in Erodium stephanianum from different areas.pdf:pdf},
issn = {1001-5302},
journal = {Zhongguo Zhong yao za zhi = Zhongguo zhongyao zazhi = China journal of Chinese materia medica},
keywords = {density estimation,em algorithm,for personal research use,gaussian mixtures,journal of the royal,maximum-likelihood,only,principal component analysis,probabilistic principal component analysis,probability model,published as,series,statistical society,this copy is supplied},
number = {15},
pages = {1935--8},
pmid = {20931840},
title = {{[Principal component analysis and cluster analysis of inorganic elements in Erodium stephanianum from different areas].}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20931840},
volume = {35},
year = {2010}
}
@article{Brown2008a,
abstract = {We propose a linear ballistic accumulator (LBA) model of decision making and reaction time. The LBA is simpler than other models of choice response time, with independent accumulators that race towards a common response threshold. Activity in the accumulators increases in a linear and deterministic manner. The simplicity of the model allows complete analytic solutions for choices between any number of alternatives. These solutions (and freely-available computer code) make the model easy to apply to both binary and multiple choice situations. Using data from five previously published experiments, we demonstrate that the LBA model successfully accommodates empirical phenomena from binary and multiple choice tasks that have proven difficult for other theoretical accounts. Our results are encouraging in a field beset by the tradeoff between complexity and completeness. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Brown, Scott D. and Heathcote, Andrew},
doi = {10.1016/j.cogpsych.2007.12.002},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Heathcote - 2008 - The simplest complete model of choice response time Linear ballistic accumulation(2).pdf:pdf},
issn = {00100285},
journal = {Cognitive Psychology},
keywords = {Choice,Decision,Lexical decision,Mathematical models,Reaction time,Response time},
number = {3},
pages = {153--178},
title = {{The simplest complete model of choice response time: Linear ballistic accumulation}},
volume = {57},
year = {2008}
}
@article{Savva2014,
abstract = {With modern computer graphics, we can generate enormous amounts of 3D scene data. It is now possible to capture high-quality 3D representations of large real-world environments. Large shape and scene databases, such as the Trimble 3D Warehouse, are publicly accessible and constantly growing. Unfortunately, while a great amount of 3D content exists, most of it is detached from the semantics and functionality of the objects it represents. In this paper, we present a novel method to establish a correlation between the geometry and the functionality of 3D environments. Using RGB-D sensors, we capture dense 3D reconstructions of real-world scenes, and observe and track people as they interact with the environment. With these observations, we train a classifier which can transfer interaction knowledge to unobserved 3D scenes. We predict a likelihood of a given action taking place over all locations in a 3D environment and refer to this representation as an action map over the scene. We demonstrate prediction of action maps in both 3D scans and virtual scenes. We evaluate our predictions against ground truth annotations by people, and we present an approach for characterizing 3D scenes by functional similarity using action maps.},
author = {Savva, Manolis and Chang, Angel X. and Hanrahan, Pat and Fisher, Matthew and Nie{\ss}ner, Matthias},
doi = {10.1145/2661229.2661230},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Savva et al. - 2014 - SceneGrok Inferring Action Maps in 3D Environments.pdf:pdf},
isbn = {0730-0301},
issn = {0730-0301},
journal = {ACM Transactions on Graphics},
keywords = {3D scenes,3d scenes,object semantics,scene understanding},
number = {6},
pages = {212:1----212:10},
pmid = {345855600041},
title = {{SceneGrok: Inferring Action Maps in 3D Environments}},
url = {http://dl.acm.org/citation.cfm?doid=2661229.2661230{\%}5Cnhttp://dx.doi.org/10.1145/2661229.2661230{\%}5Cnhttp://graphics.stanford.edu/projects/scenegrok/},
volume = {33},
year = {2014}
}
@article{Krauth2016,
abstract = {We investigate the capabilities and limitations of Gaussian process models by jointly exploring three complementary directions: (i) scalable and statistically efficient inference; (ii) flexible kernels; and (iii) objective functions for hyperparameter learning alternative to the marginal likelihood. Our approach outperforms all previously reported GP methods on the standard MNIST dataset; performs comparatively to previous kernel-based methods using the RECTANGLES-IMAGE dataset; and breaks the 1{\%} error-rate barrier in GP models using the MNIST8M dataset, showing along the way the scalability of our method at unprecedented scale for GP models (8 million observations) in classification problems. Overall, our approach represents a significant breakthrough in kernel methods and GP models, bridging the gap between deep learning approaches and kernel machines.},
archivePrefix = {arXiv},
arxivId = {1610.05392},
author = {Krauth, Karl and Bonilla, Edwin V. and Cutajar, Kurt and Filippone, Maurizio},
eprint = {1610.05392},
title = {{AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models}},
year = {2016}
}
@article{Gal2014,
abstract = {Gaussian processes (GPs) are a powerful tool for probabilistic inference over functions. They have been applied to both regression and non-linear dimensionality reduction, and offer desirable properties such as uncertainty estimates, robustness to over-fitting, and principled ways for tuning hyper-parameters. However the scalability of these models to big datasets remains an active topic of research. We introduce a novel re-parametrisation of variational inference for sparse GP regression and latent variable models that allows for an efficient distributed algorithm. This is done by exploiting the decoupling of the data given the inducing points to re-formulate the evidence lower bound in a Map-Reduce setting. We show that the inference scales well with data and computational resources, while preserving a balanced distribution of the load among the nodes. We further demonstrate the utility in scaling Gaussian processes to big data. We show that GP performance improves with increasing amounts of data in regression (on flight data with 2 million records) and latent variable modelling (on MNIST). The results show that GPs perform better than many common models often used for big data.},
archivePrefix = {arXiv},
arxivId = {1402.1389},
author = {Gal, Yarin and van der Wilk, Mark and Rasmussen, Carl E},
doi = {10.1145/1553374.1553376},
eprint = {1402.1389},
isbn = {9781605585161},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pmid = {236331100003},
title = {{Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models}},
year = {2014}
}
@book{Stepp2013,
author = {Stepp, Herbert and Stummer, Walter},
doi = {10.1007/978-1-4614-4978-2_8},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Stepp, Stummer - 2013 - Optical Methods and Instrumentation in Brain Imaging and Therapy.pdf:pdf},
isbn = {9781461449775},
pages = {173--205},
title = {{Optical Methods and Instrumentation in Brain Imaging and Therapy}},
year = {2013}
}
@article{Meeds2015,
abstract = {In many domains, scientists build complex simulators of natural phenomena that encode their hypotheses about the underlying processes. These simulators can be deterministic or stochastic, fast or slow, constrained or unconstrained, and so on. Optimizing the simulators with respect to a set of parameter values is common practice, resulting in a single parameter setting that minimizes an objective subject to constraints. We propose a post optimization posterior analysis that computes and visualizes all the models that can generate equally good or better simulation results, subject to constraints. These optimization posteriors are desirable for a number of reasons among which easy interpretability, automatic parameter sensitivity and correlation analysis and posterior predictive analysis. We develop a new sampling framework based on approximate Bayesian computation (ABC) with one-sided kernels. In collaboration with two groups of scientists we applied POPE to two important biological simulators: a fast and stochastic simulator of stem-cell cycling and a slow and deterministic simulator of tumor growth patterns.},
archivePrefix = {arXiv},
arxivId = {1412.3051},
author = {Meeds, Edward and Chiang, Michael and Lee, Mary and Cinquin, Olivier and Lowengrub, John and Welling, Max},
doi = {10.1186/s12859-015-0658-1},
eprint = {1412.3051},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Meeds et al. - 2015 - POPE Post optimization posterior evaluation of likelihood free models.pdf:pdf},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Approximate Bayesian computation,Bayesian inference,Simulation-based science},
number = {1},
pages = {1--20},
pmid = {26289041},
publisher = {BMC Bioinformatics},
title = {{POPE: Post optimization posterior evaluation of likelihood free models}},
url = {http://dx.doi.org/10.1186/s12859-015-0658-1},
volume = {16},
year = {2015}
}
@article{Wainwright2007,
abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances - including the key problems of computing marginals and modes of probability distributions - are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms - among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations - can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
author = {Wainwright, Martin J. and Jordan, Michael I.},
doi = {10.1561/2200000001},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wainwright, Jordan - 2007 - Graphical Models, Exponential Families, and Variational Inference.pdf:pdf},
isbn = {1601981848},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1–2},
pages = {1--305},
pmid = {8812124},
title = {{Graphical Models, Exponential Families, and Variational Inference}},
url = {http://www.nowpublishers.com/article/Details/MAL-001},
volume = {1},
year = {2007}
}
@article{Kucukelbir2016,
abstract = {Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference (ADVI). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. ADVI automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. ADVI supports a broad class of models-no conjugacy assumptions are required. We study ADVI across ten different models and apply it to a dataset with millions of observations. ADVI is integrated into Stan, a probabilistic programming system; it is available for immediate use.},
archivePrefix = {arXiv},
arxivId = {1603.00788},
author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
doi = {10.3847/0004-637X/819/1/50},
eprint = {1603.00788},
isbn = {1603.00788},
issn = {15337928},
title = {{Automatic Differentiation Variational Inference}},
year = {2016}
}
@article{Furnkranz2012,
abstract = {This paper makes a first step toward the integration of two subfields of machine learning, namely preference learning and reinforcement learning (RL). An important motivation for a preference-based approach to reinforcement learning is the observation that in many real-world domains, numerical feedback signals are not readily available, or are defined arbitrarily in order to satisfy the needs of conventional RL algorithms. Instead, we propose an alternative framework for reinforcement learning, in which qualitative reward signals can be directly used by the learner. The framework may be viewed as a generalization of the conventional RL framework in which only a partial order between policies is required instead of the total order induced by their respective expected long-term reward. Therefore, building on novel methods for preference learning, our general goal is to equip the RL agent with qualitative policy models, such as ranking functions that allow for sorting its available actions from most to least promising, as well as algorithms for learning such models from qualitative feedback. As a proof of concept, we realize a first simple instantiation of this framework that defines preferences based on utilities observed for trajectories. To that end, we build on an existing method for approximate policy iteration based on roll-outs. While this approach is based on the use of classification methods for generalization and policy learning, we make use of a specific type of preference learning method called label ranking. Advantages of preference-based approximate policy iteration are illustrated by means of two case studies. {\textcopyright} 2012 The Author(s).},
author = {F{\"{u}}rnkranz, Johannes and H{\"{u}}llermeier, Eyke and Cheng, Weiwei and Park, Sang-Hyeun},
doi = {10.1007/s10994-012-5313-8},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/F{\"{u}}rnkranz et al. - 2012 - Preference-based reinforcement learning a formal framework and a policy iteration algorithm.pdf:pdf},
isbn = {1099401253138},
issn = {0885-6125},
journal = {Machine Learning},
number = {1-2},
pages = {123--156},
title = {{Preference-based reinforcement learning: a formal framework and a policy iteration algorithm}},
url = {http://link.springer.com/10.1007/s10994-012-5313-8},
volume = {89},
year = {2012}
}
@article{Liu2018b,
abstract = {Multi-output regression problems have extensively arisen in modern engineering community. This article investigates the state-of-the-art multi-output Gaussian processes (MOGPs) that can transfer the knowledge across related outputs in order to improve prediction quality. We classify existing MOGPs into two main categories as (1) symmetric MOGPs that improve the predictions for all the outputs, and (2) asymmetric MOGPs, particularly the multi-fidelity MOGPs, that focus on the improvement of high fidelity output via the useful information transferred from related low fidelity outputs. We review existing symmetric/asymmetric MOGPs and analyze their characteristics, e.g., the covariance functions (separable or non-separable), the modeling process (integrated or decomposed), the information transfer (bidirectional or unidirectional), and the hyperparameter inference (joint or separate). Besides, we assess the performance of ten representative MOGPs thoroughly on eight examples in symmetric/asymmetric scenarios by considering, e.g., different training data (heterotopic or isotopic), different training sizes (small, moderate and large), different output correlations (low or high), and different output sizes (up to four outputs). Based on the qualitative and quantitative analysis, we give some recommendations regarding the usage of MOGPs and highlight potential research directions.},
author = {Liu, Haitao and Cai, Jianfei and Ong, Yew Soon},
doi = {10.1016/j.knosys.2017.12.034},
issn = {09507051},
journal = {Knowledge-Based Systems},
title = {{Remarks on multi-output Gaussian process regression}},
volume = {144},
year = {2018}
}
@misc{Zhu2017a,
abstract = {Explosive growth in data and availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems, and applications with Big Data. Bayesian methods represent one important class of statistic methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including nonparametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications.},
author = {Zhu, Jun and Chen, Jianfei and Hu, Wenbo and Zhang, Bo},
booktitle = {National Science Review},
doi = {10.1093/nsr/nwx044},
issn = {2053714X},
number = {4},
title = {{Big Learning with Bayesian methods}},
volume = {4},
year = {2017}
}
@inproceedings{Naesseth2018,
abstract = {Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.},
archivePrefix = {arXiv},
arxivId = {1705.11140},
author = {Naesseth, Christian A. and Linderman, Scott W. and Ranganath, Rajesh and Blei, David M.},
booktitle = {International Conference on Artificial Intelligence and Statistics, AISTATS 2018},
eprint = {1705.11140},
title = {{Variational sequential Monte Carlo}},
year = {2018}
}
@article{Touati2018,
abstract = {Randomized value functions offer a promising approach towards the challenge of efficient exploration in complex environments with high dimensional state and action spaces. Unlike traditional point estimate methods, randomized value functions maintain a posterior distribution over action-space values. This prevents the agent's behavior policy from prematurely exploiting early estimates and falling into local optima. In this work, we leverage recent advances in variational Bayesian neural networks and combine these with traditional Deep Q-Networks (DQN) to achieve randomized value functions for high-dimensional domains. In particular, we augment DQN with multiplicative normalizing flows in order to track an approximate posterior distribution over its parameters. This allows the agent to perform approximate Thompson sampling in a computationally efficient manner via stochastic gradient methods. We demonstrate the benefits of our approach through an empirical comparison in high dimensional environments.},
archivePrefix = {arXiv},
arxivId = {1806.02315},
author = {Touati, Ahmed and Satija, Harsh and Romoff, Joshua and Pineau, Joelle and Vincent, Pascal},
eprint = {1806.02315},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Touati et al. - 2018 - Randomized Value Functions via Multiplicative Normalizing Flows.pdf:pdf},
title = {{Randomized Value Functions via Multiplicative Normalizing Flows}},
url = {http://arxiv.org/abs/1806.02315},
year = {2018}
}
@article{Wu2015,
abstract = {Common spatial patterns (CSP) is a well-known spatial filtering algorithm for multichannel electroencephalogram (EEG) analysis. In this paper, we cast the CSP algorithm in a probabilistic modeling setting. Specifically, probabilistic CSP (P-CSP) is proposed as a generic EEG spatio-temporal modeling framework that subsumes the CSP and regularized CSP algorithms. The proposed framework enables us to resolve the overfitting issue of CSP in a principled manner. We derive statistical inference algorithms that can alleviate the issue of local optima. In particular, an efficient algorithm based on eigendecomposition is developed for maximum a posteriori (MAP) estimation in the case of isotropic noise. For more general cases, a variational algorithm is developed for group-wise sparse Bayesian learning for the P-CSP model and for automatically determining the model size. The two proposed algorithms are validated on a simulated data set. Their practical efficacy is also demonstrated by successful applications to single-trial classifications of three motor imagery EEG data sets and by the spatio-temporal pattern analysis of one EEG data set recorded in a Stroop color naming task.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Wu, Wei and Chen, Zhe and Gao, Xiaorong and Li, Yuanqing and Brown, Emery N. and Gao, Shangkai},
doi = {10.1109/TPAMI.2014.2330598},
eprint = {15334406},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2015 - Probabilistic common spatial patterns for multichannel EEG analysis.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Brain-computer interface,Common spatial patterns,Electroencephalogram,Fukunaga-Koontz transform,Sparse bayesian learning,Variational bayes},
pmid = {26005228},
title = {{Probabilistic common spatial patterns for multichannel EEG analysis}},
year = {2015}
}
@article{Chu2012,
abstract = {Unsupervised discovery of commonalities in images has recently attracted much interest due to the need to find correspondences in large amounts of visual data. A natural extension, and a relatively unexplored problem, is how to discover common semantic temporal patterns in videos. That is, given two or more videos, find the subsequences that contain similar visual content in an unsupervised manner. We call this problem Temporal Commonality Discovery (TCD). The naive exhaustive search approach to solve the TCD problem has a computational complexity quadratic with the length of each sequence, making it impractical for regular-length sequences. This paper proposes an efficient branch and bound (B{\&}B) algorithm to tackle the TCD problem. We derive tight bounds for classical distances between temporal bag of words of two segments, including ℓ1, intersection and $\chi$ 2. Using these bounds the B{\&}B algorithm can efficiently find the global optimal solution. Our algorithm is general, and it can be applied to any feature that has been quantified into histograms. Experiments on finding common facial actions in video and human actions in motion capture data demonstrate the benefits of our approach. To the best of our knowledge, this is the first work that addresses unsupervised discovery of common events in videos.},
author = {Chu, Wen Sheng and Zhou, Feng and {De La Torre}, Fernando},
doi = {10.1007/978-3-642-33765-9_27},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chu, Zhou, De La Torre - 2012 - Unsupervised temporal commonality discovery.pdf:pdf},
isbn = {9783642337642},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Temporal bag of words,branch and bound,temporal commonality discovery},
number = {PART 4},
pages = {373--387},
title = {{Unsupervised temporal commonality discovery}},
volume = {7575 LNCS},
year = {2012}
}
@phdthesis{Paiva2008,
author = {Paiva, Antonio R. C.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Paiva - 2008 - REPRODUCING KERNEL HILBERT SPACES FOR POINT PROCESSES, WITH APPLICATIONS TO NEURAL ACTIVITY ANALYSIS.pdf:pdf},
title = {{REPRODUCING KERNEL HILBERT SPACES FOR POINT PROCESSES, WITH APPLICATIONS TO NEURAL ACTIVITY ANALYSIS}},
year = {2008}
}
@article{Ha2019,
abstract = {Applying Bayesian optimization in problems wherein the search space is unknown is challenging. To address this problem, we propose a systematic volume expansion strategy for the Bayesian optimization. We devise a strategy to guarantee that in iterative expansions of the search space, our method can find a point whose function value within epsilon of the objective function maximum. Without the need to specify any parameters, our algorithm automatically triggers a minimal expansion required iteratively. We derive analytic expressions for when to trigger the expansion and by how much to expand. We also provide theoretical analysis to show that our method achieves epsilon-accuracy after a finite number of iterations. We demonstrate our method on both benchmark test functions and machine learning hyper-parameter tuning tasks and demonstrate that our method outperforms baselines.},
archivePrefix = {arXiv},
arxivId = {1910.13092},
author = {Ha, Huong and Rana, Santu and Gupta, Sunil and Nguyen, Thanh and Tran-The, Hung and Venkatesh, Svetha},
eprint = {1910.13092},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ha et al. - 2019 - Bayesian Optimization with Unknown Search Space.pdf:pdf},
number = {NeurIPS},
pages = {1--15},
title = {{Bayesian Optimization with Unknown Search Space}},
url = {http://arxiv.org/abs/1910.13092},
year = {2019}
}
@article{Aoi2017,
abstract = {We examine the problem of rapidly and efficiently estimating a neuron's linear receptive field (RF) from responses to high-dimensional stimuli. This problem poses important statistical and computational challenges. Statistical challenges arise from the need for strong regularization when using correlated stimuli in high- dimensional parameter spaces, while computational challenges arise from extensive time and memory costs associated with evidence-optimization and inference in high-dimensional settings. Here we focus on novel methods for scaling up automatic smoothness determination (ASD), an empirical Bayesian method for RF estimation, to high-dimensional settings. First, we show that using a zero-padded Fourier domain representation and a 'coarse-to-fine' evidence optimization strategy gives substantial improvements in speed and memory, while maintaining exact numerical accuracy. We then introduce a suite of scalable approximate methods that exploit Kronecker and Toeplitz structure in the stimulus autocovariance, which can be related to the method of expected log-likelihoods. When applied together, these methods reduce the cost of estimating an RF with tensor order D and d coefficients per tensor dimension from O(d3D) time and O(d2D) space for standard ASD to O(Dd log d) time and O(Dd) space. We show that evidence optimization for a linear RF with 160K coefficients using 5K samples of data can be carried out on a laptop in {\textless} 2s.},
author = {Aoi, Mikio and Pillow, Jonathan W.},
doi = {10.1101/212217},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Aoi, Pillow - 2017 - Scalable Bayesian inference for high-dimensional neural receptive fields.pdf:pdf},
journal = {bioRxiv},
pages = {212217},
title = {{Scalable Bayesian inference for high-dimensional neural receptive fields}},
url = {https://www.biorxiv.org/content/early/2017/11/01/212217},
year = {2017}
}
@article{Zhao2013,
abstract = {A new generalized multilinear regression model, termed the Higher-Order Partial Least Squares (HOPLS), is introduced with the aim to predict a tensor (multiway array) {\$}\backslashtensor{\{}Y{\}}{\$} from a tensor {\$}\backslashtensor{\{}X{\}}{\$} through projecting the data onto the latent space and performing regression on the corresponding latent variables. HOPLS differs substantially from other regression models in that it explains the data by a sum of orthogonal Tucker tensors, while the number of orthogonal loadings serves as a parameter to control model complexity and prevent overfitting. The low dimensional latent space is optimized sequentially via a deflation operation, yielding the best joint subspace approximation for both {\$}\backslashtensor{\{}X{\}}{\$} and {\$}\backslashtensor{\{}Y{\}}{\$}. Instead of decomposing {\$}\backslashtensor{\{}X{\}}{\$} and {\$}\backslashtensor{\{}Y{\}}{\$} individually, higher order singular value decomposition on a newly defined generalized cross-covariance tensor is employed to optimize the orthogonal loadings. A systematic comparison on both synthetic data and real-world decoding of 3D movement trajectories from electrocorticogram (ECoG) signals demonstrate the advantages of HOPLS over the existing methods in terms of better predictive ability, suitability to handle small sample sizes, and robustness to noise.},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.1230v1},
author = {Zhao, Qibin and Caiafa, Cesar F. and Mandic, Danilo P. and Chao, Zenas C. and Nagasaka, Yasuo and Fujii, Naotaka and Zhang, Liqing and Cichocki, Andrzej},
doi = {10.1109/TPAMI.2012.254},
eprint = {arXiv:1207.1230v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhao et al. - 2013 - Higher order partial least squares (HOPLS) A generalized multilinear regression method.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multilinear regression,constrained block Tucker decomposition,electrocorticogram,fusion of behavioral and neural data,higher order singular value decomposition,partial least squares},
number = {7},
pages = {1660--1673},
title = {{Higher order partial least squares (HOPLS): A generalized multilinear regression method}},
volume = {35},
year = {2013}
}
@article{Glasser2018,
abstract = {Tensor networks have found a wide use in a variety of applications in physics and computer science, recently leading to both theoretical insights as well as practical algorithms in machine learning. In this work we explore the connection between tensor networks and probabilistic graphical models, and show that it motivates the definition of generalized tensor networks where information from a tensor can be copied and reused in other parts of the network. We discuss the relationship between generalized tensor network architectures used in quantum physics, such as String-Bond States and Entangled Plaquette States, and architectures commonly used in machine learning. We provide an algorithm to train these networks in a supervised learning context and show that they overcome the limitations of regular tensor networks in higher dimensions, while keeping the computation efficient. A method to combine neural networks and tensor networks as part of a common deep learning architecture is also introduced. We benchmark our algorithm for several generalized tensor network architectures on the task of classifying images and sounds, and show that they outperform previously introduced tensor network algorithms. Some of the models we consider can be realized on a quantum computer and may guide the development of near-term quantum machine learning architectures.},
archivePrefix = {arXiv},
arxivId = {1806.05964},
author = {Glasser, Ivan and Pancotti, Nicola and Cirac, J. Ignacio},
eprint = {1806.05964},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Glasser, Pancotti, Cirac - 2018 - Supervised learning with generalized tensor networks.pdf:pdf},
pages = {1--15},
title = {{Supervised learning with generalized tensor networks}},
url = {http://arxiv.org/abs/1806.05964},
year = {2018}
}
@inproceedings{Kang2013,
abstract = {Common spatial patterns (CSP) or its probabilistic counterpart, PCSP, is$\backslash$na popular discriminative feature extraction method for$\backslash$nelectroencephalography (EEG) classification. Models in CSP or PCSP are$\backslash$ntrained on a subject-by-subject basis so that inter-subject information$\backslash$nis not used. In the case of multi-subject EEG classification where brain$\backslash$nwaves recorded from multiple subjects who undergo the same mental task$\backslash$nare available, it is desirable to capture inter-subject relatedness in$\backslash$nlearning a model. In this paper we present a nonparametric Bayesian$\backslash$nmodel for a multi-subject extension of CSP where subject relatedness is$\backslash$ncaptured by assuming that spatial patterns across subjects share a$\backslash$nlatent subspace. Spatial patterns and the shared latent subspace are$\backslash$njointly learned by variational inference. We use an infinite latent$\backslash$nfeature model to automatically infer the dimension of the shared latent$\backslash$nsubspace, placing Indian Buffet process (IBP) priors on our model.$\backslash$nNumerical experiments on BCI competition IV 2a dataset demonstrate the$\backslash$nhigh performance of our method, compared to PCSP and existing Bayesian$\backslash$nmulti-task CSP models.},
author = {Kang, Hyohyeong and Choi, Seungjin},
booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6638278},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kang, Choi - 2013 - Bayesian multi-subject common spatial patterns with Indian Buffet process priors.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {15206149},
keywords = {Brain computer interface,EEG classification,Indian Buffet processes,common spatial patterns,nonparametric Bayesian methods},
month = {may},
pages = {3347--3351},
publisher = {IEEE},
title = {{Bayesian multi-subject common spatial patterns with Indian Buffet process priors}},
url = {http://ieeexplore.ieee.org/document/6638278/},
year = {2013}
}
@article{Yi2018,
abstract = {We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8{\%} on the CLEVR dataset. Second, the model is more data-and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.},
archivePrefix = {arXiv},
arxivId = {1810.02338v1},
author = {Yi, Kexin and Wu, Jiajun and Gan, Chuang and Torralba, Antonio and Deepmind, Pushmeet Kohli and Tenenbaum, Joshua B},
eprint = {1810.02338v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yi et al. - 2018 - Neural-Symbolic VQA Disentangling Reasoning from Vision and Language Understanding.pdf:pdf},
number = {Nips},
title = {{Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding}},
url = {https://arxiv.org/pdf/1810.02338.pdf},
year = {2018}
}
@article{Author2019,
abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incremen-tally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz {\&} Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency. 1},
author = {Author, Anonymous},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Author - 2019 - Efficient Lifelong Learning with A-GEM.pdf:pdf},
journal = {ICLR'19 Submission},
pages = {1--18},
title = {{Efficient Lifelong Learning with A-GEM}},
year = {2019}
}
@article{Chung2018a,
abstract = {Inference on unknown quantities in dynamical systems via observational data is essential for providing meaningful insight, furnishing accurate predictions, enabling robust control, and establishing appropriate designs for future experiments. Merging mathematical theory with empirical measurements in a statistically coherent way is critical and challenges abound, e.g.,: ill-posedness of the parameter estimation problem, proper regularization and incorporation of prior knowledge, and computational limitations on full uncertainty qualification. To address these issues, we propose a new method for learning parameterized dynamical systems from data. In many ways, our proposal turns the canonical framework on its head. We first fit a surrogate stochastic process to observational data, enforcing prior knowledge (e.g., smoothness), and coping with challenging data features like heteroskedasticity, heavy tails and censoring. Then, samples of the stochastic process are used as "surrogate data" and point estimates are computed via ordinary point estimation methods in a modular fashion. An attractive feature of this approach is that it is fully Bayesian and simultaneously parallelizable. We demonstrate the advantages of our new approach on a predator prey simulation study and on a real world application involving within-host influenza virus infection data paired with a viral kinetic model.},
archivePrefix = {arXiv},
arxivId = {1802.00852},
author = {Chung, M. and Binois, M. and Gramacy, R. B. and Moquin, D. J. and Smith, A. P. and Smith, A. M.},
eprint = {1802.00852},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2018 - Parameter and Uncertainty Estimation for Dynamical Systems Using Surrogate Stochastic Processes.pdf:pdf},
keywords = {60g15,62f10,62f15,65l05,65l09,92-08,ams subject classifications,dynamical systems,gaussian process,inference,parameter estimation,regulariza-,tion,uncertainty estimation,viral kinetic model},
pages = {1--24},
title = {{Parameter and Uncertainty Estimation for Dynamical Systems Using Surrogate Stochastic Processes}},
url = {http://arxiv.org/abs/1802.00852},
year = {2018}
}
@article{Griffiths2015,
abstract = {Marr's levels of analysis—computational, algorithmic, and implementation—have served cogni- tive science well over the last 30 years. But the recent increase in the popularity of the computa- tional level raises a new challenge: How do we begin to relate models at different levels of analysis? We propose that it is possible to define levels of analysis that lie between the computa- tional and the algorithmic, providing a way to build a bridge between computational- and algorith- mic-level models. The key idea is to push the notion of rationality, often used in defining computational-level models, deeper toward the algorithmic level. We offer a simple recipe for reverse-engineering the mind's cognitive strategies by deriving optimal algorithms for a series of increasingly more realistic abstract computational architectures, which we call “resource-rational analysis.”},
author = {Griffiths, Thomas L. and Lieder, Falk and Goodman, Noah D.},
doi = {10.1111/tops.12142},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Griffiths, Lieder, Goodman - 2015 - Rational use of cognitive resources Levels of analysis between the computational and the algorithmic.pdf:pdf},
isbn = {1756-8757, 1756-8757},
issn = {17568765},
journal = {Topics in Cognitive Science},
keywords = {Algorithmic level,Bayesian models of cognition,Computational level,Levels of analysis,Rational process models,Resource-rational models},
number = {2},
pages = {217--229},
pmid = {25898807},
title = {{Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic}},
volume = {7},
year = {2015}
}
@article{Taniguchi2015,
abstract = {In this paper, we propose an active perception method for recognizing object categories based on the multimodal hierarchical Dirichlet process (MHDP). The MHDP enables a robot to form object categories using multimodal information, e.g., visual, auditory, and haptic information, which can be observed by performing actions on an object. However, performing many actions on a target object requires a long time. In a real-time scenario, i.e., when the time is limited, the robot has to determine the set of actions that is most effective for recognizing a target object. We propose an MHDP-based active perception method that uses the information gain (IG) maximization criterion and lazy greedy algorithm. We show that the IG maximization criterion is optimal in the sense that the criterion is equivalent to a minimization of the expected Kullback--Leibler divergence between a final recognition state and the recognition state after the next set of actions. However, a straightforward calculation of IG is practically impossible. Therefore, we derive an efficient Monte Carlo approximation method for IG by making use of a property of the MHDP. We also show that the IG has submodular and non-decreasing properties as a set function because of the structure of the graphical model of the MHDP. Therefore, the IG maximization problem is reduced to a submodular maximization problem. This means that greedy and lazy greedy algorithms are effective and have a theoretical justification for their performance. We conducted an experiment using an upper-torso humanoid robot and a second one using synthetic data. The experimental results show that the method enables the robot to select a set of actions that allow it to recognize target objects quickly and accurately. The results support our theoretical outcomes.},
archivePrefix = {arXiv},
arxivId = {1510.00331},
author = {Taniguchi, Tadahiro and Takano, Toshiaki and Yoshino, Ryo},
eprint = {1510.00331},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Taniguchi, Takano, Yoshino - 2015 - Multimodal Hierarchical Dirichlet Process-based Active Perception.pdf:pdf},
keywords = {active perception,cognitive robotics,multimodal machine learning,submodular maximization,topic model},
pages = {1--32},
title = {{Multimodal Hierarchical Dirichlet Process-based Active Perception}},
url = {http://arxiv.org/abs/1510.00331},
volume = {1},
year = {2015}
}
@article{May2013,
abstract = {In a 2-alternative forced-choice (2AFC) discrimination task, observers choose which of two stimuli has the higher value. The psychometric function for this task gives the probability of a correct response for a given stimulus difference, $\Delta$x. This paper proves four theorems about the psychometric function. Assuming the observer applies a transducer and adds noise, Theorem 1 derives a convenient general expression for the psychometric function. Discrimination data are often fitted with a Weibull function. Theorem 2 proves that the Weibull "slope" parameter, $\beta$, can be approximated by $\beta$Noise × $\beta$Transducer, where $\beta$Noise is the $\beta$ of the Weibull function that fits best to the cumulative noise distribution, and $\beta$Transducer depends on the transducer. We derive general expressions for $\beta$Noise and $\beta$Transducer, from which we derive expressions for specific cases. One case that follows naturally from our general analysis is Pelli's finding that, when d′ ∞($\Delta$x)b, $\beta$ ≈ $\beta$Noise × b. We also consider two limiting cases. Theorem 3 proves that, as sensitivity improves, 2AFC performance will usually approach that for a linear transducer, whatever the actual transducer; we show that this does not apply at signal levels where the transducer gradient is zero, which explains why it does not apply to contrast detection. Theorem 4 proves that, when the exponent of a power-function transducer approaches zero, 2AFC performance approaches that of a logarithmic transducer. We show that the power-function exponents of 0.4-0.5 fitted to suprathreshold contrast discrimination data are close enough to zero for the fitted psychometric function to be practically indistinguishable from that of a log transducer. Finally, Weibull $\beta$ reflects the shape of the noise distribution, and we used our results to assess the recent claim that internal noise has higher kurtosis than a Gaussian. Our analysis of $\beta$ for contrast discrimination suggests that, if internal noise is stimulus-independent, it has lower kurtosis than a Gaussian. {\textcopyright} 2013 May, Solomon.},
author = {May, Keith A. and Solomon, Joshua A.},
doi = {10.1371/journal.pone.0074815},
file = {:Users/mshvarts/Downloads/journal.pone.0074815.PDF:PDF},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pmid = {24124456},
title = {{Four Theorems on the Psychometric Function}},
volume = {8},
year = {2013}
}
@article{Licklider1960,
author = {Licklider, J. C.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Licklider - 1960 - Man-Computer Symbiosis.pdf:pdf},
journal = {IRE transactions on human factors in electronics},
pages = {4--11},
title = {{Man-Computer Symbiosis}},
volume = {1},
year = {1960}
}
@article{Alvarez2011,
author = {Alvarez, Mauricio A and Lawrence, Neil D},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Alvarez, Lawrence - 2011 - Computationally Efficient Convolved Multiple Output Gaussian Processes.pdf:pdf},
keywords = {convolution processes,efficient approximations,gaussian processes,ing,multitask learn-,multivariate processes,structured outputs},
pages = {1459--1500},
title = {{Computationally Efficient Convolved Multiple Output Gaussian Processes}},
volume = {12},
year = {2011}
}
@article{Taitz2019,
author = {Taitz, Alan and Shalom, Diego E},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Taitz, Shalom - 2019 - Ocular dynamics reveal articulatory processing at single-phoneme level during silent reading.pdf:pdf},
title = {{Ocular dynamics reveal articulatory processing at single-phoneme level during silent reading}},
year = {2019}
}
@article{Laub2015,
abstract = {Hawkes processes are a particularly interesting class of stochastic process that have been applied in diverse areas, from earthquake modelling to financial analysis. They are point processes whose defining characteristic is that they 'self-excite', meaning that each arrival increases the rate of future arrivals for some period of time. Hawkes processes are well established, particularly within the financial literature, yet many of the treatments are inaccessible to one not acquainted with the topic. This survey provides background, introduces the field and historical developments, and touches upon all major aspects of Hawkes processes.},
archivePrefix = {arXiv},
arxivId = {1507.02822},
author = {Laub, Patrick J. and Taimre, Thomas and Pollett, Philip K.},
eprint = {1507.02822},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Laub, Taimre, Pollett - 2015 - Hawkes Processes.pdf:pdf},
title = {{Hawkes Processes}},
url = {http://arxiv.org/abs/1507.02822},
year = {2015}
}
@article{Ambikasaran2016,
abstract = {A number of problems in probability and statistics can be addressed using the multivariate normal (Gaussian) distribution. In the one-dimensional case, computing the probability for a given mean and variance simply requires the evaluation of the corresponding Gaussian density. In the {\$}n{\$} -dimensional setting, however, it requires the inversion of an {\$}n times n{\$} covariance matrix, {\$}C{\$} , as well as the evaluation of its determinant, {\$}det (C){\$} . In many cases, such as regression using Gaussian processes, the covariance matrix is of the form {\$}C = sigma {\^{}}2 I + K{\$} , where {\$}K{\$} is computed using a specified covariance kernel which depends on the data and additional parameters (hyperparameters). The matrix {\$}C{\$} is typically dense, causing standard direct methods for inversion and determinant evaluation to require {\$}mathcal {\{}O{\}}(n{\^{}}3){\$} work. This cost is prohibitive for large-scale modeling. Here, we show that for the most commonly used covariance functions, the matrix {\$}C{\$} can be hierarchically factored into a product of block low-rank updates of the identity matrix, yielding an {\$}mathcal {\{}O{\}} (n,log{\^{}}2, n){\$} algorithm for inversion. More importantly, we show that this factorization enables the evaluation of the determinant {\$}det (C){\$}, permitting the direct calculation of probabilities in high dimensions under fairly broad assumptions on the kernel defining {\$}K{\$} . Our fast algorithm brings many problems in marginalization and the adaptation of hyperparameters within practical reach using a single CPU core. The combination of nearly optimal scaling in terms of problem size with high-performance computing resources will permit the modeling of previously intractable problems. We illustrate the performance of the scheme on standard covariance kernels.},
archivePrefix = {arXiv},
arxivId = {1403.6015},
author = {Ambikasaran, Sivaram and Foreman-Mackey, Daniel and Greengard, Leslie and Hogg, David W. and O'Neil, Michael},
doi = {10.1109/TPAMI.2015.2448083},
eprint = {1403.6015},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
title = {{Fast Direct Methods for Gaussian Processes}},
volume = {38},
year = {2016}
}
@article{Engbert2003,
abstract = {Fixational eye movements are subdivided into tremor, drift, and microsaccades. All three types of miniature eye movements generate small random displacements of the retinal image when viewing a stationary scene. Here we investigate the modulation of microsaccades by shifts of covert attention in a classical spatial cueing paradigm. First, we replicate the suppression of microsaccades with a minimum rate about 150 ms after cue onset. Second, as a new finding we observe microsaccadic enhancement with a maximum rate about 350 ms after presentation of the cue. Third, we find a modulation of the orientation towards the cue direction. These multiple influences of visual attention on microsaccades accentuate their role for visual information processing. Furthermore, our results suggest that microsaccades can be used to map the orientation of visual attention in psychophysical experiments. {\textcopyright} 2003 Elsevier Science Ltd. All rights reserved.},
author = {Engbert, Ralf and Kliegl, Reinhold},
doi = {10.1016/S0042-6989(03)00084-1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Engbert, Kliegl - 2003 - Microsaccades uncover the orientation of covert attention.pdf:pdf},
isbn = {0042-6989},
issn = {00426989},
journal = {Vision Research},
keywords = {Attention,Eye movements,Inhibition,Microsaccade,Spatial cueing},
month = {apr},
number = {9},
pages = {1035--1045},
pmid = {12676246},
title = {{Microsaccades uncover the orientation of covert attention}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0042698903000841},
volume = {43},
year = {2003}
}
@article{Korattikara2015,
abstract = {We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities, e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for "distilling" a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [Hernandez-Lobato and Adams, 2015] and an approach based on variational Bayes [Blundell et al., 2015]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.},
archivePrefix = {arXiv},
arxivId = {1506.04416},
author = {Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Welling, Max},
doi = {10.1017/CBO9781107415324.004},
eprint = {1506.04416},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{Bayesian Dark Knowledge}},
year = {2015}
}
@article{Solway2014,
abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
archivePrefix = {arXiv},
arxivId = {1703.04730},
author = {Solway, Alec and Diuk, Carlos and C{\'{o}}rdova, Natalia and Yee, Debbie and Barto, Andrew G. and Niv, Yael and Botvinick, Matthew M.},
doi = {10.1371/journal.pcbi.1003779},
editor = {Sporns, Olaf},
eprint = {1703.04730},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Solway et al. - 2014 - Optimal Behavioral Hierarchy.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {1553-7358},
journal = {PLoS Computational Biology},
month = {aug},
number = {8},
pages = {e1003779},
pmid = {25122479},
title = {{Optimal Behavioral Hierarchy}},
url = {http://arxiv.org/abs/1703.04730 http://dx.plos.org/10.1371/journal.pcbi.1003779},
volume = {10},
year = {2014}
}
@article{Luo2019,
abstract = {We propose a novel fused Gromov-Wasserstein alignment method to jointly learn the Hawkes processes in different event spaces, and align their event types. Given two Hawkes processes, we use fused Gromov-Wasserstein discrepancy to measure their dissimilarity, which considers both the Wasserstein discrepancy based on their base intensities and the Gromov-Wasserstein discrepancy based on their infectivity matrices. Accordingly, the learned optimal transport reflects the correspondence between the event types of these two Hawkes processes. The Hawkes processes and their optimal transport are learned jointly via maximum likelihood estimation, with a fused Gromov-Wasserstein regularizer. Experimental results show that the proposed method works well on synthetic and real-world data.},
archivePrefix = {arXiv},
arxivId = {1910.02096},
author = {Luo, Dixin and Xu, Hongteng and Carin, Lawrence},
eprint = {1910.02096},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Luo, Xu, Carin - 2019 - Fused Gromov-Wasserstein Alignment for Hawkes Processes.pdf:pdf},
number = {NeurIPS},
pages = {1--5},
title = {{Fused Gromov-Wasserstein Alignment for Hawkes Processes}},
url = {http://arxiv.org/abs/1910.02096},
year = {2019}
}
@article{Brainard1997,
abstract = {The Psychophysics Toolbox is a software package that supports visual psychophysics. Its routines provide an interface between a high-level interpreted language (MATLAB on the Macintosh) and the video display hardware. A set of example programs is included with the Toolbox distribution.},
author = {Brainard, David H.},
doi = {10.1163/156856897X00357},
file = {:Users/mshvarts/Downloads/PsychToolbox.pdf:pdf},
issn = {01691015},
journal = {Spatial Vision},
number = {4},
pages = {433--436},
pmid = {9176952},
title = {{The Psychophysics Toolbox}},
volume = {10},
year = {1997}
}
@article{Kendall2017,
abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
archivePrefix = {arXiv},
arxivId = {1703.04977},
author = {Kendall, Alex and Gal, Yarin},
doi = {10.1109/TDEI.2009.5211872},
eprint = {1703.04977},
isbn = {1070-9878},
issn = {16653521},
pmid = {24335434},
title = {{What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}},
year = {2017}
}
@article{Zhao2017,
abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized in- ference distributions and, in some cases, improv- ing the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distri- bution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can sig- nificantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decod- ing distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
archivePrefix = {arXiv},
arxivId = {1706.02262},
author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
eprint = {1706.02262},
journal = {ICML Workshop},
title = {{InfoVAE: Balancing Learning and Inference in Variational Autoencoders}},
year = {2017}
}
@article{Musslick2017,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 317019423 Multitasking Efficiency Conference CITATIONS 0 READS 105 6 , including : Some : Cognitive Beacon - referenced Sebastian Princeton 11 SEE Kayhan Princeton 18 SEE Biswadip Princeton 18 SEE All . The . All - text and , letting . Abstract One of the most salient and well - recognized features of human goal - directed behavior is our limited ability to conduct mul - tiple demanding tasks at once . Previous work has identified overlap between task processing pathways as a limiting fac - tor for multitasking performance in neural architectures . This raises an important question : insofar as shared representation between tasks introduces the risk of cross - talk and thereby lim - itations in multitasking , why would the brain prefer shared task representations over separate representations across tasks ? We seek to answer this question by introducing formal considera - tions and neural network simulations in which we contrast the multitasking limitations that shared task representations incur with their benefits for task learning . Our results suggest that neural network architectures face a fundamental tradeoff be - tween learning efficiency and multitasking performance in en - vironments with shared structure between tasks .},
author = {Musslick, Sebastian and Saxe, Andrew M. and Ozcimder, Kayhan and Dey, Biswadip and Henselman, G. and Cohen, Jonathan D.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Musslick et al. - 2017 - Multitasking Capability Versus Learning Efficiency in Neural Network Architectures.pdf:pdf},
journal = {Proceedings of the 39th annual meeting of the Cognitive Science Society},
keywords = {publications},
number = {July},
pages = {829--834},
title = {{Multitasking Capability Versus Learning Efficiency in Neural Network Architectures}},
year = {2017}
}
@article{Lu2018,
abstract = {We tackle the problem of optimizing a black- box objective function defined over a highly- structured input space. This problem is ubiquitous in machine learning. Inferring the structure of a neural network or the Automatic Statistician (AS), where the kernel combination for a Gaussian process is optimized, are two of many possible examples. We use the AS as a case study to describe our approach, that can be easily generalized to other domains. We propose an Structure Generating Variational Auto-encoder (SG-VAE) to embed the original space of kernel combinations into some low-dimensional continuous manifold where Bayesian optimization (BO) ideas are used. This is possible when structural knowledge of the problem is available, which can be given via a simulator or any other form of generating potentially good solutions. The right exploration-exploitation balance is imposed by propagating into the search the uncertainty of the latent space of the SG-VAE, that is computed using variational inference. The key aspect of our approach is that the SG-VAE can be used to bias the search towards relevant regions, making it suitable for transfer learning tasks. Several experiments in various application domains are used to illustrate the utility and generality of the approach described in this work.},
author = {Lu, Xiaoyu and Gonz{\'{a}}lez, Javier and Dai, Zhenwen and Lawrence, Neil D.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lu et al. - 2018 - Structured variationally auto-encoded optimization.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
pages = {5169--5180},
title = {{Structured variationally auto-encoded optimization}},
volume = {7},
year = {2018}
}
@article{Scargle2013,
abstract = {This paper addresses the problem of detecting and characterizing local variability in time series and other forms of sequential data. The goal is to identify and characterize statistically significant variations, at the same time suppressing the inevitable corrupting observational errors. We present a simple nonparametric modeling technique and an algorithm implementing it - an improved and generalized version of Bayesian Blocks (Scargle 1998) - that finds the optimal segmentation of the data in the observation interval. The structure of the algorithm allows it to be used in either a real-time trigger mode, or a retrospective mode. Maximum likelihood or marginal posterior functions to measure model fitness are presented for events, binned counts, and measurements at arbitrary times with known error distributions. Problems addressed include those connected with data gaps, variable exposure, extension to piecewise linear and piecewise exponential representations, multi-variate time series data, analysis of variance, data on the circle, other data modes, and dispersed data. Simulations provide evidence that the detection efficiency for weak signals is close to a theoretical asymptotic limit derived by (Arias-Castro, Donoho and Huo 2003). In the spirit of Reproducible Research (Donoho et al. 2008) all of the code and data necessary to reproduce all of the figures in this paper are included as auxiliary material.},
archivePrefix = {arXiv},
arxivId = {1207.5578},
author = {Scargle, Jeffrey D. and Norris, Jay P. and Jackson, Brad and Chiang, James},
doi = {10.1088/0004-637X/764/2/167},
eprint = {1207.5578},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Scargle et al. - 2013 - Studies in astronomical time series analysis. VI. Bayesian block representations.pdf:pdf},
isbn = {0004-637X},
issn = {15384357},
journal = {Astrophysical Journal},
keywords = {methods: data analysis,methods: statistical},
number = {2},
pages = {1--82},
pmid = {17556540},
title = {{Studies in astronomical time series analysis. VI. Bayesian block representations}},
volume = {764},
year = {2013}
}
@article{Sykacek2003,
abstract = {We propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classification that combines the computational advantage of a parametric solution with the flexibility of sequential sam- pling techniques. We regard the parameters of the classifier as latent states in a first order Markov process and propose an algorithm which can be regarded as variational generalization of standard Kalman filter- ing. The variational Kalman filter is based on two novel lower bounds that enable us to use a non-degenerate distribution over the adaptation rate. An extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classifiers both in stationary and non-stationary environments. Although we focus on classification, the algorithm is easily extended to other generalized nonlinear models.},
author = {Sykacek, Peter and Roberts, Stephen},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sykacek, Roberts - 2003 - Adaptive Classification by Variational Kalman Filtering.pdf:pdf},
isbn = {0262025507},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems},
number = {1},
pages = {737--744},
title = {{Adaptive Classification by Variational Kalman Filtering}},
volume = {15},
year = {2003}
}
@article{Bishop1992,
abstract = {The elements of the Hessian matrix consist of the second derivatives of the error measure with respect to the weights and thresholds in the network. They are needed in Bayesian estimation of network regularization parameters, for estimation of error bars on the ... $\backslash$n},
author = {Bishop, Chris},
doi = {10.1162/neco.1992.4.4.494},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bishop - 1992 - Exact Calculation of the Hessian Matrix for the Multilayer Perceptron.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
number = {4},
pages = {494--501},
title = {{Exact Calculation of the Hessian Matrix for the Multilayer Perceptron}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1992.4.4.494},
volume = {4},
year = {1992}
}
@article{Bouvrie2017,
abstract = {We introduce a data-based approach to estimating key quantities which arise in the study of nonlinear control systems and random nonlinear dynamical systems. Our approach hinges on the observation that much of the existing linear theory may be readily extended to nonlinear systems - with a reasonable expectation of success - once the nonlinear system has been mapped into a high or infinite dimensional feature space. In particular, we develop computable, non-parametric estimators approximating controllability and observability energy functions for nonlinear systems, and study the ellipsoids they induce. In all cases the relevant quantities are estimated from simulated or observed data. It is then shown that the controllability energy estimator provides a key means for approximating the invariant measure of an ergodic, stochastically forced nonlinear system.},
archivePrefix = {arXiv},
arxivId = {arXiv:1204.0563v2},
author = {Bouvrie, Jake and Hamzi, Boumediene},
doi = {10.3934/jcd.2017001},
eprint = {arXiv:1204.0563v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bouvrie, Hamzi - 2017 - Kernel methods for the approximation of some key quantities of nonlinear systems.pdf:pdf},
issn = {2158-2491},
journal = {Journal of Computational Dynamics},
keywords = {controllability,energy,fokker-planck equation,gramians,kernel methods,machine learning,nonlinear systems,observability energy,stationary solution of the},
number = {1},
pages = {1--1},
title = {{Kernel methods for the approximation of some key quantities of nonlinear systems}},
volume = {4},
year = {2017}
}
@article{Schulz2018,
abstract = {This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration–exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.},
author = {Schulz, Eric and Speekenbrink, Maarten and Krause, Andreas},
doi = {10.1016/j.jmp.2018.03.001},
file = {:Users/mshvarts/Downloads/Schulz2018tutorial.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Active learning,Bandit problems,Exploration–exploitation,Gaussian process regression},
pages = {1--16},
publisher = {Elsevier Inc.},
title = {{A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions}},
url = {https://doi.org/10.1016/j.jmp.2018.03.001},
volume = {85},
year = {2018}
}
@article{Hoffman2018,
abstract = {Deriving conditional and marginal distributions using conjugacy relationships can be time consuming and error prone. In this paper, we propose a strategy for automating such derivations. Unlike previous systems which focus on relationships between pairs of random variables, our system (which we call Autoconj) operates directly on Python functions that compute log-joint distribution functions. Autoconj provides support for conjugacy-exploiting algorithms in any Python-embedded PPL. This paves the way for accelerating development of novel inference algorithms and structure-exploiting modeling strategies. 1},
archivePrefix = {arXiv},
arxivId = {1811.11926},
author = {Hoffman, Matthew D. and Johnson, Matthew J. and Tran, Dustin},
eprint = {1811.11926},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hoffman, Johnson, Tran - 2018 - AutoConj Recognizing and exploiting conjugacy without a domain-specific language.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {10716--10726},
title = {{AutoConj: Recognizing and exploiting conjugacy without a domain-specific language}},
volume = {2018-Decem},
year = {2018}
}
@article{Ma2019,
abstract = {In this paper, we propose irreversible versions of the Metropolis Hastings (MH) and Metropolis adjusted Langevin algorithm (MALA) with a main focus on the latter. For the former, we show how one can simply switch between different proposal and acceptance distributions upon rejection to obtain an irreversible jump sampler (I-Jump). The resulting algorithm has a simple implementation akin to MH, but with the demonstrated benefits of irreversibility. We then show how the previously proposed MALA method can also be extended to exploit irreversible stochastic dynamics as proposal distributions in the I-Jump sampler. Our experiments explore how irreversibility can increase the efficiency of the samplers in different situations.},
author = {Ma, Yi An and Fox, Emily B. and Chen, Tianqi and Wu, Lei},
doi = {10.1007/s11222-018-9802-x},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ma et al. - 2019 - Irreversible samplers from jump and continuous Markov processes.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Bayesian inference,Hamiltonian Monte Carlo,Irreversible samplers,Jump processes,Markov chain Monte Carlo,Metropolis–Hastings},
number = {1},
pages = {177--202},
title = {{Irreversible samplers from jump and continuous Markov processes}},
volume = {29},
year = {2019}
}
@article{Nguyen-Tuong2009,
abstract = {Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian Process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other nonparametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and nu-SVR.},
author = {Nguyen-Tuong, Duy and Peters, JR and Seeger, Matthias},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen-Tuong, Peters, Seeger - 2009 - Local Gaussian process regression for real time online model learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1193----1200},
title = {{Local Gaussian process regression for real time online model learning}},
url = {https://papers.nips.cc/paper/3403-local-gaussian-process-regression-for-real-time-online-model-learning.pdf},
year = {2009}
}
@article{Zhao2016,
abstract = {This paper presents a dependent multi-output Gaussian process (GP) for modeling complex dynamical systems. The outputs are dependent in this model, which is largely different from previous GP dynamical systems. We adopt convolved multi-output GPs to model the outputs, which are provided with a flexible multi-output covariance function. We adapt the variational inference method with inducing points for learning the model. Conjugate gradient based optimization is used to solve parameters involved by maximizing the vari-ational lower bound of the marginal likelihood. The proposed model has superiority on modeling dynamical systems under the more reasonable assumption and the fully Bayesian learning framework. Further, it can be flexibly extended to handle regression problems. We evaluate the model on both synthetic and real-world data including motion capture data, traffic flow data and robot inverse dynamics data. Various evaluation methods are taken on the experiments to demonstrate the effectiveness of our model, and encouraging results are observed.},
author = {Zhao, Jing and Sun, Shiliang},
doi = {10.1007/978-3-319-11812-3_30},
isbn = {9783319118116},
issn = {16113349},
journal = {Journal of Machine Learning Research},
title = {{Variational Dependent Multi-output Gaussian Process Dynamical Systems}},
volume = {17},
year = {2016}
}
@inproceedings{Laue2018,
abstract = {Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives.},
author = {Laue, Soeren and Mitterreiter, Matthias and Giesen, Joachim},
booktitle = {NIPS},
title = {{Computing Higher Order Derivatives of Matrix and Tensor Expressions}},
year = {2018}
}
@article{Chu2005,
abstract = {We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation prop-agation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach.},
author = {Chu, Wei and Ghahramani, Zoubin},
doi = {10.1145/1102351.1102369},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chu, Ghahramani - 2005 - Gaussian Processes for Ordinal Regression.pdf:pdf},
isbn = {1595931805},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Gaussian processes,approximate Bayesian inference,collaborative filtering,feature selection,gene expression analysis,ordinal regression},
pages = {1019--1041},
pmid = {1889912417787065159},
title = {{Gaussian Processes for Ordinal Regression}},
url = {http://www.jmlr.org/papers/volume6/chu05a/chu05a.pdf},
volume = {6},
year = {2005}
}
@article{Noorani2016,
author = {Noorani, Imran and Carpenter, R.H.S.},
doi = {10.1016/j.neubiorev.2016.02.018},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Noorani, Carpenter - 2016 - The LATER model of reaction time and decision.pdf:pdf},
issn = {01497634},
journal = {Neuroscience {\&} Biobehavioral Reviews},
month = {may},
pages = {229--251},
publisher = {Elsevier Ltd},
title = {{The LATER model of reaction time and decision}},
url = {http://dx.doi.org/10.1016/j.neubiorev.2016.02.018 http://linkinghub.elsevier.com/retrieve/pii/S0149763415301226},
volume = {64},
year = {2016}
}
@article{Hensman2013,
abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets. 1},
author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hensman, Fusi, Lawrence - 2013 - Gaussian Processes for Big Data.pdf:pdf},
journal = {UAI},
title = {{Gaussian Processes for Big Data}},
year = {2013}
}
@article{Chan2017,
abstract = {Consider a large number of detectors each generating a data stream. The task is to detect online, distribution changes in a small fraction of the data streams. Previous approaches to this problem include the use of mixture likelihood ratios and sum of CUSUMs. We provide here extensions and modifications of these approaches that are optimal in detecting normal mean shifts. We show how the (optimal) detection delay depends on the fraction of data streams undergoing distribution changes as the number of detectors goes to infinity. There are three detection domains. In the first domain for moderately large fractions, immediate detection is possible. In the second domain for smaller fractions, the detection delay grows logarithmically with the number of detectors, with an asymptotic constant extending those in sparse normal mixture detection. In the third domain for even smaller fractions, the detection delay lies in the framework of the classical detection delay formula of Lorden. We show that the optimal detection delay is achieved by the sum of detectability score transformations of either the partial scores or CUSUM scores of the data streams.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.08504v2},
author = {Chan, Hock Peng},
doi = {10.1214/17-AOS1546},
eprint = {arXiv:1506.08504v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chan - 2017 - Optimal sequential detection in multi-stream data.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Average run length,CUSUM,Detectability score,Detection delay,Mixture likelihood ratio,Sparse detection,Stopping rule},
number = {6},
pages = {2736--2763},
title = {{Optimal sequential detection in multi-stream data}},
volume = {45},
year = {2017}
}
@article{Ng2000,
abstract = {This paper addresses the problem of inverse reinforcement learning (IRL) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. IRL may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for IRL. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracythe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ng, Andrew and Russell, Stuart},
doi = {10.2460/ajvr.67.2.323},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ng, Russell - 2000 - Algorithms for inverse reinforcement learning.pdf:pdf},
isbn = {1-55860-707-2},
issn = {00029645},
journal = {Proceedings of the Seventeenth International Conference on Machine Learning},
pages = {663--670},
pmid = {16454640},
title = {{Algorithms for inverse reinforcement learning}},
url = {http://www-cs.stanford.edu/people/ang/papers/icml00-irl.pdf},
volume = {0},
year = {2000}
}
@article{Zhang2017a,
abstract = {Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.{\~{}}fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1712.02390},
author = {Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
doi = {10.1007/BF02715046},
eprint = {1712.02390},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Noisy Natural Gradient as Variational Inference.pdf:pdf},
issn = {02506335},
title = {{Noisy Natural Gradient as Variational Inference}},
year = {2017}
}
@article{Shankar2012,
abstract = {We propose a principled way to construct an internal representation of the temporal stimulus history leading up to the present moment. A set of leaky integrators performs a Laplace transform on the stimulus function, and a linear operator approximates the inversion of the Laplace transform. The result is a representation of stimulus history that retains information about the temporal sequence of stimuli. This procedure naturally represents more recent stimuli more accurately than less recent stimuli; the decrement in accuracy is precisely scale invariant. This procedure also yields time cells that fire at specific latencies following the stimulus with a scale-invariant temporal spread. Combined with a simple associative memory, this representation gives rise to a moment-to-moment prediction that is also scale invariant in time. We propose that this scale-invariant representation of temporal stimulus history could serve as an underlying representation accessible to higher-level behavioral and cognitive mechanisms. In order to illustrate the potential utility of this scale-invariant representation in a variety of fields, we sketch applications using minimal performance functions to problems in classical conditioning, interval timing, scale-invariant learning in autoshaping, and the persistence of the recency effect in episodic memory across timescales.},
author = {Shankar, Karthik H. and Howard, Marc W.},
doi = {10.1162/NECO_a_00212},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shankar, Howard - 2012 - A scale-invariant internal representation of time.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
number = {1},
pages = {134--193},
title = {{A scale-invariant internal representation of time}},
volume = {24},
year = {2012}
}
@article{Chu2005a,
abstract = {In this paper, we propose a probabilistic kernel approach to preference learning based on Gaussian processes. A new likelihood function is proposed to capture the preference relations in the Bayesian framework. The generalized formulation is also applicable to tackle many multiclass problems. The overall approach has the advantages of Bayesian methods for model selection and probabilistic prediction. Experimental results compared against the constraint classification approach on several benchmark datasets verify the usefulness of this algorithm.},
author = {Chu, Wei and Ghahramani, Zoubin},
doi = {10.1145/1102351.1102369},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chu, Ghahramani - 2005 - Preference learning with Gaussian processes.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning  - ICML '05},
pages = {137--144},
title = {{Preference learning with Gaussian processes}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102369},
year = {2005}
}
@article{Wu2014,
abstract = {Working memory (WM) models have traditionally assumed at least two domain-specific storage systems for verbal and visuo-spatial information. We review data that suggest the existence of an additional slave system devoted to the temporary storage of body movements, and present a novel instrument for its assessment: the movement span task. The movement span task assesses individuals' ability to remember and reproduce meaningless configurations of the body. During the encoding phase of a trial, participants watch short videos of meaningless movements presented in sets varying in size from one to five items. Immediately after encoding, they are prompted to reenact as many items as possible. The movement span task was administered to 90 participants along with standard tests of verbal WM, visuo-spatial WM, and a gesture classification test in which participants judged whether a speaker's gestures were congruent or incongruent with his accompanying speech. Performance on the gesture classification task was not related to standard measures of verbal or visuo-spatial working memory capacity, but was predicted by scores on the movement span task. Results suggest the movement span task can serve as an assessment of individual differences in WM capacity for body-centric information.},
author = {Wu, Ying Choon and Coulson, Seana},
doi = {10.1371/journal.pone.0084834},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Coulson - 2014 - A psychometric measure of working memory capacity for configured body movement.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {1},
pmid = {24465437},
title = {{A psychometric measure of working memory capacity for configured body movement}},
volume = {9},
year = {2014}
}
@article{Aguilar1954,
abstract = {By means of the two-colour threshold method the threshold response of the rod or dark-adapting visual mechanism can be followed to relatively high field intensities. From the results obtained here by this method it is concluded that at a field intensity of about 100 scotopic trolands the sensitivity of the rod mechanism to stimulus differences begins to fall off rapidly and that at about 2000 to 5000 scotopic trolands (corresponding approximately to daylight luminances of 120 to 300 cd/m2) the rod mechanism becomes saturated and is no longer capable of responding to an increase of stimulus. D'apr{\`{e}}s les r{\'{e}}sultats obtenus par cette m{\'{e}}thode, on conclut que, pour une intensit{\'{e}} du champ d'environ 100 trolands scotopiques, la sensibilit{\'{e}} des b{\^{a}}tonnets aux variations d'excitation commence {\`{a}} baisser rapidement et que, pour 2 000 {\`{a}} 5 000 trolands scotopiques (correspondant approximativement {\`{a}} des luminances de 120 {\`{a}} 300 cd/m2 en lumi{\`{e}}re du jour) les b{\^{a}}tonnets se saturent et ne peuvent plus r{\'{e}}pondre {\`{a}} une augmentation de l'excitation. Die Ergebnisse, die mit dieser Methode erhalten werden, lassen schliessen, dass bei Feldhelligkeiten von etwa 100 skotopischen Troland die Empfindlichkeit des St{\"{a}}bchenapparates f{\"{u}}r die Reizunterschiede schnell nachl{\"{a}}sst und dass bei 2000?5000 skotopischen Troland (n{\"{a}}herungsweise bei einer Tageslicht-Leuchtdichte von 120?300 cd/m2) der St{\"{a}}bchenapparat abges{\"{a}}ttigt ist und auf eine weitere Steigerung des Reizes nicht anspricht.},
annote = {doi: 10.1080/713818657},
author = {Aguilar, M and Stiles, W S},
doi = {10.1080/713818657},
issn = {0030-3909},
journal = {Optica Acta: International Journal of Optics},
month = {jan},
number = {1},
pages = {59--65},
publisher = {Taylor {\&} Francis},
title = {{Saturation of the Rod Mechanism of the Retina at High Levels of Stimulation}},
url = {https://doi.org/10.1080/713818657},
volume = {1},
year = {1954}
}
@article{Wu2019,
abstract = {Bayesian optimization is popular for optimizing time-consuming black-box objectives. Nonetheless, for hyperparameter tuning in deep neural networks, the time required to evaluate the validation error for even a few hyperparameter settings remains a bottleneck. Multi-fidelity optimization promises relief using cheaper proxies to such objectives --- for example, validation error for a network trained using a subset of the training points or fewer iterations than required for convergence. We propose a highly flexible and practical approach to multi-fidelity Bayesian optimization, focused on efficiently optimizing hyperparameters for iteratively trained supervised learning models. We introduce a new acquisition function, the trace-aware knowledge-gradient, which efficiently leverages both multiple continuous fidelity controls and trace observations --- values of the objective at a sequence of fidelities, available when varying fidelity using training iterations. We provide a provably convergent method for optimizing our acquisition function and show it outperforms state-of-the-art alternatives for hyperparameter tuning of deep neural networks and large-scale kernel learning.},
archivePrefix = {arXiv},
arxivId = {1903.04703},
author = {Wu, Jian and Toscano-Palmerin, Saul and Frazier, Peter I. and Wilson, Andrew Gordon},
eprint = {1903.04703},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2019 - Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning.pdf:pdf},
title = {{Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning}},
url = {http://arxiv.org/abs/1903.04703},
year = {2019}
}
@article{Huang2015,
abstract = {We propose a scalable Gaussian process model for regression by applying a deep neural network as the feature-mapping function. We first pre-train the deep neural network with a stacked denoising auto-encoder in an unsupervised way. Then, we perform a Bayesian linear regression on the top layer of the pre-trained deep network. The resulting model, Deep-Neural-Network-based Gaussian Pro-cess (DNN-GP), can learn much more meaningful representation of the data by the finite-dimensional but deep-layered feature-mapping function. Unlike standard Gaussian processes, our model scales well with the size of the training set due to the avoidance of kernel matrix inversion. Moreover, we present a mixture of DNN-GPs to further improve the re-gression performance. For the experiments on three representative large datasets, our proposed models significantly outperform the state-of-the-art algo-rithms of Gaussian process regression.},
author = {Huang, Wenbing and Zhao, Deli and Sun, Fuchun and Liu, Huaping and Chang, Edward},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2015 - Scalable Gaussian process regression using deep neural networks.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Special Track on Machine Learning},
number = {Ijcai},
pages = {3576--3582},
title = {{Scalable Gaussian process regression using deep neural networks}},
volume = {2015-Janua},
year = {2015}
}
@article{Laue2018a,
abstract = {Optimization is an integral part of most machine learning systems and most numerical optimization schemes rely on the computation of derivatives. Therefore, frameworks for computing derivatives are an active area of machine learning research. Surprisingly, as of yet, no existing framework is capable of computing higher order matrix and tensor derivatives directly. Here, we close this fundamental gap and present an algorithmic framework for computing matrix and tensor derivatives that extends seamlessly to higher order derivatives. The framework can be used for symbolic as well as for forward and reverse mode automatic differentiation. Experiments show a speedup between one and four orders of magnitude over state-of-the-art frameworks when evaluating higher order derivatives.},
author = {Laue, S{\"{o}}ren and Mitterreiter, Matthias and Giesen, Joachim},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Laue, Mitterreiter, Giesen - 2018 - Computing higher order derivatives of matrix and tensor expressions.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {2750--2759},
title = {{Computing higher order derivatives of matrix and tensor expressions}},
volume = {2018-Decem},
year = {2018}
}
@article{Kingma2016,
abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
archivePrefix = {arXiv},
arxivId = {1606.04934},
author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
eprint = {1606.04934},
isbn = {9781611970685},
issn = {10495258},
title = {{Improving Variational Inference with Inverse Autoregressive Flow}},
year = {2016}
}
@article{Lawrence2005,
abstract = {Summarising a high dimensional data-set with a low dimensional embedding is a standard approach for exploring its structure. In this paper we provide an overview of some existing techniques for discovering such embeddings. We then introduce a novel probabilistic interpretation of principal component analysis (PCA) that we term dual probabilistic PCA (DPPCA). The DPPCA model has the additional advantage that the linear mappings from the embedded space can easily be non-linearised through Gaussian processes. We refer to this model as a Gaussian process latent variable model (GPLVM). We develop a practical algorithm for GPLVMs which allow for non-linear mappings from the embedded space giving a non-linear probabilistic version of PCA. We develop the new algorithm to provide a principled approach to handling discrete valued data and missing attributes. We demonstrate the algorithm on a range of real-world and artificially generated data-sets and finally, through analysis of the GPLVM objective function, we relate the algorithm to popular spectral techniques such as kernel PCA and multidimensional scaling.},
author = {Lawrence, Neil},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lawrence - 2005 - Probabilistic non-linear Principal Component Analysis with Gaussian Process Latent Variable Models.pdf:pdf},
isbn = {1532-4435},
issn = {1476-4687},
journal = {Journal of Machine Learning Research},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {1783--1816},
title = {{Probabilistic non-linear Principal Component Analysis with Gaussian Process Latent Variable Models}},
url = {http://eprints.pascal-network.org/archive/00000914/},
volume = {6},
year = {2005}
}
@article{Mcnamara2017,
author = {Mcnamara, Daniel and Balcan, Maria-florina},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mcnamara, Balcan - 2017 - Performance guarantees for transferring representations.pdf:pdf},
pages = {1--4},
title = {{Performance guarantees for transferring representations}},
year = {2017}
}
@article{Hu2013,
abstract = {Extensive research on time series classification in the last decade has produced fast and accurate algorithms for the single-dimensional case. However, the increasing prevalence of inexpensive sensors has reinforced the need for algorithms to handle multi-dimensional time series. For example, modern smartphones have at least a dozen sensors capable of producing streaming time series, and hospital-based (and increasingly, home-based) medical devices can produce time series streams from more than twenty sensors. The two most common ways to generalize from single to multi-dimensional data are to use all the streams or just the single best stream as determined at training time. However, as we show here, both approaches can be very brittle. Moreover, neither approach exploits the observation that different sensors may be considered “experts” on different classes. In this work, we introduce a novel framework for multi-dimensional time series classification that weights the class prediction from each time series stream. These weights are based not only on each stream's previous track record on the class it is currently predicting, but also on the distance from the unlabeled object. As we demonstrate with extensive experiments on real data, our method is more accurate than current approaches and particularly robust in the face of concept drift or sensor noise.},
author = {Hu, Bing and Chen, Yanping and Zakaria, Jesin and Ulanova, Liudmila and Keogh, Eamonn},
doi = {10.1109/ICDM.2013.33},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2013 - Classification of multi-dimensional streaming time series by weighting each classifier's track record.pdf:pdf},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {classification,multi-dimensional time series},
pages = {281--290},
title = {{Classification of multi-dimensional streaming time series by weighting each classifier's track record}},
year = {2013}
}
@article{Yu2006,
abstract = {We consider the problem of multi-task learning, that is, learning multiple related functions. Our approach is based on a hierarchical Bayesian framework, that exploits the equivalence between parametric linear models and nonparametric Gaussian processes (GPs). The resulting models can be learned easily via an EM-algorithm. Empirical studies on multi-label text categorization suggest that the presented models allow accurate solutions of these multi-task problems.},
author = {Yu, Kai and Tresp, Volker and Schwaighofer, Anton},
doi = {10.1145/1102351.1102479},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Tresp, Schwaighofer - 2006 - Learning Gaussian processes from multiple tasks.pdf:pdf},
pages = {1012--1019},
title = {{Learning Gaussian processes from multiple tasks}},
year = {2006}
}
@article{Maris2007,
abstract = {In this paper, we show how ElectroEncephaloGraphic (EEG) and MagnetoEncephaloGraphic (MEG) data can be analyzed statistically using nonparametric techniques. Nonparametric statistical tests offer complete freedom to the user with respect to the test statistic by means of which the experimental conditions are compared. This freedom provides a straightforward way to solve the multiple comparisons problem (MCP) and it allows to incorporate biophysically motivated constraints in the test statistic, which may drastically increase the sensitivity of the statistical test. The paper is written for two audiences: (1) empirical neuroscientists looking for the most appropriate data analysis method, and (2) methodologists interested in the theoretical concepts behind nonparametric statistical tests. For the empirical neuroscientist, a large part of the paper is written in a tutorial-like fashion, enabling neuroscientists to construct their own statistical test, maximizing the sensitivity to the expected effect. And for the methodologist, it is explained why the nonparametric test is formally correct. This means that we formulate a null hypothesis (identical probability distribution in the different experimental conditions) and show that the nonparametric test controls the false alarm rate under this null hypothesis. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Maris, Eric and Oostenveld, Robert},
doi = {10.1016/j.jneumeth.2007.03.024},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Maris, Oostenveld - 2007 - Nonparametric statistical testing of EEG- and MEG-data.pdf:pdf},
isbn = {0165-0270 (Print)$\backslash$r0165-0270 (Linking)},
issn = {01650270},
journal = {Journal of Neuroscience Methods},
keywords = {EEG,Hypothesis testing,MEG,Multiple comparisons problem,Nonparametric statistical testing},
number = {1},
pages = {177--190},
pmid = {17517438},
title = {{Nonparametric statistical testing of EEG- and MEG-data}},
volume = {164},
year = {2007}
}
@article{Yang2018b,
abstract = {In this paper, we propose an active learning algorithm and models which can gradually learn individual's preference through pairwise comparisons. The active learning scheme aims at finding individual's most preferred choice with minimized number of pairwise comparisons. The pairwise comparisons are encoded into probabilistic models based on assumptions of choice models and deep Gaussian processes. The next-to-compare decision is determined by a novel acquisition function. We benchmark the proposed algorithm and models using functions with multiple local optima and one public airline itinerary dataset. The experiments indicate the effectiveness of our active learning algorithm and models.},
archivePrefix = {arXiv},
arxivId = {1805.01867},
author = {Yang, Jie and Klabjan, Diego},
eprint = {1805.01867},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Klabjan - 2018 - Bayesian active learning for choice models with deep Gaussian processes.pdf:pdf},
keywords = {active learning,choice models,deep gaussian processes},
pages = {1--27},
title = {{Bayesian active learning for choice models with deep Gaussian processes}},
url = {http://arxiv.org/abs/1805.01867},
year = {2018}
}
@article{Zafeiriou,
author = {Zafeiriou, Stefanos and Schuller, W},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zafeiriou, Schuller - Unknown - Deep Canonical Time Warping.pdf:pdf},
title = {{Deep Canonical Time Warping}}
}
@article{Betancourt2013,
author = {Betancourt, Michael and Carlo, Hamiltonian Monte and Monte, Hamiltonian},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Betancourt, Carlo, Monte - 2013 - The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling.pdf:pdf},
title = {{The Fundamental Incompatibility of Scalable Hamiltonian Monte Carlo and Naive Data Subsampling}},
year = {2013}
}
@article{Hensman2015a,
abstract = {Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper will be available shortly.},
archivePrefix = {arXiv},
arxivId = {1506.04000},
author = {Hensman, James and Matthews, Alexander G. de G. and Filippone, Maurizio and Ghahramani, Zoubin},
doi = {10.1103/PhysRevB.91.180402},
eprint = {1506.04000},
issn = {10495258},
title = {{MCMC for Variationally Sparse Gaussian Processes}},
year = {2015}
}
@article{Milletari2016,
abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
archivePrefix = {arXiv},
arxivId = {1606.04797},
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed Ahmad},
doi = {10.1109/3DV.2016.79},
eprint = {1606.04797},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Milletari, Navab, Ahmadi - 2016 - V-Net Fully convolutional neural networks for volumetric medical image segmentation.pdf:pdf},
isbn = {9781509054077},
journal = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
keywords = {Deep learning,convolutional neural networks,machine learning,prostate,segmentation},
pages = {565--571},
pmid = {1000436750},
title = {{V-Net: Fully convolutional neural networks for volumetric medical image segmentation}},
year = {2016}
}
@article{Vul2014,
abstract = {In many learning or inference tasks human behavior approximates that of a Bayesian ideal observer, suggesting that, at some level, cognition can be described as Bayesian inference. However, a number of findings have highlighted an intriguing mismatch between human behavior and standard assumptions about optimality: People often appear to make decisions based on just one or a few samples from the appropriate posterior probability distribution, rather than using the full distribution. Although sampling-based approximations are a common way to implement Bayesian inference, the very limited numbers of samples often used by humans seem insufficient to approximate the required probability distributions very accurately. Here, we consider this discrepancy in the broader framework of statistical decision theory, and ask: If people are making decisions based on samples--but as samples are costly--how many samples should people use to optimize their total expected or worst-case reward over a large number of decisions? We find that under reasonable assumptions about the time costs of sampling, making many quick but locally suboptimal decisions based on very few samples may be the globally optimal strategy over long periods. These results help to reconcile a large body of work showing sampling-based or probability matching behavior with the hypothesis that human cognition can be understood in Bayesian terms, and they suggest promising future directions for studies of resource-constrained cognition.},
archivePrefix = {arXiv},
arxivId = {1604.05129},
author = {Vul, Edward and Goodman, Noah and Griffiths, Thomas L. and Tenenbaum, Joshua B.},
doi = {10.1111/cogs.12101},
eprint = {1604.05129},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive Science},
number = {4},
pmid = {24467492},
title = {{One and done? Optimal decisions from very few samples}},
volume = {38},
year = {2014}
}
@article{Baldassano2017,
author = {Baldassano, Christopher and Chen, Janice and Zadbood, Asieh and Pillow, Jonathan W and Hasson, Uri and Norman, Kenneth A},
doi = {10.1016/j.neuron.2017.06.041},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Baldassano et al. - 2017 - Discovering Event Structure in Continuous Narrative Perception and Memory.pdf:pdf},
issn = {08966273},
journal = {Neuron},
keywords = {event model,event segmentation,fmri,hidden markov model,hippocampus,memory,narrative,perception,recall,reinstatement,situation model},
month = {aug},
number = {3},
pages = {709--721.e5},
title = {{Discovering Event Structure in Continuous Narrative Perception and Memory}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627317305937},
volume = {95},
year = {2017}
}
@article{Johnson2014,
abstract = {1) Mini-batch - set of independent series$\backslash$r$\backslash$n2) Dependencies do not need to be broken},
author = {Johnson, Matthew and Willsky, Alan},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Willsky - 2014 - Stochastic Variational Inference for Bayesian Time Series Models.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
pages = {1854--1862},
title = {{Stochastic Variational Inference for Bayesian Time Series Models}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2{\_}johnson14},
year = {2014}
}
@article{Hasson2004,
abstract = {We propose an experimental arrangement to image, with attosecond resolution, transient surface plasmonic excitations. The required modifications to state-of-the-art setups used for attosecond streaking experiments from solid surfaces only involve available technology. Buildup and life times of surface plasmon polaritons can be extracted and local modulations of the exciting optical pulse can be diagnosed {\{}$\backslash$it in situ{\}}.},
archivePrefix = {arXiv},
arxivId = {1401.4290},
author = {Hasson, U.},
doi = {10.1126/science.1089506},
eprint = {1401.4290},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hasson - 2004 - Intersubject Synchronization of Cortical Activity During Natural Vision.pdf:pdf},
isbn = {1095-9203 (Electronic)},
issn = {0036-8075},
journal = {Science},
keywords = {Adult,Attention,Auditory Cortex,Auditory Cortex: physiology,Brain Mapping,Cerebral Cortex,Cerebral Cortex: physiology,Emotions,Face,Female,Humans,Magnetic Resonance Imaging,Male,Middle Aged,Motion Pictures as Topic,Occipital Lobe,Occipital Lobe: physiology,Ocular,Photic Stimulation,Temporal Lobe,Temporal Lobe: physiology,Vision,Visual Cortex,Visual Cortex: physiology,Visual Perception},
month = {mar},
number = {5664},
pages = {1634--1640},
pmid = {15016991},
title = {{Intersubject Synchronization of Cortical Activity During Natural Vision}},
url = {http://science.sciencemag.org/content/303/5664/1634.abstract http://arxiv.org/abs/1401.4290 http://dx.doi.org/10.1103/PhysRevLett.113.113903 http://www.sciencemag.org/cgi/doi/10.1126/science.1089506},
volume = {303},
year = {2004}
}
@article{Regier2017,
abstract = {We introduce TrustVI, a fast second-order algorithm for black-box variational inference based on trust-region optimization and the reparameterization trick. At each iteration, TrustVI proposes and assesses a step based on minibatches of draws from the variational distribution. The algorithm provably converges to a stationary point. We implemented TrustVI in the Stan framework and compared it to two alternatives: Automatic Differentiation Variational Inference (ADVI) and Hessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is based on stochastic first-order optimization. The latter uses second-order information, but lacks convergence guarantees. TrustVI typically converged at least one order of magnitude faster than ADVI, demonstrating the value of stochastic second-order information. TrustVI often found substantially better variational distributions than HFSGVI, demonstrating that our convergence theory can matter in practice.},
archivePrefix = {arXiv},
arxivId = {1706.02375},
author = {Regier, Jeffrey and Jordan, Michael I. and McAuliffe, Jon},
eprint = {1706.02375},
issn = {10495258},
title = {{Fast Black-box Variational Inference through Stochastic Trust-Region Optimization}},
year = {2017}
}
@article{Gardner2015,
abstract = {Many tasks in computer vision can be cast as a "label changing" problem, where the goal is to make a semantic change to the appearance of an image or some subject in an image in order to alter the class membership. Although successful task-specific methods have been developed for some label changing applications, to date no general purpose method exists. Motivated by this we propose deep manifold traversal, a method that addresses the problem in its most general form: it first approximates the manifold of natural images then morphs a test image along a traversal path away from a source class and towards a target class while staying near the manifold throughout. The resulting algorithm is surprisingly effective and versatile. It is completely data driven, requiring only an example set of images from the desired source and target domains. We demonstrate deep manifold traversal on highly diverse label changing tasks: changing an individual's appearance (age and hair color), changing the season of an outdoor image, and transforming a city skyline towards nighttime.},
archivePrefix = {arXiv},
arxivId = {1511.06421},
author = {Gardner, Jacob R. and Upchurch, Paul and Kusner, Matt J. and Li, Yixuan and Weinberger, Kilian Q. and Bala, Kavita and Hopcroft, John E.},
eprint = {1511.06421},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gardner et al. - 2015 - Deep Manifold Traversal Changing Labels with Convolutional Features.pdf:pdf},
title = {{Deep Manifold Traversal: Changing Labels with Convolutional Features}},
url = {http://arxiv.org/abs/1511.06421},
year = {2015}
}
@article{Schad2019,
abstract = {Experiments in research on memory, language, and in other areas of cognitive science are increasingly being analyzed using Bayesian methods. This has been facilitated by the development of probabilistic programming languages such as Stan, and easily accessible front-end packages such as brms. However, the utility of Bayesian methods ultimately depends on the relevance of the Bayesian model, in particular whether or not it accurately captures the structure of the data and the data analyst's domain expertise. Even with powerful software, the analyst is responsible for verifying the utility of their model. To accomplish this, we introduce a principled Bayesian workflow (Betancourt, 2018) to cognitive science. Using a concrete working example, we describe basic questions one should ask about the model: prior predictive checks, computational faithfulness, model sensitivity, and posterior predictive checks. The running example for demonstrating the workflow is data on reading times with a linguistic manipulation of object versus subject relative sentences. This principled Bayesian workflow also demonstrates how to use domain knowledge to inform prior distributions. It provides guidelines and checks for valid data analysis, avoiding overfitting complex models to noise, and capturing relevant data structure in a probabilistic model. Given the increasing use of Bayesian methods, we aim to discuss how these methods can be properly employed to obtain robust answers to scientific questions. All data and code accompanying this paper are available from https://osf.io/b2vx9/.},
archivePrefix = {arXiv},
arxivId = {1904.12765},
author = {Schad, Daniel J. and Betancourt, Michael and Vasishth, Shravan},
eprint = {1904.12765},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Schad, Betancourt, Vasishth - 2019 - Toward a principled Bayesian workflow in cognitive science.pdf:pdf},
title = {{Toward a principled Bayesian workflow in cognitive science}},
url = {http://arxiv.org/abs/1904.12765},
year = {2019}
}
@article{Krishnan2016,
abstract = {Reinforcement Learning (RL) struggles in problems with delayed rewards, and one approach is to segment the task into sub-tasks with incremental rewards. We propose a framework called Hierarchical Inverse Reinforcement Learning (HIRL), which is a model for learning sub-task structure from demonstrations. HIRL decomposes the task into sub-tasks based on transitions that are consistent across demonstrations. These transitions are defined as changes in local linearity w.r.t to a kernel function. Then, HIRL uses the inferred structure to learn reward functions local to the sub-tasks but also handle any global dependencies such as sequentiality. We have evaluated HIRL on several standard RL benchmarks: Parallel Parking with noisy dynamics, Two-Link Pendulum, 2D Noisy Motion Planning, and a Pinball environment. In the parallel parking task, we find that rewards constructed with HIRL converge to a policy with an 80{\%} success rate in 32{\%} fewer time-steps than those constructed with Maximum Entropy Inverse RL (MaxEnt IRL), and with partial state observation, the policies learned with IRL fail to achieve this accuracy while HIRL still converges. We further find that that the rewards learned with HIRL are robust to environment noise where they can tolerate 1 stdev. of random perturbation in the poses in the environment obstacles while maintaining roughly the same convergence rate. We find that HIRL rewards can converge up-to 6x faster than rewards constructed with IRL.},
archivePrefix = {arXiv},
arxivId = {1604.06508},
author = {Krishnan, Sanjay and Garg, Animesh and Liaw, Richard and Miller, Lauren and Pokorny, Florian T. and Goldberg, Ken},
eprint = {1604.06508},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Krishnan et al. - 2016 - HIRL Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks with Delayed Rewards.pdf:pdf},
number = {1},
title = {{HIRL: Hierarchical Inverse Reinforcement Learning for Long-Horizon Tasks with Delayed Rewards}},
url = {http://arxiv.org/abs/1604.06508},
year = {2016}
}
@article{Wang2013,
author = {Wang, Yuyang},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang - 2013 - Nonparametric Bayesian Mixed-effects Models for Multi-task Learning.pdf:pdf},
title = {{Nonparametric Bayesian Mixed-effects Models for Multi-task Learning}},
year = {2013}
}
@article{Uga2014,
abstract = {An increasing number of functional near-infrared spectroscopy (fNIRS) studies utilize a general linear model (GLM) approach, which serves as a standard statistical method for functional magnetic resonance imaging (fMRI) data analysis. While fMRI solely measures the blood oxygen level dependent (BOLD) signal, fNIRS measures the changes of oxy-hemoglobin (oxy-Hb) and deoxy-hemoglobin (deoxy-Hb) signals at a temporal resolution severalfold higher. This suggests the necessity of adjusting the temporal parameters of a GLM for fNIRS signals. Thus, we devised a GLM-based method utilizing an adaptive hemodynamic response function (HRF). We sought the optimum temporal parameters to best explain the observed time series data during verbal fluency and naming tasks. The peak delay of the HRF was systematically changed to achieve the best-fit model for the observed oxy- and deoxy-Hb time series data. The optimized peak delay showed different values for each Hb signal and task. When the optimized peak delays were adopted, the deoxy-Hb data yielded comparable activations with similar statistical power and spatial patterns to oxy-Hb data. The adaptive HRF method could suitably explain the behaviors of both Hb parameters during tasks with the different cognitive loads during a time course, and thus would serve as an objective method to fully utilize the temporal structures of all fNIRS data.},
author = {Uga, Minako and Dan, Ippeita and Sano, Toshifumi and Dan, Haruka and Watanabe, Eiju},
doi = {10.1117/1.nph.1.1.015004},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Uga et al. - 2014 - Optimizing the general linear model for functional near-infrared spectroscopy an adaptive hemodynamic response funct.pdf:pdf},
issn = {2329-423X},
journal = {Neurophotonics},
keywords = {cortical hemodynamics,diffuse optical imaging,optical topography,regression analysis,statistical,statistical analysis},
number = {1},
pages = {015004},
title = {{Optimizing the general linear model for functional near-infrared spectroscopy: an adaptive hemodynamic response function approach}},
volume = {1},
year = {2014}
}
@article{Campbell2017,
abstract = {The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.},
archivePrefix = {arXiv},
arxivId = {1710.05053},
author = {Campbell, Trevor and Broderick, Tamara},
eprint = {1710.05053},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Campbell, Broderick - 2017 - Automated Scalable Bayesian Inference via Hilbert Coresets.pdf:pdf},
pages = {1--36},
title = {{Automated Scalable Bayesian Inference via Hilbert Coresets}},
url = {http://arxiv.org/abs/1710.05053},
year = {2017}
}
@article{Scarpa2013,
abstract = {Functional near-infrared spectroscopy (fNIRS) uses near-infrared light to measure cortical concentration changes in oxygenated (HbO) and deoxygenated hemoglobin (HbR) held to be correlated with cognitive activity. Providing a parametric depiction of such changes in the classic form of stimulus-evoked hemodynamic responses (HRs) can be attained with this technique only by solving two problems. One problem concerns the separation of informative optical signal from structurally analogous noise generated by a variety of spurious sources, such as heart beat, respiration, and vasomotor waves. Another problem pertains to the inherent variability of HRs, which is notoriously contingent on the type of experiment, brain region monitored, and human phenotype. A novel method was devised in the present context to solve both problems based on a two-step algorithm combining the treatment of noise-only data extrapolated from a reference-channel and a Bayesian filter applied on a per-trial basis. The present method was compared to two current methods based on conventional averaging, namely, a typical averaging method and an averaging method implementing the use of a reference-channel. The result of the comparison, carried out both on artificial and real data, revealed a sensitive accuracy improvement in HR estimation using the present method relative to each of the other methods. {\textcopyright} 2013 Elsevier Inc.},
author = {Scarpa, Fabio and Brigadoi, Sabrina and Cutini, Simone and Scatturin, Pietro and Zorzi, Marco and Dell'Acqua, Roberto and Sparacino, Giovanni},
doi = {10.1016/j.neuroimage.2013.01.021},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Scarpa et al. - 2013 - A reference-channel based methodology to improve estimation of event-related hemodynamic response from fNIRS meas.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Bayesian filtering,FNIRS,Near-infrared spectroscopy,Physiological noise,Reference-channel},
pages = {106--119},
publisher = {Elsevier Inc.},
title = {{A reference-channel based methodology to improve estimation of event-related hemodynamic response from fNIRS measurements}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2013.01.021},
volume = {72},
year = {2013}
}
@article{Zhang2017,
abstract = {Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.{\~{}}fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1712.02390},
author = {Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
doi = {10.1007/BF02715046},
eprint = {1712.02390},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2017 - Noisy Natural Gradient as Variational Inference.pdf:pdf},
issn = {02506335},
title = {{Noisy Natural Gradient as Variational Inference}},
year = {2017}
}
@article{Eslami2016,
abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
archivePrefix = {arXiv},
arxivId = {1603.08575},
author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Kavukcuoglu, Koray and Hinton, Geoffrey E.},
doi = {10.1038/nature14236},
eprint = {1603.08575},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Eslami et al. - 2016 - Attend, Infer, Repeat Fast Scene Understanding with Generative Models.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {10450823},
pmid = {25719670},
title = {{Attend, Infer, Repeat: Fast Scene Understanding with Generative Models}},
url = {http://arxiv.org/abs/1603.08575},
year = {2016}
}
@article{Wu,
author = {Wu, Te-lin and Zamir, Amir R and Silvio, Prof and Cvgl, Savarese},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - Unknown - Feedback based Neural Networks.pdf:pdf},
title = {{Feedback based Neural Networks}}
}
@article{Low2018,
abstract = {A novel fouling-resistant nanofiltration mixed-matrix membrane was obtained by the incorporation of 2D boron nitride nanosheets (BNNS) in polyethersulfone (PES). The addition of just 0.05 wt{\%} of BNNS into the PES matrix led to a 4-fold increase in pure water permeance with a 10{\%} decrease in the rejection of the dye Rose Bengal; up to 95{\%} rejection of humic acid and nearly 100{\%} flux recovery over two cycles in cross-flow fouling tests without the need for chemical cleansing. This performance is attributed to the uniform distribution of the BNNS in the PES matrix, observed via Raman mapping, and the surface chemistry and structure of the BNNS, which hydrophilised the polymer matrix and reduced its surface roughness. The low amount of BNNS filler needed to render the mixed-matrix membrane fouling-resistant opens the way to its use in waste-water treatment applications where organic fouling remains a major challenge.},
archivePrefix = {arXiv},
arxivId = {1806.01979v1},
author = {Low, Ze Xian and Ji, Jing and Blumenstock, David and Chew, Yong Min and Wolverson, Daniel and Mattia, Davide},
doi = {10.1016/j.memsci.2018.07.003},
eprint = {1806.01979v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Low et al. - 2018 - Fouling resistant 2D boron nitride nanosheet – PES nanofiltration membranes.pdf:pdf},
issn = {18733123},
journal = {Journal of Membrane Science},
keywords = {Boron nitride nanosheets,Fouling,Nanofiltration,PES},
pages = {949--956},
title = {{Fouling resistant 2D boron nitride nanosheet – PES nanofiltration membranes}},
volume = {563},
year = {2018}
}
@article{Dai2017,
abstract = {Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) and that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP, of which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.},
archivePrefix = {arXiv},
arxivId = {1705.09862},
author = {Dai, Zhenwen and {\'{A}}lvarez, Mauricio A. and Lawrence, Neil D.},
eprint = {1705.09862},
title = {{Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes}},
year = {2017}
}
@article{Muandet2017,
abstract = {A Hilbert space embedding of a distribution-in short, a kernel mean embedding-has recently emerged as a powerful tool for machine learning and statistical inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines (SVMs) and other kernel methods. In addition to the classical applications of kernel methods, the kernel mean embedding has found novel applications in fields ranging from probabilistic modeling to statistical inference, causal discovery, and deep learning. This survey aims to give a comprehensive review of existing work and recent advances in this research area, and to discuss challenging issues and open problems that could potentially lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules-which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning- in a non-parametric way using this new representation of distributions. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
archivePrefix = {arXiv},
arxivId = {1605.09522},
author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1561/2200000060},
eprint = {1605.09522},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Muandet et al. - 2017 - Kernel mean embedding of distributions A review and beyond.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
number = {1-2},
pages = {1--141},
title = {{Kernel mean embedding of distributions: A review and beyond}},
volume = {10},
year = {2017}
}
@article{Karaletsos2015,
abstract = {Representation learning systems typically rely on massive amounts of labeled data in order to be trained to high accuracy. Recently, high-dimensional parametric models like neural networks have succeeded in building rich representations using either compressive, reconstructive or supervised criteria. However, the semantic structure inherent in observations is oftentimes lost in the process. Human perception excels at understanding semantics but cannot always be expressed in terms of labels. Thus, $\backslash$emph{\{}oracles{\}} or $\backslash$emph{\{}human-in-the-loop systems{\}}, for example crowdsourcing, are often employed to generate similarity constraints using an implicit similarity function encoded in human perception. In this work we propose to combine $\backslash$emph{\{}generative unsupervised feature learning{\}} with a $\backslash$emph{\{}probabilistic treatment of oracle information like triplets{\}} in order to transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian latent factor models of the observations. We use a fast variational algorithm to learn the joint model and demonstrate applicability to a well-known image dataset. We show how implicit triplet information can provide rich information to learn representations that outperform previous metric learning approaches as well as generative models without this side-information in a variety of predictive tasks. In addition, we illustrate that the proposed approach compartmentalizes the latent spaces semantically which allows interpretation of the latent variables.},
archivePrefix = {arXiv},
arxivId = {1506.05011},
author = {Karaletsos, Theofanis and Belongie, Serge and R{\"{a}}tsch, Gunnar},
eprint = {1506.05011},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Karaletsos, Belongie, R{\"{a}}tsch - 2015 - Bayesian representation learning with oracle constraints.pdf:pdf},
pages = {1--16},
title = {{Bayesian representation learning with oracle constraints}},
url = {http://arxiv.org/abs/1506.05011},
year = {2015}
}
@article{Charles2017,
abstract = {{The Kalman Filtering process seeks to discover an underlying set of state variables {\{}x k {\}} for k ∈ [0, n] given a set of measurements {\{}y k {\}}. The process and measurement equations are both linear and given by x n+1 = F n x n + $\nu$ o,n+1 (1) y n = $\Phi$x n + $\nu$ d,n . (2) The Kalman filter wants to find, at each iteration, the most likely cause of the mea-surement y n given the approximation made by a flawed estimation (the linear dynamics F n . Figure 1 shows a 2-dimensional graphical depiction. What is important here is not only that we have the measurement and the prediction, but knowledge of how each is flawed. In the Kalman case, this knowledge is given by the covariance matrixes (essentially fully describing the distribution of the measurement and prediction for the Gaussian case). In Figure 1, this knowledge is represented by the ovals surrounding each point. The power of the Kalman fil-ter comes from it's ability not only to perform this estimation once (a simple Bayesian task), but to use both estimates and knowledge of their distributions to find a distribution for the updated estimate, thus iteratively calculating the best solution for state at each iteration. While many derivations of the Kalman filter are available, utilizing the orthogonality principle or finding iterative updates to the Best Linear Unbiased Estimator (BLUE), I will derive the Kalman Gilter here using a Bayesian approach, where 'best' is interpreted in the Maximum A-Posteriori (MAP) sense instead of an L 2 sense (which for Gaussian innovations and measurement noise is the same estimate). Bayesian analysis uses Bayes rule, p(a|b)p(b) = p(b|a)p(a), to express the posterior probability in terms of the likelihood and the prior. In this case we want to optimize over all states x k : x k {\}} k∈[0,n] = arg max n i=1 p(x i |x i−1)p(y i |x i) p(y 0 |x 0)p(x 0) (3) = arg max p(y n |x n)p(x n |x n−1) n−1 i=1 p(x i |x i−1)p(y i |x i) p(y 0 |x 0)p(x 0) (4) 1 Figure 1: The Kalman filter uses the prediction of a current state based on a previous estimate (blue points) in conjunction with a current measurement (red point) to estimate the true current state (green point). The error in the dynamics (shown here by the blue ovals which represent the covariance)is a combination of the error in the past state and the error in the model of the system. This error in conjunction with the measurement error (the red ovals) allow the covariance of the state update (green oval) to be calculated, propogating forward the confidence of each update. 2},
author = {Charles, Adam S},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Charles - 2017 - Kalman Filtering A Bayesian Approach.pdf:pdf},
isbn = {9061869552},
number = {6},
pages = {1--3},
title = {{Kalman Filtering: A Bayesian Approach}},
url = {http://www.princeton.edu/{~}adamsc/documents/KalmanFilterBayesDerivation.pdf},
year = {2017}
}
@article{Bitzer2015,
abstract = {Even for simple perceptual decisions, the mechanisms that the brain employs are still under debate. Although current consensus states that the brain accumulates evidence extracted from noisy sensory information, open questions remain about how this simple model relates to other perceptual phenomena such as flexibility in decisions, decision-dependent modulation of sensory gain, or confidence about a decision. We propose a novel approach of how perceptual decisions are made by combining two influential formalisms into a new model. Specifically, we embed an attractor model of decision making into a probabilistic framework that models decision making as Bayesian inference. We show that the new model can explain decision making behaviour by fitting it to experimental data. In addition, the new model combines for the first time three important features: First, the model can update decisions in response to switches in the underlying stimulus. Second, the probabilistic formulation accounts for top-down effects that may explain recent experimental findings of decision-related gain modulation of sensory neurons. Finally, the model computes an explicit measure of confidence which we relate to recent experimental evidence for confidence computations in perceptual decision tasks.},
author = {Bitzer, Sebastian and Bruineberg, Jelle and Kiebel, Stefan J.},
doi = {10.1371/journal.pcbi.1004442},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bitzer, Bruineberg, Kiebel - 2015 - A Bayesian Attractor Model for Perceptual Decision Making.pdf:pdf},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {8},
pages = {1--35},
title = {{A Bayesian Attractor Model for Perceptual Decision Making}},
volume = {11},
year = {2015}
}
@inproceedings{Kim2007a,
author = {Kim, Tae-kyun and Wong, Shu-fai and Cipolla, Roberto},
booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383137},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Wong, Cipolla - 2007 - Tensor Canonical Correlation Analysis for Action Classification.pdf:pdf},
isbn = {1-4244-1179-3},
month = {jun},
number = {June 2007},
pages = {1--8},
publisher = {IEEE},
title = {{Tensor Canonical Correlation Analysis for Action Classification}},
url = {http://ieeexplore.ieee.org/document/4270162/},
year = {2007}
}
@article{Berridge1998,
abstract = {What roles do mesolimbic and neostriatal dopamine systems play in reward? Do they mediate the hedonic impact of rewarding stimuli? Do they mediate hedonic reward learning and associati{\~{O}}e prediction? Our review of the literature, together with results of a new study of residual reward capacity after dopamine depletion, indicates the answer to both questions is ‘no'. Rather, dopamine systems may mediate the incenti{\~{O}}e salience of rewards, modulating their motivational value in a manner separable from hedonia and reward learning. In a study of the consequences of dopamine loss, rats were depleted of dopamine in the nucleus accumbens and neostriatum by up to 99{\%} using 6-hydroxydopamine. In a series of experiments, we applied the ‘taste reactivity' measure of affective reactions gapes, {\v{Z}} etc. to assess the capacity of dopamine-depleted rats for: 1 normal affect hedonic and aversive reactions , 2 modulation of hedonic .. {\v{Z}} . . affect {\v{Z}}. . by associative learning taste aversion conditioning , and 3 hedonic enhancement of affect by non-dopaminergic pharmacological manipulation {\v{Z}}.of palatability benzodiazepine administration . We found normal hedonic reaction patterns to sucrose vs. quinine, normal learning {\v{Z}}.of new hedonic stimulus values a change in palatability based on predictive relations , and normal pharmacological hedonic enhancement of palatability. We discuss these results in the context of hypotheses and data concerning the role of dopamine in reward. We review neurochemical, electrophysiological, and other behavioral evidence. We conclude that dopamine systems are not needed either to mediate the hedonic pleasure of reinforcers or to mediate predictive associations involved in hedonic reward learning. We conclude instead that dopamine may be more important to incenti{\~{O}}e salience attributions to the neural representations of reward-related stimuli. Incentive salience, we suggest, is a distinct component of motivation and reward. In other words, dopamine systems are necessary for ‘wanting' incenti{\~{O}}es, but not for ‘liking' them or for learning new ‘likes' and ‘dislikes'. q1998 Elsevier Science B.V. All rights reserved.},
author = {Berridge, Kent C and Robinson, Terry E},
doi = {10.1016/S0165-0173(98)00019-8},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Berridge, Robinson - 1998 - What is the role of dopamine in reward Hedonics, learning, or incentive salience.pdf:pdf},
isbn = {0165-0173},
issn = {01650173},
journal = {Brain Research Reviews},
keywords = {6-hydroxydopamine,addiction,affect,anhedonia,aphagia,appetite,aversion,brain,conditioned taste aversion,conditioning,consummatory behavior,diazepam,dopamine,emotion,feeding,food intake appetitive behavior,hedonic,ingestion,lateral hypothalamus,mesolimbic,mesotelencephalic mesostriatal,motivation,neostriatum,neurotransmitters,nigrostriatal,nucleus accumbens,palatability,pleasure,reactivity,reinforcement,reward,self-administration,substance-related disorders,substantia nigra,taste,tegmentum},
pages = {308--67},
pmid = {9858756},
title = {{What is the role of dopamine in reward: Hedonics, learning, or incentive salience?}},
volume = {28},
year = {1998}
}
@article{Leen2006,
abstract = {Abstract. We investigate a nonparametric model with which to visualize the relationship between two datasets. We base our model on Gaussian Process Latent Variable Models (GPLVM)[1],[2], a probabilistically defined latent variable model which takes the alternative ...},
author = {Leen, Gayle and Fyfe, Colin},
doi = {10.1.1.136.8302},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Leen, Fyfe - 2006 - A Gaussian process latent variable model formulation of canonical correlation analysis.pdf:pdf},
isbn = {2930307064},
journal = {Esann},
number = {April},
pages = {413--418},
title = {{A Gaussian process latent variable model formulation of canonical correlation analysis.}},
url = {http://users.ics.aalto.fi/gleen/es2006-104.pdf},
volume = {1},
year = {2006}
}
@article{Kodirov2017,
abstract = {Existing zero-shot learning (ZSL) models typically learn a projection function from a feature space to a semantic embedding space (e.g.{\~{}}attribute space). However, such a projection function is only concerned with predicting the training seen class semantic representation (e.g.{\~{}}attribute prediction) or classification. When applied to test data, which in the context of ZSL contains different (unseen) classes without training data, a ZSL model typically suffers from the project domain shift problem. In this work, we present a novel solution to ZSL based on learning a Semantic AutoEncoder (SAE). Taking the encoder-decoder paradigm, an encoder aims to project a visual feature vector into the semantic space as in the existing ZSL models. However, the decoder exerts an additional constraint, that is, the projection/code must be able to reconstruct the original visual feature. We show that with this additional reconstruction constraint, the learned projection function from the seen classes is able to generalise better to the new unseen classes. Importantly, the encoder and decoder are linear and symmetric which enable us to develop an extremely efficient learning algorithm. Extensive experiments on six benchmark datasets demonstrate that the proposed SAE outperforms significantly the existing ZSL models with the additional benefit of lower computational cost. Furthermore, when the SAE is applied to supervised clustering problem, it also beats the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1704.08345},
author = {Kodirov, Elyor and Xiang, Tao and Gong, Shaogang},
doi = {10.1109/CVPR.2017.473},
eprint = {1704.08345},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Kodirov, Xiang, Gong - 2017 - Semantic autoencoder for zero-shot learning.pdf:pdf},
isbn = {9781538604571},
issn = {0449296X},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {4447--4456},
title = {{Semantic autoencoder for zero-shot learning}},
volume = {2017-Janua},
year = {2017}
}
@article{Wilson2015a,
abstract = {Bayesian nonparametric models, such as Gaussian processes, provide a compelling framework for automatic statistical modelling: these models have a high degree of flexibility, and automatically calibrated complexity. However, automating human expertise remains elusive; for example, Gaussian processes with standard kernels struggle on function extrapolation problems that are trivial for human learners. In this paper, we create function extrapolation problems and acquire human responses, and then design a kernel learning framework to reverse engineer the inductive biases of human learners across a set of behavioral experiments. We use the learned kernels to gain psychological insights and to extrapolate in human-like ways that go beyond traditional stationary and polynomial kernels. Finally, we investigate Occam's razor in human and Gaussian process based function learning.},
archivePrefix = {arXiv},
arxivId = {1510.07389},
author = {Wilson, Andrew Gordon and Dann, Christoph and Lucas, Christopher G. and Xing, Eric P.},
eprint = {1510.07389},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilson et al. - 2015 - The Human Kernel.pdf:pdf},
pages = {1--11},
title = {{The Human Kernel}},
url = {http://arxiv.org/abs/1510.07389},
year = {2015}
}
@article{Steyvers2012,
abstract = {The idea of viewing human cognition as a rational solution to computational problems posed by the environment has influenced several recent theories of human memory. The first rational models of memory demonstrated that human memory seems to be remarkably well adapted to environmental statistics but made only minimal assumptions about the form of the environmental information represented in memory. Recently, several probabilistic methods for representing the latent semantic structure of language have been developed, drawing on research in computer science, statistics and computational linguistics. These methods provide a means of extending rational models of memory retrieval to linguistic stimuli, and a way to explore the influence of the statistics of language on human memory.},
author = {Steyvers, Mark and Griffiths, Thomas L.},
doi = {10.1093/acprof:oso/9780199216093.003.0015},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Steyvers, Griffiths - 2012 - Rational analysis as a link between human memory and information retrieval.pdf:pdf},
isbn = {9780191695971},
journal = {The Probabilistic Mind: Prospects for Bayesian cognitive science},
keywords = {Bayesian analysis,Cognitive psychologists,Human memory,Information retrieval,Knowledge,Memory organization,Memory retrieval},
pages = {327--347},
title = {{Rational analysis as a link between human memory and information retrieval}},
year = {2012}
}
@article{VandenBoogaart2001,
abstract = {Physical laws are often expressed in terms of partial differential equations. However these laws completely determine the observable process only, when sufficiently many boundary conditions are known. We can use numerical methods like splines or finite elements to interpolate such processes. Kriging is a method to interpolate stationary random processes based on their estimated second order moments, typically unaware of the physical law governing the observed spatial process. For linear partial differential equations both approaches can be unified. It can be shown that linear differential equations impose restrictions on the class of admissible variograms and trend models. Thus the known physical law can help to select a physically reasonable variogram model. When these restrictions are honored, then the resulting universal kriging estimates and conditional simulations solve the differential equation in mean square sense. As a introductory example the problem of kriging the gravity potential of the earth in free space is considered and it is shown that none of the commonly used variogram models is admissible. General methods to construct physically admissible variograms are shown.},
author = {van den Boogaart, Karl Gerald},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/van den Boogaart - 2001 - Kriging for processes solving partial differential equations.pdf:pdf},
journal = {IAMG2001, Cancun, Mexiko},
number = {July},
pages = {1--21},
title = {{Kriging for processes solving partial differential equations}},
url = {http://www.kgs.ku.edu/Conferences/IAMG//Sessions/E/Papers/boogaart.pdf},
year = {2001}
}
@article{Ulrich2014,
abstract = {用LFP的数据，加上iHMM和高斯过程，搞了更好的brain state的特征学习},
author = {Ulrich, Kyle R and Carlson, David E and Lian, Wenzhao and Borg, Jana Schaich and Dzirasa, Kafui and Carin, Lawrence},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ulrich et al. - 2014 - Analysis of Brain States from Multi-Region LFP Time-Series.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2483--2491},
pmid = {613105},
title = {{Analysis of Brain States from Multi-Region LFP Time-Series}},
year = {2014}
}
@article{Willia1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Willia, Ronald J.},
doi = {10.1023/A:1022672621406},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Willia - 1992 - Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.pdf:pdf},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
number = {3},
pages = {229--256},
pmid = {903},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@article{Li2016,
abstract = {This paper introduces the variational R$\backslash$'enyi bound (VR) that extends traditional variational inference to R$\backslash$'enyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.},
archivePrefix = {arXiv},
arxivId = {1602.02311},
author = {Li, Yingzhen and Turner, Richard E.},
eprint = {1602.02311},
isbn = {0034-9356},
issn = {10495258},
title = {{R$\backslash$'enyi Divergence Variational Inference}},
year = {2016}
}
@book{Yasui2009,
abstract = {Statistical Learning from a Regression Perspective considers statistical learning applications when interest centers on the conditional distribution of the response variable, given a set of predictors, and when it is important to characterize how the predictors are related to the response. As a first approximation, this is can be seen as an extension of nonparametric regression. Among the statistical learning procedures examined are bagging, random forests, boosting, and support vector machines. Response variables may be quantitative or categorical.},
author = {Yasui, Yutaka and Wang, Xiaoming},
booktitle = {Biometrics},
doi = {10.1111/j.1541-0420.2009.01343_5.x},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yasui, Wang - 2009 - Statistical Learning from a Regression Perspective.pdf:pdf},
isbn = {9783319440477},
issn = {0006-341X},
number = {4},
pages = {1309--1310},
title = {{Statistical Learning from a Regression Perspective}},
volume = {65},
year = {2009}
}
@article{Train2018,
abstract = {In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.},
archivePrefix = {arXiv},
arxivId = {1806.05393},
author = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S. and Pennington, Jeffrey},
eprint = {1806.05393},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Xiao et al. - 2018 - Dynamical Isometry and a Mean Field Theory of CNNs How to Train 10,000-Layer Vanilla Convolutional Neural Networks.pdf:pdf},
month = {jun},
title = {{Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1806.05393},
year = {2018}
}
@article{Tan2018,
abstract = {We consider the problem of learning a Gaussian variational approximation to the posterior distribution for a high-dimensional parameter, where we impose sparsity in the precision matrix to reflect appropriate conditional independence structure in the model. Incorporating sparsity in the precision matrix allows the Gaussian variational distribution to be both flexible and parsimonious, and the sparsity is achieved through parameterization in terms of the Cholesky factor. Efficient stochastic gradient methods which make appropriate use of gradient information for the target distribution are developed for the optimization. We consider alternative estimators of the stochastic gradients which have lower variation and are more stable. Our approach is illustrated using generalized linear mixed models and state space models for time series.},
author = {Tan, Linda S.L. and Nott, David J.},
doi = {10.1007/s11222-017-9729-7},
issn = {15731375},
journal = {Statistics and Computing},
number = {2},
title = {{Gaussian variational approximation with sparse precision matrices}},
volume = {28},
year = {2018}
}
@article{Vapnik2015,
abstract = {V. Vapnik and R. Izmailov. Learning using privileged information: Similarity control and knowledge transfer. Journal of Machine Learning Research, 16:2023–2049, 2015.},
author = {Vapnik, Vladimir and Izmailov, Rauf},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Vapnik, Izmailov - 2015 - Learning using privileged information Similarity control and knowledge transfer.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Classification,Frames,Intelligent Teacher,Kernel functions,Knowledge representation,Knowledge transfer,Learning theory,Privileged information,Regression,SVM+,Similarity control,Similarity functions,Support vector machines},
pages = {2023--2049},
title = {{Learning using privileged information: Similarity control and knowledge transfer}},
volume = {16},
year = {2015}
}
@article{Horrace2005,
abstract = {This note formalizes some analytical results on the n-dimensional multivariate truncated normal distribution where truncation is one-sided and at an arbitrary point. Results on linear transformations, marginal and conditional distributions, and independence are provided. Also, results on log-concavity, A-unimodality and the MTP 2 property are derived. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Horrace, William C.},
doi = {10.1016/j.jmva.2004.10.007},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Horrace - 2005 - Some results on the multivariate truncated normal distribution.pdf:pdf},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {A-unimodality,Characteristic function,Log-concavity,MTP2},
number = {1},
pages = {209--221},
title = {{Some results on the multivariate truncated normal distribution}},
volume = {94},
year = {2005}
}
@article{Gigerenzer2011,
abstract = {As reflected in the amount of controversy, few areas in psychology have undergone such dramatic conceptual changes in the past decade as the emerging science of heuristics. Heuristics are efficient cognitive processes, conscious or unconscious, that ignore part of the information. Because using heuristics saves effort, the classical view has been that heuristic decisions imply greater errors than do "rational" decisions as defined by logic or statistical models. However, for many decisions, the assumptions of rational models are not met, and it is an empirical rather than an a priori issue how well cognitive heuristics function in an uncertain world. To answer both the descriptive question ("Which heuristics do people use in which situations?") and the prescriptive question ("When should people rely on a given heuristic rather than a complex strategy to make better judgments?"), formal models are indispensable. We review research that tests formal models of heuristic inference, including in business organizations, health care, and legal institutions. This research indicates that (a) individuals and organizations often rely on simple heuristics in an adaptive way, and (b) ignoring part of the information can lead to more accurate judgments than weighting and adding all information, for instance for low predictability and small samples. The big future challenge is to develop a systematic theory of the building blocks of heuristics as well as the core capacities and environmental structures these exploit.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Gigerenzer, Gerd and Gaissmaier, Wolfgang},
doi = {10.1146/annurev-psych-120709-145346},
eprint = {0402594v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gigerenzer, Gaissmaier - 2011 - Heuristic Decision Making.pdf:pdf;:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gigerenzer, Gaissmaier - 2011 - Heuristic Decision Making(2).pdf:pdf},
isbn = {1545-2085 (Electronic)$\backslash$n0066-4308 (Linking)},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {accuracy-effort trade-off,business decisions,ecological rationality,legal decision making,medical decision making,social intelligence},
number = {1},
pages = {451--482},
pmid = {21126183},
primaryClass = {arXiv:cond-mat},
title = {{Heuristic Decision Making}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-120709-145346},
volume = {62},
year = {2011}
}
@article{Avron2018,
abstract = {Random Fourier features is one of the most popular techniques for scaling up kernel methods, such as kernel ridge regression. However, despite impressive empirical results, the statistical properties of random Fourier features are still not well understood. In this paper we take steps toward filling this gap. Specifically, we approach random Fourier features from a spectral matrix approximation point of view, give tight bounds on the number of Fourier features required to achieve a spectral approximation, and show how spectral matrix approximation bounds imply statistical guarantees for kernel ridge regression. Qualitatively, our results are twofold: on the one hand, we show that random Fourier feature approximation can provably speed up kernel ridge regression under reasonable assumptions. At the same time, we show that the method is suboptimal, and sampling from a modified distribution in Fourier space, given by the leverage function of the kernel, yields provably better performance. We study this optimal sampling distribution for the Gaussian kernel, achieving a nearly complete characterization for the case of low-dimensional bounded datasets. Based on this characterization, we propose an efficient sampling scheme with guarantees superior to random Fourier features in this regime.},
archivePrefix = {arXiv},
arxivId = {1804.09893},
author = {Avron, Haim and Kapralov, Michael and Musco, Cameron and Musco, Christopher and Velingker, Ameya and Zandieh, Amir},
eprint = {1804.09893},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees}},
year = {2018}
}
@article{Chen2018c,
abstract = {In finance, economics and many other fields, observations in a matrix form are often generated over time. For example, a set of key economic indicators are regularly reported in different countries every quarter. The observations at each quarter neatly form a matrix and are observed over many consecutive quarters. Dynamic transport networks with observations generated on the edges can be formed as a matrix observed over time. Although it is natural to turn the matrix observations into a long vector, and then use the standard vector time series models for analysis, it is often the case that the columns and rows of the matrix represent different types of structures that are closely interplayed. In this paper we follow the autoregressive structure for modeling time series and propose a novel matrix autoregressive model in a bilinear form that maintains and utilizes the matrix structure to achieve a greater dimensional reduction, as well as more interpretable results. Probabilistic properties of the models are investigated. Estimation procedures with their theoretical properties are presented and demonstrated with simulated and real examples.},
archivePrefix = {arXiv},
arxivId = {1812.08916},
author = {Chen, Rong and Xiao, Han and Yang, Dan},
eprint = {1812.08916},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Xiao, Yang - 2018 - Autoregressive Models for Matrix-Valued Time Series.pdf:pdf},
keywords = {autoregressive,bilinear,economic indicators,kronecker product,matrix-valued time series,multivariate,nearest kronecker product projection,prediction,time series},
pages = {1--53},
title = {{Autoregressive Models for Matrix-Valued Time Series}},
url = {http://arxiv.org/abs/1812.08916},
year = {2018}
}
@article{Niekum2010,
abstract = {Reward functions in reinforcement learning have largely been assumed given as part of the problem being solved by the agent. However, the psychological notion of intrinsic motivation has recently inspired inquiry into whether there exist alternate reward functions that enable an agent to learn a task more easily than the natural task-based reward function allows. This paper presents a genetic programming algorithm to search for alternate reward functions that improve agent learning performance. We present experiments that show the superiority of these reward functions, demonstrate the possible scalability of our method, and define three classes of problems where reward function search might be particularly useful: distributions of environments, nonstationary environments, and problems with short agent lifetimes.},
author = {Niekum, Scott and Barto, Andrew G and Spector, Lee},
doi = {10.1109/TAMD.2010.2051436},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Niekum, Barto, Spector - 2010 - Genetic Programming for Reward Function Search.pdf:pdf},
issn = {1943-0604},
journal = {IEEE Transactions on Autonomous Mental Development},
number = {2},
pages = {83--90},
title = {{Genetic Programming for Reward Function Search}},
url = {http://ieeexplore.ieee.org/document/5473118/},
volume = {2},
year = {2010}
}
@article{Bigot2013,
abstract = {In this paper, we consider the problem of estimating nonparametrically a mean pattern intensity $\lambda$ from the observation of n independent and non-homogeneous Poisson processes N1,..., Nnn on the interval [0, 1]. This problem arises when data (counts) are collected independently from n individuals according to similar Poisson processes. We show that estimating this intensity is a deconvolution problem for which the density of the random shifts plays the role of the convolution operator. In an asymptotic setting where the number n of observed trajectories tends to infinity, we derive upper and lower bounds for the minimax quadratic risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used to derive an adaptive estimator of the intensity. The proposed estimator is shown to achieve a near-minimax rate of convergence. This rate depends both on the smoothness of the intensity function and the density of the random shifts, which makes a connection between the classical deconvolution problem in nonparametric statistics and the estimation of a mean intensity from the observations of independent Poisson processes.},
author = {Bigot, J{\'{e}}r{\'{e}}mie and Gadat, S{\'{e}}bastien and Klein, Thierry and Marteau, Cl{\'{e}}ment},
doi = {10.1214/13-EJS794},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bigot et al. - 2013 - Intensity estimation of non-homogeneous Poisson processes from shifted trajectories.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Adaptive estimation,Besov space,Deconvolution,Intensity estimation,Meyer wavelets,Minimax rate,Poisson processes,Random shifts},
number = {1},
pages = {881--931},
title = {{Intensity estimation of non-homogeneous Poisson processes from shifted trajectories}},
volume = {7},
year = {2013}
}
@article{Anderson1991,
author = {Anderson, John R. and Schooler, Lael J.},
doi = {10.1111/j.1467-9280.1991.tb00174.x},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Anderson, Schooler - 1991 - Reflections of the Environment in Memory.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
month = {nov},
number = {6},
pages = {396--408},
title = {{Reflections of the Environment in Memory}},
url = {http://journals.sagepub.com/doi/10.1111/j.1467-9280.1991.tb00174.x},
volume = {2},
year = {1991}
}
@article{Azizzadenesheli2016,
abstract = {We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.},
archivePrefix = {arXiv},
arxivId = {1602.07764},
author = {Azizzadenesheli, Kamyar and Lazaric, Alessandro and Anandkumar, Animashree},
eprint = {1602.07764},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Azizzadenesheli, Lazaric, Anandkumar - 2016 - Reinforcement Learning of POMDPs using Spectral Methods.pdf:pdf},
journal = {Conference on Learning Theory (COLT)},
keywords = {cess,latent variable model,method of moments,partially observable markov decision,pro-,spectral methods,upper confidence reinforcement learning},
pages = {1--64},
title = {{Reinforcement Learning of POMDPs using Spectral Methods}},
url = {http://arxiv.org/abs/1602.07764},
volume = {49},
year = {2016}
}
@article{Pu2017,
abstract = {A new method for learning variational autoencoders (VAEs) is developed, based on Stein variational gradient descent. A key advantage of this approach is that one need not make parametric assumptions about the form of the encoder distribution. Performance is further enhanced by integrating the proposed encoder with importance sampling. Excellent performance is demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.},
archivePrefix = {arXiv},
arxivId = {1704.05155},
author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Li, Chunyuan and Han, Shaobo and Carin, Lawrence},
doi = {10.1109/ICPR.2006.684},
eprint = {1704.05155},
isbn = {0769525210},
issn = {10495258},
title = {{VAE Learning via Stein Variational Gradient Descent}},
year = {2017}
}
@article{Atkinson2018a,
abstract = {We introduce a Bayesian Gaussian process latent variable model that explicitly captures spatial correlations in data using a parameterized spatial kernel and leveraging structure-exploiting algebra on the model covariance matrices for computational tractability. Inference is made tractable through a collapsed variational bound with similar computational complexity to that of the traditional Bayesian GP-LVM. Inference over partially-observed test cases is achieved by optimizing a "partially-collapsed" bound. Modeling high-dimensional time series systems is enabled through use of a dynamical GP latent variable prior. Examples imputing missing data on images and super-resolution imputation of missing video frames demonstrate the model.},
archivePrefix = {arXiv},
arxivId = {1805.08665},
author = {Atkinson, Steven and Zabaras, Nicholas},
doi = {10.1162/089976699300016331},
eprint = {1805.08665},
isbn = {978-1-4503-1285-1},
issn = {0899-7667},
pmid = {80990000001},
title = {{Structured Bayesian Gaussian process latent variable model}},
year = {2018}
}
@article{Dhir2017,
author = {Dhir, Neil},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Dhir - 2017 - Bayesian nonparametric methods for dynamics identification and segmentation for powered prosthesis control.pdf:pdf},
title = {{Bayesian nonparametric methods for dynamics identification and segmentation for powered prosthesis control}},
url = {http://www.robots.ox.ac.uk/{~}mobile/Theses/2018{\_}Dhir.pdf},
year = {2017}
}
@article{Pham2018,
abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89{\%}, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65{\%}.},
archivePrefix = {arXiv},
arxivId = {1802.03268},
author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
eprint = {1802.03268},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter Sharing.pdf:pdf},
title = {{Efficient Neural Architecture Search via Parameter Sharing}},
url = {http://arxiv.org/abs/1802.03268},
year = {2018}
}
@article{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
file = {:Users/mshvarts/Downloads/1601.00670.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
month = {apr},
number = {518},
pages = {859--877},
pmid = {303902},
title = {{Variational Inference: A Review for Statisticians}},
url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
volume = {112},
year = {2017}
}
@article{Bamler2017,
abstract = {Continuous latent time series models are prevalent in Bayesian modeling; examples include the Kalman filter, dynamic collaborative filtering, or dynamic topic models. These models often benefit from structured, non mean field variational approximations that capture correlations between time steps. Black box variational inference with reparameterization gradients (BBVI) allows us to explore a rich new class of Bayesian non-conjugate latent time series models; however, a naive application of BBVI to such structured variational models would scale quadratically in the number of time steps. We describe a BBVI algorithm analogous to the forward-backward algorithm which instead scales linearly in time. It allows us to efficiently sample from the variational distribution and estimate the gradients of the ELBO. Finally, we show results on the recently proposed dynamic word embedding model, which was trained using our method.},
archivePrefix = {arXiv},
arxivId = {1707.01069},
author = {Bamler, Robert and Mandt, Stephan},
doi = {10.3934/jcd.2014.1.391},
eprint = {1707.01069},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bamler, Mandt - 2017 - Structured Black Box Variational Inference for Latent Time Series Models(2).pdf:pdf},
isbn = {9781303457005},
issn = {2158-2491},
month = {jul},
number = {Dmd},
pages = {1--30},
pmid = {1458341928},
title = {{Structured Black Box Variational Inference for Latent Time Series Models}},
url = {http://arxiv.org/abs/1312.0041{\%}0Ahttp://dx.doi.org/10.3934/jcd.2014.1.391 http://arxiv.org/abs/1707.01069},
year = {2017}
}
@article{Boucher2007,
abstract = {The stop-signal task has been used to study normal cognitive control and clinical dysfunction. Its utility is derived from a race model that accounts for performance and provides an estimate of the time it takes to stop a movement. This model posits a race between go and stop processes with stochastically independent finish times. However, neurophysiological studies demonstrate that the neural correlates of the go and stop processes produce movements through a network of interacting neurons. The juxtaposition of the computational model with the neural data exposes a paradox-how can a network of interacting units produce behavior that appears to be the outcome of an independent race? The authors report how a simple, competitive network can solve this paradox and provide an account of what is measured by stop-signal reaction time.},
author = {Boucher, Leanne and Palmeri, Thomas J. and Logan, Gordon D. and Schall, Jeffrey D.},
doi = {10.1037/0033-295X.114.2.376},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Boucher et al. - 2007 - Inhibitory control in mind and brain An interactive race model of countermanding saccades.pdf:pdf},
isbn = {0033-295X (Print) 0033-295X (Linking)},
issn = {0033295X},
journal = {Psychological Review},
keywords = {Brain,Cognitive control,Cognitive modeling,Frontal eye field,Inhibitory control,Mind,Stochastic decision models,Stop signal reaction time,Stop-signal task},
number = {2},
pages = {376--397},
pmid = {17500631},
title = {{Inhibitory control in mind and brain: An interactive race model of countermanding saccades.}},
volume = {114},
year = {2007}
}
@article{Gorbach2017,
abstract = {Gradient matching is a promising tool for learning parameters and state dynamics of ordinary differential equations. It is a grid free inference approach, which, for fully observable systems is at times competitive with numerical integration. However, for many real-world applications, only sparse observations are available or even unobserved variables are included in the model description. In these cases most gradient matching methods are difficult to apply or simply do not provide satisfactory results. That is why, despite the high computational cost, numerical integration is still the gold standard in many applications. Using an existing gradient matching approach, we propose a scalable variational inference framework which can infer states and parameters simultaneously, offers computational speedups, improved accuracy and works well even under model misspecifications in a partially observable system.},
archivePrefix = {arXiv},
arxivId = {1705.07079},
author = {Gorbach, Nico S. and Bauer, Stefan and Buhmann, Joachim M.},
eprint = {1705.07079},
issn = {10495258},
title = {{Scalable Variational Inference for Dynamical Systems}},
year = {2017}
}
@article{Meeds2015a,
abstract = {Approximate Bayesian computation (ABC) is a powerful and elegant framework for performing inference in simulation-based models. However, due to the difficulty in scaling likelihood estimates, ABC remains useful for relatively low-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of likelihood-free algorithms that apply recent advances in scaling Bayesian learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find that a small number forward simulations can effectively approximate the ABC gradient, allowing Hamiltonian dynamics to efficiently traverse parameter spaces. We also describe a new simple yet general approach of incorporating random seeds into the state of the Markov chain, further reducing the random walk behavior of HABC. We demonstrate HABC on several typical ABC problems, and show that HABC samples comparably to regular Bayesian inference using true gradients on a high-dimensional problem from machine learning.},
archivePrefix = {arXiv},
arxivId = {1503.01916},
author = {Meeds, Edward and Leenders, Robert and Welling, Max},
eprint = {1503.01916},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Meeds, Leenders, Welling - 2015 - Hamiltonian ABC.pdf:pdf},
isbn = {9780000000002},
pages = {1--16},
title = {{Hamiltonian ABC}},
url = {http://arxiv.org/abs/1503.01916},
year = {2015}
}
@article{Schmon2019,
author = {Schmon, Sebastian M},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Schmon - 2019 - Implicit Priors for Knowledge Sharing in Bayesian Neural Networks.pdf:pdf},
number = {NeurIPS},
title = {{Implicit Priors for Knowledge Sharing in Bayesian Neural Networks}},
year = {2019}
}
@inproceedings{Takeishi2017,
abstract = {Dynamic mode decomposition (DMD) is a data-driven method for calculating a modal representa-tion of a nonlinear dynamical system, and it has been utilized in various fields of science and en-gineering. In this paper, we propose Bayesian DMD, which provides a principled way to trans-fer the advantages of the Bayesian formulation into DMD. To this end, we first develop a probabilistic model corresponding to DMD, and then, provide the Gibbs sampler for the posterior inference in Bayesian DMD. Moreover, as a specific example, we discuss the case of using a sparsity-promoting prior for an automatic determination of the number of dynamic modes. We investigate the empirical performance of Bayesian DMD using synthetic and real-world datasets.},
address = {California},
author = {Takeishi, Naoya and Kawahara, Yoshinobu and Tabei, Yasuo and Yairi, Takehisa},
booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/392},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Takeishi et al. - 2017 - Bayesian Dynamic Mode Decomposition.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
keywords = {Machine Learning: Time-series/Data Streams,Machine Learning: Unsupervised Learning,Uncertainty in AI: Uncertainty in AI},
month = {aug},
pages = {2814--2821},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
title = {{Bayesian Dynamic Mode Decomposition}},
url = {https://www.ijcai.org/proceedings/2017/0392.pdf https://www.ijcai.org/proceedings/2017/392},
year = {2017}
}
@article{Press2011,
author = {Press, William H},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Press - 2011 - Canonical Correlation Clarified by Singular Value Decomposition.pdf:pdf},
number = {2},
pages = {10},
title = {{Canonical Correlation Clarified by Singular Value Decomposition}},
url = {http://numerical.recipes/whp/workingpapers.html},
year = {2011}
}
@article{Alvarez,
author = {Alvarez, Mauricio and Lawrence, Neil D},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Alvarez, Lawrence - Unknown - Sparse Convolved Gaussian Processes for Multi-output Regression.pdf:pdf},
title = {{Sparse Convolved Gaussian Processes for Multi-output Regression}}
}
@techreport{Lopez-Paz,
abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1706.08840v5},
author = {Lopez-Paz, David and Ranzato, Marc ' Aurelio},
eprint = {1706.08840v5},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lopez-Paz, Ranzato - Unknown - Gradient Episodic Memory for Continual Learning.pdf:pdf},
title = {{Gradient Episodic Memory for Continual Learning}},
url = {https://github.com/}
}
@article{Wu2017,
abstract = {In many problem settings, parameter vectors are not merely sparse, but dependent in such a way that non-zero coefficients tend to cluster together. We refer to this form of dependency as "region sparsity". Classical sparse regression methods, such as the lasso and automatic relevance determination (ARD), which model parameters as independent a priori, and therefore do not exploit such dependencies. Here we introduce a hierarchical model for smooth, region-sparse weight vectors and tensors in a linear regression setting. Our approach represents a hierarchical extension of the relevance determination framework, where we add a transformed Gaussian process to model the dependencies between the prior variances of regression weights. We combine this with a structured model of the prior variances of Fourier coefficients, which eliminates unnecessary high frequencies. The resulting prior encourages weights to be region-sparse in two different bases simultaneously. We develop Laplace approximation and Monte Carlo Markov Chain (MCMC) sampling to provide efficient inference for the posterior. Furthermore, a two-stage convex relaxation of the Laplace approximation approach is also provided to relax the inevitable non-convexity during the optimization. We finally show substantial improvements over comparable methods for both simulated and real datasets from brain imaging.},
archivePrefix = {arXiv},
arxivId = {1711.10058},
author = {Wu, Anqi and Koyejo, Oluwasanmi and Pillow, Jonathan W.},
eprint = {1711.10058},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Koyejo, Pillow - 2017 - Dependent relevance determination for smooth and structured sparse regression.pdf:pdf},
issn = {10495258},
number = {Mcmc},
pages = {1--38},
title = {{Dependent relevance determination for smooth and structured sparse regression}},
url = {http://arxiv.org/abs/1711.10058},
year = {2017}
}
@article{Bui2016,
abstract = {Gaussian processes (GPs) are flexible distributions over functions that enable high-level assumptions about unknown functions to be encoded in a parsimonious, flexible and general way. Although elegant, the application of GPs is limited by computational and analytical intractabilities that arise when data are sufficiently numerous or when employing non-Gaussian models. Consequently, a wealth of GP approximation schemes have been developed over the last 15 years to address these key limitations. Many of these schemes employ a small set of pseudo data points to summarise the actual data. In this paper, we develop a new pseudo-point approximation framework using Power Expectation Propagation (Power EP) that unifies a large number of these pseudo-point approximations. Unlike much of the previous venerable work in this area, the new framework is built on standard methods for approximate inference (variational free-energy, EP and Power EP methods) rather than employing approximations to the probabilistic generative model itself. In this way, all of approximation is performed at `inference time' rather than at `modelling time' resolving awkward philosophical and empirical questions that trouble previous approaches. Crucially, we demonstrate that the new framework includes new pseudo-point approximation methods that outperform current approaches on regression and classification tasks.},
archivePrefix = {arXiv},
arxivId = {1605.07066},
author = {Bui, Thang D. and Yan, Josiah and Turner, Richard E.},
eprint = {1605.07066},
issn = {15324435},
title = {{A Unifying Framework for Gaussian Process Pseudo-Point Approximations using Power Expectation Propagation}},
year = {2016}
}
@article{Deisenroth2015,
abstract = {To scale Gaussian processes (GPs) to large data sets we introduce the robust Bayesian Committee Machine (rBCM), a practical and scalable product-of-experts model for large-scale distributed GP regression. Unlike state-of-the-art sparse GP approximations, the rBCM is conceptually simple and does not rely on inducing or variational parameters. The key idea is to recursively distribute computations to independent computational units and, subsequently, recombine them to form an overall result. Efficient closed-form inference allows for straightforward parallelisation and distributed computations with a small memory footprint. The rBCM is independent of the computational graph and can be used on heterogeneous computing infrastructures, ranging from laptops to clusters. With sufficient computing resources our distributed GP model can handle arbitrarily large data sets.},
archivePrefix = {arXiv},
arxivId = {1502.02843},
author = {Deisenroth, Marc Peter and Ng, Jun Wei},
doi = {10.1016/j.physa.2015.02.029},
eprint = {1502.02843},
isbn = {9781510810587},
title = {{Distributed Gaussian Processes}},
year = {2015}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
pmid = {24920543},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@phdthesis{Saatchi2011,
author = {Saatchi, Yunus},
booktitle = {Dissertation},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Saatchi - 2011 - Scalable Inference for Structured Gaussian Process Models.pdf:pdf},
title = {{Scalable Inference for Structured Gaussian Process Models}},
year = {2011}
}
@article{Cichocki2017,
abstract = {Part 2 of this monograph builds on the introduction to tensor networks and their operations presented in Part 1. It focuses on tensor network models for super-compressed higher-order representation of data/parameters and related cost functions, while providing an outline of their applications in machine learning and data analytics. A particular emphasis is on the tensor train (TT) and Hierarchical Tucker (HT) decompositions, and their physically meaningful interpretations which reflect the scalability of the tensor network approach. Through a graphical approach, we also elucidate how, by virtue of the underlying low-rank tensor approximations and sophisticated contractions of core tensors, tensor networks have the ability to perform distributed computations on otherwise prohibitively large volumes of data/parameters, thereby alleviating or even eliminating the curse of dimensionality. The usefulness of this concept is illustrated over a number of applied areas, including generalized regression and classification (support tensor machines, canonical correlation analysis, higher order partial least squares), generalized eigenvalue decomposition, Riemannian optimization, and in the optimization of deep neural networks. Part 1 and Part 2 of this work can be used either as stand-alone separate texts, or indeed as a conjoint comprehensive review of the exciting field of low-rank tensor networks and tensor decompositions.},
archivePrefix = {arXiv},
arxivId = {1708.09165},
author = {Cichocki, Andrzej and Lee, Namgil and Oseledets, Ivan and Phan, Anh-Huy and Zhao, Qibin and Sugiyama, Masashi and Mandic, Danilo P.},
doi = {10.1561/2200000067},
eprint = {1708.09165},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cichocki et al. - 2017 - Tensor Networks for Dimensionality Reduction and Large-scale Optimization Part 2 Applications and Future Perspe.pdf:pdf},
isbn = {9781680832761},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {6},
pages = {249--429},
title = {{Tensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 2 Applications and Future Perspectives}},
url = {http://arxiv.org/abs/1708.09165{\%}0Ahttp://dx.doi.org/10.1561/2200000067 http://www.nowpublishers.com/article/Details/MAL-067},
volume = {9},
year = {2017}
}
@article{Song2017a,
abstract = {In this work, we address multimodal learning problem with Gaussian process latent variable models (GPLVMs) and their application to cross-modal retrieval. Existing GPLVM based studies generally impose individual priors over the model parameters and ignore the intrinsic rela-tions among these parameters. Considering the strong com-plementarity between modalities, we propose a novel joint prior over the parameters for multimodal GPLVMs to prop-agate multimodal information in both kernel hyperparam-eter spaces and latent space. The joint prior is formulat-ed as a harmonization constraint on the model parameters, which enforces the agreement among the modality-specific GP kernels and the similarity in the latent space. We in-corporate the harmonization mechanism into the learning process of multimodal GPLVMs. The proposed method-s are evaluated on three widely used multimodal datasets for cross-modal retrieval. Experimental results show that the harmonization mechanism is beneficial to the GPLVM algorithms for learning non-linear correlation among het-erogeneous modalities.},
author = {Song, Guoli and Wang, Shuhui and Huang, Qingming and Tian, Qi},
doi = {10.1109/ICCV.2017.538},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Song et al. - 2017 - Multimodal Gaussian Process Latent Variable Models with Harmonization.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5039--5047},
title = {{Multimodal Gaussian Process Latent Variable Models with Harmonization}},
volume = {2017-Octob},
year = {2017}
}
@article{Barto2003,
abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
author = {Barto, Andrew G and Mahadevan, Sridhar},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Barto, Mahadevan - 2003 - Discrete Event Dynamic Systems Theory and Applications.pdf:pdf},
pages = {41--77},
title = {{Discrete Event Dynamic Systems: Theory and Applications}},
volume = {13},
year = {2003}
}
@article{Denkova2014,
abstract = {Episodic autobiographical memory (AM) allows one, through the recollection of sensory-perceptual details, thoughts and feelings, to become aware of an event as belonging to one's own past as well as being able to project into one's future. Because AM provides a sense of self-continuity, contributes to the integrity of the self, and helps predicting future experiences, any deficit of AM may have debilitating consequences for everyday life functioning. Understanding AM failure and the underlying neural mechanisms has the potential to shed light on brain reorganization mechanisms and engagement of compensatory processes. Functional magnetic resonance imaging (fMRI) provides the most promising imaging method to tackle these issues. We reviewed evidence from the few studies that used fMRI to investigate the functionality of the residual tissue, the neural reorganization and compensatory mechanisms in patients with neurological conditions due to impaired medial temporal lobe. Overall, these studies highlight the importance of the left hippocampus, which when atrophied and not functional leads to AM deficits but its residual functionality may support relatively normal AM recollection. When damaged hippocampal tissue is not functional, other brain regions (e.g., the medial prefrontal cortex) may be involved to compensate impairment, but they appear generally ineffective to support detailed episodic recollection.},
author = {Denkova, Ekaterina J},
doi = {10.4329/wjr.v6.i4.93},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Denkova - 2014 - FMRI contributions to addressing autobiographical memory impairment in temporal lobe pathology.pdf:pdf},
issn = {1949-8470},
journal = {World Journal of Radiology},
keywords = {amnesia,autobiographical memory,core tip,functional magnetic resonance imaging,in-,lobe,medial temporal,memory deficit,reorganization},
number = {4},
pages = {93},
title = {{FMRI contributions to addressing autobiographical memory impairment in temporal lobe pathology}},
volume = {6},
year = {2014}
}
@article{Berridge2000,
abstract = {The hedonic impact of taste is reflected in affective facial reactions made by human infants, other primates, and even rats. Originally studied in human infants, affective reactions to taste have also been used by affective neuroscience to identify hedonic brain systems in studies of animals (via application of neural stimulation, pharmacological activation, and neural lesion manipulations). The chief limitation of measuring affective reactions is that it is difficult for experimenters to know how to interpret them, and therefore how to interpret changes produced by brain manipulations. This paper notes guidelines to interpretation. It examines the phylogenetic continuity between humans, other primates, and rats in terms of the microstructure of taste-elicited affective reactions. It reviews evidence that affective taste reactivity patterns truly reflect a 'core hedonic process' of palatability or affect, rather than being an ingestion measure, consummatory behavior measure, or a sensory reflex measure. It reviews affective neuroscience studies of taste reactivity that have identified true hedonic brain substrates, and discriminated them from false hedonic brain substrates. It considers the neural bases of incentive 'wanting' versus 'liking'. Finally, it notes the difference between human subjective affective ratings of pleasure and 'core hedonic processes' reflected by behavioral affective reactions. (C) 2000 Elsevier Science Ltd.},
author = {Berridge, K},
doi = {10.1016/S0149-7634(99)00072-X},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Berridge - 2000 - Measuring hedonic impact in animals and infants microstructure of affective taste reactivity patterns.pdf:pdf},
isbn = {0149-7634},
issn = {01497634},
journal = {Neuroscience {\&} Biobehavioral Reviews},
keywords = {Accumbens,Affect,Amygdala,Ape,Aversion,Benzodiazepin e,Brain,Brainstem,Consciousness,Development,Distaste,Dopamine,Emotion,Facial expression,Feeding behavior,GABA,Gustation,Hedonic,Human,Hypothalamus,Ingestion,Intake,Monkey,Morphine,Neonate,Ontogeny,Opioid,Pallidum,Pleasure,Primate,Rat,Review,Reward,Sensation,Shell,Video analysis},
month = {mar},
number = {2},
pages = {173--198},
pmid = {10714382},
title = {{Measuring hedonic impact in animals and infants: microstructure of affective taste reactivity patterns}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S014976349900072X},
volume = {24},
year = {2000}
}
@article{Tripathy2016,
abstract = {Uncertainty quantification (UQ) tasks, such as model calibration, uncertainty propagation, and optimization under uncertainty, typically require several thousand evaluations of the underlying computer codes. To cope with the cost of simulations, one replaces the real response surface with a cheap surrogate based, e.g., on polynomial chaos expansions, neural networks, support vector machines, or Gaussian processes (GP). However, the number of simulations required to learn a generic multivariate response grows exponentially as the input dimension increases. This curse of dimensionality can only be addressed, if the response exhibits some special structure that can be discovered and exploited. A wide range of physical responses exhibit a special structure known as an active subspace (AS). An AS is a linear manifold of the stochastic space characterized by maximal response variation. The idea is that one should first identify this low dimensional manifold, project the high-dimensional input onto it, and then link the projection to the output. If the dimensionality of the AS is low enough, then learning the link function is a much easier problem than the original problem of learning a high-dimensional function. The classic approach to discovering the AS requires gradient information, a fact that severely limits its applicability. Furthermore, and partly because of its reliance to gradients, it is not able to handle noisy observations. The latter is an essential trait if one wants to be able to propagate uncertainty through stochastic simulators, e.g., through molecular dynamics codes. In this work, we develop a probabilistic version of AS which is gradient-free and robust to observational noise. Our approach relies on a novel Gaussian process regression with built-in dimensionality reduction. In particular, the AS is represented as an orthogonal projection matrix that serves as yet another covariance function hyper-parameter to be estimated from the data. To train the model, we design a two-step maximum likelihood optimization procedure that ensures the orthogonality of the projection matrix by exploiting recent results on the Stiefel manifold, i.e., the manifold of matrices with orthogonal columns. The additional benefit of our probabilistic formulation, is that it allows us to select the dimensionality of the AS via the Bayesian information criterion. We validate our approach by showing that it can discover the right AS in synthetic examples without gradient information using both noiseless and noisy observations. We demonstrate that our method is able to discover the same AS as the classical approach in a challenging one-hundred-dimensional problem involving an elliptic stochastic partial differential equation with random conductivity. Finally, we use our approach to study the effect of geometric and material uncertainties in the propagation of solitary waves in a one dimensional granular system.},
archivePrefix = {arXiv},
arxivId = {1602.04550},
author = {Tripathy, Rohit and Bilionis, Ilias and Gonzalez, Marcial},
doi = {10.1016/j.jcp.2016.05.039},
eprint = {1602.04550},
issn = {10902716},
journal = {Journal of Computational Physics},
title = {{Gaussian processes with built-in dimensionality reduction: Applications to high-dimensional uncertainty propagation}},
volume = {321},
year = {2016}
}
@article{Li2015,
abstract = {Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of {\$}N{\$}. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.},
archivePrefix = {arXiv},
arxivId = {1506.04132},
author = {Li, Yingzhen and Hernandez-Lobato, Jose Miguel and Turner, Richard E.},
doi = {10.1007/s11467-015-0514-9},
eprint = {1506.04132},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Li, Hernandez-Lobato, Turner - 2015 - Stochastic Expectation Propagation.pdf:pdf},
isbn = {1506.04132},
issn = {10495258},
title = {{Stochastic Expectation Propagation}},
year = {2015}
}
@techreport{Voss2007,
abstract = {In this paper, we describe a new algorithmic approach for parameter estimation in Rat-cliff's (1978) diffusion model. This problem, especially if inter-trial variabilities of parameters are included in the model, is computationally very expensive; the parameter estimation procedure often takes a long time even with today's high-speed computers. The algorithm described here makes the calculation of the cumulative distribution functions for predicted process durations computationally much less expensive. This improvement is achieved by solving the Kolmogorov backward equation numerically instead of employing the previously used closed form solution. Additionally, the algorithm can determine the optimum fit for one of the model parameters (the starting point z) directly, thereby reducing the dimension of the parameter search space by one. The resulting method is shown to be notably faster than the standard (closed-form solution) method for parameter estimation.},
author = {Voss, Andreas and Voss, Jochen},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Voss, Voss - 2007 - A Fast Numerical Algorithm for the Estimation of Diffusion Model Parameters.pdf:pdf},
title = {{A Fast Numerical Algorithm for the Estimation of Diffusion Model Parameters}},
url = {http://www.psychologie.uni-freiburg.de/Members/voss/fast-dm},
year = {2007}
}
@article{Howard2016,
abstract = {Autonomous neural systems must efficiently process information in a wide range of novel environments, which may have very different statistical properties. We consider the problem of how to optimally distribute receptors along a one-dimensional continuum consistent with the following design principles. First, neural representations of the world should obey a neural uncertainty principle---making as few assumptions as possible about the statistical structure of the world. Second, neural representations should convey, as much as possible, equivalent information about environments with different statistics. The results of these arguments resemble the structure of the visual system and provide a natural explanation of the behavioral Weber-Fechner law, a foundational result in psychology. Because the derivation is extremely general, this suggests that similar scaling relationships should be observed not only in sensory continua, but also in neural representations of ``cognitive' one-dimensional quantities such as time or numerosity.},
archivePrefix = {arXiv},
arxivId = {1607.04886},
author = {Howard, Marc W and Shankar, Karthik H},
eprint = {1607.04886},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Shankar - 2016 - Neural scaling laws for an uncertain world.pdf:pdf},
month = {jul},
pages = {1--22},
title = {{Neural scaling laws for an uncertain world}},
url = {http://arxiv.org/abs/1607.04886},
year = {2016}
}
@article{Duchowski2018,
abstract = {A novel eye-tracked measure of the frequency of pupil diame-ter oscillation is proposed for capturing what is thought to be an indicator of cognitive load. The proposed metric, termed the Index of Pupillary Activity, is shown to discriminate task difficulty vis-{\`{a}}-vis cognitive load (if the implied causality can be assumed) in an experiment where participants performed easy and difficult mental arithmetic tasks while fixating a cen-tral target (a requirement for replication of prior work). The paper's contribution is twofold: full documentation is provided for the calculation of the proposed measurement which can be considered as an alternative to the existing proprietary Index of Cognitive Activity (ICA). Thus, it is possible for researchers to replicate the experiment and build their own software which implements this measurement. Second, several aspects of the ICA are approached in a more data-sensitive way with the goal of improving the measurement's performance.},
author = {Duchowski, Andrew T. and Krejtz, Krzysztof and Krejtz, Izabela and Biele, Cezary and Niedzielska, Anna and Kiefer, Peter and Raubal, Martin and Giannopoulos, Ioannis},
doi = {10.1145/3173574.3173856},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Duchowski et al. - 2018 - The Index of Pupillary Activity.pdf:pdf},
isbn = {9781450356206},
journal = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems  - CHI '18},
keywords = {eye tracking,pupillometry,task difficulty},
pages = {1--13},
title = {{The Index of Pupillary Activity}},
url = {http://dl.acm.org/citation.cfm?doid=3173574.3173856},
year = {2018}
}
@article{Eleftheriadis2017a,
abstract = {The Gaussian process state space model (GPSSM) is a non-linear dynamical system, where unknown transition and/or measurement mappings are described by GPs. Most research in GPSSMs has focussed on the state estimation problem, i.e., computing a posterior of the latent state given the model. However, the key challenge in GPSSMs has not been satisfactorily addressed yet: system identification, i.e., learning the model. To address this challenge, we impose a structured Gaussian variational posterior distribution over the latent states, which is parameterised by a recognition model in the form of a bi-directional recurrent neural network. Inference with this structure allows us to recover a posterior smoothed over sequences of data. We provide a practical algorithm for efficiently computing a lower bound on the marginal likelihood using the reparameterisation trick. This further allows for the use of arbitrary kernels within the GPSSM. We demonstrate that the learnt GPSSM can efficiently generate plausible future trajectories of the identified system after only observing a small number of episodes from the true system.},
archivePrefix = {arXiv},
arxivId = {1705.10888},
author = {Eleftheriadis, Stefanos and Nicholson, Thomas F. W. and Deisenroth, Marc Peter and Hensman, James},
doi = {10.1109/DATE.2010.5457147},
eprint = {1705.10888},
isbn = {9783981080162},
issn = {10495258},
title = {{Identification of Gaussian Process State Space Models}},
year = {2017}
}
@article{VandeMeent2018,
abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
archivePrefix = {arXiv},
arxivId = {1809.10756},
author = {van de Meent, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
eprint = {1809.10756},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/van de Meent et al. - 2018 - An Introduction to Probabilistic Programming.pdf:pdf},
title = {{An Introduction to Probabilistic Programming}},
url = {http://arxiv.org/abs/1809.10756},
year = {2018}
}
@article{Chung2018,
abstract = {We present a personalized and reliable prediction model for healthcare, which can provide individually tailored medical services such as diagnosis, disease treatment and prevention. Our proposed framework targets to making reliable predictions from time-series data, such as Electronic Health Records (EHR), by modeling two complementary components: i) shared component that captures global trend across diverse patients and ii) patient-specific component that models idiosyncratic variability for each patient. To this end, we propose a composite model of a deep recurrent neural network (RNN) to exploit expressive power of the RNN in estimating global trends from large number of patients, and Gaussian Processes (GP) to probabilistically model individual time-series given relatively small number of time points. We evaluate the strength of our model on diverse and heterogeneous tasks in EHR datasets. The results show that our model significantly outperforms baselines such as RNN, demonstrating clear advantage over existing models when working with noisy medical data.},
archivePrefix = {arXiv},
arxivId = {1806.01551},
author = {Chung, Ingyo and Kim, Saehoon and Lee, Juho and Hwang, Sung Ju and Yang, Eunho},
eprint = {1806.01551},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2018 - Mixed Effect Composite RNN-GP A Personalized and Reliable Prediction Model for Healthcare.pdf:pdf},
pages = {1--13},
title = {{Mixed Effect Composite RNN-GP: A Personalized and Reliable Prediction Model for Healthcare}},
url = {http://arxiv.org/abs/1806.01551},
year = {2018}
}
@article{Guio2011,
abstract = {The Levi-Civita symbol ijk is a tensor of rank three and is defined by ijk =    0, if any two labels are the same 1, if i, j, k is an even permutation of 1,2,3 −1, if i, j, k is an odd permutation of 1,2,3 (1) The Levi-Civita symbol ijk is anti-symmetric on each pair of indexes. The determinant of a matrix A with elements a ij can be written in term of ijk as det a 11 a 12 a 13 a 21 a 22 a 23 a 31 a 32 a 33 = 3 i=1 3 j=1 3 k=1 ijk a 1i a 2j a 3k = ijk a 1i a 2j a 3k (2) Note the compact notation where the summation over the spatial directions is dropped. It is this one that is in use. (4) orforeachcoordinate(a×b)i=ijkajbk(5)12Properties • TheLevi-Civitatensorijkhas3×3×3=27components. • 3×(6+1)=21componentsareequalto0. • 3componentsareequalto1. • 3componentsareequalto−1.3IdentitiesTheproductoftwoLevi-CivitasymbolscanbeexpressedasafunctionoftheKronecker'ssym-bol$\delta$ijijklmn=+$\delta$il$\delta$jm$\delta$kn+$\delta$im$\delta$jn$\delta$kl+$\delta$in$\delta$jl$\delta$km−$\delta$im$\delta$jl$\delta$kn−$\delta$il$\delta$jn$\delta$km−$\delta$in$\delta$jm$\delta$kl(6)Settingi=lgivesijkimn=$\delta$jm$\delta$kn−$\delta$jn$\delta$km(7)proofijkimn=$\delta$ii($\delta$jm$\delta$kn−$\delta$jn$\delta$km)+$\delta$im$\delta$jn$\delta$ki+$\delta$in$\delta$ji$\delta$km−$\delta$im$\delta$ji$\delta$kn−$\delta$in$\delta$jm$\delta$ki=3($\delta$jm$\delta$kn−$\delta$jn$\delta$km)+$\delta$km$\delta$jn+$\delta$jn$\delta$km−$\delta$jm$\delta$kn−$\delta$kn$\delta$jm=$\delta$jm$\delta$kn−$\delta$jn$\delta$kmSettingi=landj=mgivesijkijn=2$\delta$kn(8)},
author = {Guio, Patrick},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Guio - 2011 - Levi-Civita symbol and cross product vector tensor.pdf:pdf},
isbn = {0810820404},
journal = {Lecture note},
number = {4},
pages = {1--4},
title = {{Levi-Civita symbol and cross product vector / tensor}},
year = {2011}
}
@article{Brown2008,
abstract = {We propose a linear ballistic accumulator (LBA) model of decision making and reaction time. The LBA is simpler than other models of choice response time, with independent accumulators that race towards a common response threshold. Activity in the accumulators increases in a linear and deterministic manner. The simplicity of the model allows complete analytic solutions for choices between any number of alternatives. These solutions (and freely-available computer code) make the model easy to apply to both binary and multiple choice situations. Using data from five previously published experiments, we demonstrate that the LBA model successfully accommodates empirical phenomena from binary and multiple choice tasks that have proven difficult for other theoretical accounts. Our results are encouraging in a field beset by the tradeoff between complexity and completeness. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Brown, Scott D. and Heathcote, Andrew},
doi = {10.1016/j.cogpsych.2007.12.002},
eprint = {arXiv:1011.1669v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Heathcote - 2008 - The simplest complete model of choice response time Linear ballistic accumulation.pdf:pdf},
isbn = {1095-5623 (Electronic)$\backslash$n0010-0285 (Linking)},
issn = {00100285},
journal = {Cognitive Psychology},
keywords = {Choice,Decision,Lexical decision,Mathematical models,Reaction time,Response time},
number = {3},
pages = {153--178},
pmid = {18243170},
title = {{The simplest complete model of choice response time: Linear ballistic accumulation}},
volume = {57},
year = {2008}
}
@article{Katzfuss2017,
abstract = {Gaussian processes (GPs) are commonly used as models for functions, time series, and spatial fields, but they are computationally infeasible for large datasets. Focusing on the typical setting of modeling spatial data as a GP plus an additive noise term, we propose a generalization of the Vecchia (1988) approach as a framework for GP approximations. We show that our general Vecchia approach contains many popular existing GP approximations as special cases, allowing for comparisons among the different methods within a unified framework. Representing the models by directed acyclic graphs, we determine the sparsity of the matrices necessary for inference, which leads to new insights regarding the computational properties. Based on these results, we propose a novel sparse general Vecchia approximation, which ensures computational feasibility for large spatial datasets but can lead to tremendous improvements in approximation accuracy over Vecchia's original approach. We provide several theoretical results and conduct numerical comparisons. We conclude with guidelines for the use of Vecchia approximations in spatial statistics.},
archivePrefix = {arXiv},
arxivId = {1708.06302},
author = {Katzfuss, Matthias and Guinness, Joseph},
doi = {10.1021/jp805708w},
eprint = {1708.06302},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Katzfuss, Guinness - 2017 - A general framework for Vecchia approximations of Gaussian processes.pdf:pdf},
issn = {0009-9147},
keywords = {computational complexity,covariance approximation,datasets,directed acyclic graphs,large,sparsity,spatial statistics},
pages = {1--22},
pmid = {10545075},
title = {{A general framework for Vecchia approximations of Gaussian processes}},
url = {http://arxiv.org/abs/1708.06302},
year = {2017}
}
@article{Bigdely-Shamlo2015,
abstract = {The technology to collect brain imaging and physiological measures has become portable and ubiquitous, opening the possibility of large-scale analysis of real-world human imaging. By its nature, such data is large and complex, making automated processing essential. This paper shows how lack of attention to the very early stages of an EEG preprocessing pipeline can reduce the signal-to-noise ratio and introduce unwanted artifacts into the data, particularly for computations done in single precision. We demonstrate that ordinary average referencing improves the signal-to-noise ratio, but that noisy channels can contaminate the results. We also show that identification of noisy channels depends on the reference and examine the complex interaction of filtering, noisy channel identification, and referencing. We introduce a multi-stage robust referencing scheme to deal with the noisy channel-reference interaction. We propose a standardized early-stage EEG processing pipeline (PREP) and discuss the application of the pipeline to more than 600 EEG datasets. The pipeline includes an automatically generated report for each dataset processed. Users can download the PREP pipeline as a freely available MATLAB library from http://eegstudy.org/prepcode.},
author = {Bigdely-Shamlo, Nima and Mullen, Tim and Kothe, Christian and Su, Kyung-Min and Robbins, Kay A.},
doi = {10.3389/fninf.2015.00016},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bigdely-Shamlo et al. - 2015 - The PREP pipeline standardized preprocessing for large-scale EEG analysis.pdf:pdf},
isbn = {1662-5196},
issn = {1662-5196},
journal = {Frontiers in Neuroinformatics},
keywords = {EEG, artifact, preprocessing, EEGLAB, BCILAB, mach,artifact,bcilab,big data,eeg,eeglab,machine learning,preprocessing},
number = {June},
pages = {1--20},
pmid = {26150785},
title = {{The PREP pipeline: standardized preprocessing for large-scale EEG analysis}},
url = {http://journal.frontiersin.org/Article/10.3389/fninf.2015.00016/abstract},
volume = {9},
year = {2015}
}
@inproceedings{WeiWu2009,
author = {{Wei Wu} and {Zhe Chen} and {Shangkai Gao} and Brown, E.N.},
booktitle = {2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society},
doi = {10.1109/IEMBS.2009.5332646},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wei Wu et al. - 2009 - A probabilistic framework for learning robust common spatial patterns.pdf:pdf},
isbn = {9781424432967},
keywords = {Biomedical signal classification,Multivariate signal processing,Pattern recognition methods for data mining},
month = {sep},
pages = {4658--4661},
publisher = {IEEE},
title = {{A probabilistic framework for learning robust common spatial patterns}},
url = {http://ieeexplore.ieee.org/document/5332646/},
year = {2009}
}
@article{Gardner2018,
abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from {\$}O(n{\^{}}3){\$} to {\$}O(n{\^{}}2){\$}. Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
archivePrefix = {arXiv},
arxivId = {1809.11165},
author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
eprint = {1809.11165},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Gardner et al. - 2018 - GPyTorch Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration.pdf:pdf},
number = {NeurIPS},
title = {{GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration}},
url = {http://arxiv.org/abs/1809.11165},
year = {2018}
}
@article{Haxby2011,
abstract = {We present a high-dimensional model of the representational space in human ventral temporal (VT) cortex in which dimensions are response-tuning functions that are common across individuals and patterns of response are modeled as weighted sums of basis patterns associated with these response tunings. We map response-pattern vectors, measured with fMRI, from individual subjects' voxel spaces into this common model space using a new method, " hyperalignment." Hyperalignment parameters based on responses during one experiment-movie viewing-identified 35 common response-tuning functions that captured fine-grained distinctions among a wide range of stimuli in the movie and in two category perception experiments. Between-subject classification (BSC, multivariate pattern classification based on other subjects' data) of response-pattern vectors in common model space greatly exceeded BSC of anatomically aligned responses and matched within-subject classification. Results indicate that population codes for complex visual stimuli in VT cortex are based on response-tuning functions that are common across individuals. {\textcopyright} 2011 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Haxby, James V. and Guntupalli, J. Swaroop and Connolly, Andrew C. and Halchenko, Yaroslav O. and Conroy, Bryan R. and Gobbini, M. Ida and Hanke, Michael and Ramadge, Peter J.},
doi = {10.1016/j.neuron.2011.08.026},
eprint = {NIHMS150003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Haxby et al. - 2011 - A common, high-dimensional model of the representational space in human ventral temporal cortex.pdf:pdf},
isbn = {1097-4199},
issn = {08966273},
journal = {Neuron},
number = {2},
pages = {404--416},
pmid = {22017997},
title = {{A common, high-dimensional model of the representational space in human ventral temporal cortex}},
volume = {72},
year = {2011}
}
@article{Shvartsman2017,
abstract = {Multivariate analysis of fMRI data has benefited substantially from advances in machine learning. Most recently, a range of probabilistic latent variable models applied to fMRI data have been successful in a variety of tasks, including identifying similarity patterns in neural data (Representational Similarity Analysis and its empirical Bayes variant, RSA and BRSA; Intersubject Functional Connectivity, ISFC), combining multi-subject datasets (Shared Response Mapping; SRM), and mapping between brain and behavior (Joint Modeling). Although these methods share some underpinnings, they have been developed as distinct methods, with distinct algorithms and software tools. We show how the matrix-variate normal (MN) formalism can unify some of these methods into a single framework. In doing so, we gain the ability to reuse noise modeling assumptions, algorithms, and code across models. Our primary theoretical contribution shows how some of these methods can be written as instantiations of the same model, allowing us to generalize them to flexibly modeling structured noise covariances. Our formalism permits novel model variants and improved estimation strategies: in contrast to SRM, the number of parameters for MN-SRM does not scale with the number of voxels or subjects; in contrast to BRSA, the number of parameters for MN-RSA scales additively rather than multiplicatively in the number of voxels. We empirically demonstrate advantages of two new methods derived in the formalism: for MN-RSA, we show up to 10x improvement in runtime, up to 6x improvement in RMSE, and more conservative behavior under the null. For MN-SRM, our method grants a modest improvement to out-of-sample reconstruction while relaxing an orthonormality constraint of SRM. We also provide a software prototyping tool for MN models that can flexibly reuse noise covariance assumptions and algorithms across models.},
archivePrefix = {arXiv},
arxivId = {1711.03058},
author = {Shvartsman, Michael and Sundaram, Narayanan and Aoi, Mikio C. and Charles, Adam and Wilke, Theodore C. and Cohen, Jonathan D.},
eprint = {1711.03058},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shvartsman et al. - 2017 - Matrix-normal models for fMRI analysis.pdf:pdf},
title = {{Matrix-normal models for fMRI analysis}},
url = {http://arxiv.org/abs/1711.03058},
year = {2017}
}
@article{Bonilla2007,
abstract = {In this paper we investigate multi-task learning in the context of Gaussian Processes (GP). We propose a model that learns a shared covariance function on input-dependent features and a "free-form" covariance matrix over tasks. This allows for good flexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the assumption of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task transfer occurs. We evaluate the benefits of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in order to provide scalability to large data sets.},
author = {Bonilla, Edwin and Chai, Kian Ming and Williams, Christopher},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bonilla, Chai, Williams - 2007 - Multi-task Gaussian Process Prediction.pdf:pdf},
isbn = {160560352X},
journal = {Advances in neural information processing systems},
keywords = {Learning/Statistics {\&} Optimisation},
pages = {153--160},
title = {{Multi-task Gaussian Process Prediction}},
url = {http://eprints.pascal-network.org/archive/00003442/},
year = {2007}
}
@article{Barr2014,
author = {Barr, Dale and Levy, Roger and Scheepers, Christoph and Tily, Harry J},
doi = {10.1016/j.jml.2012.11.001.Random},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Barr et al. - 2014 - Keep it maximal.pdf:pdf},
journal = {Journal of Memory and Language},
keywords = {alternative,collection,generalization,i see no real,in most confirmatory studies,in which a question,is specified by all,linear mixed-effects models,main,monte carlo simulation,of design,question,statistics,to having a single},
number = {3},
pages = {1--43},
title = {{Keep it maximal}},
volume = {68},
year = {2014}
}
@article{Titsias2014,
abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimi-sation that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic ap-proximation and allows for efficient use of gra-dient information from the model joint density. We demonstrate these properties using illustra-tive examples as well as in challenging and di-verse Bayesian inference problems such as vari-able selection in logistic regression and fully Bayesian inference over kernel hyperparameters in Gaussian process regression.},
author = {Titsias, Michalis and L{\'{a}}zaro-Gredilla, Miguel},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Titsias, L{\'{a}}zaro-Gredilla - 2014 - Doubly Stochastic Variational Bayes for non-Conjugate Inference.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st International Conference on Machine Learning},
pages = {1971--1979},
title = {{Doubly Stochastic Variational Bayes for non-Conjugate Inference}},
url = {http://jmlr.org/proceedings/papers/v32/titsias14},
volume = {32},
year = {2014}
}
@article{Tubridy2018,
abstract = {One goal of cognitive science is to build theories of mental function that predict individual behavior. In this project we focus on predicting, for individual participants, which specific items in a list will be remembered at some point in the future. If you want to know if an individual will remember something, one commonsense approach is to give them a quiz or test such that a correct answer likely indicates later memory for an item. In this project we attempt to predict later memory without ex-plicit assessments by jointly modeling both neural and behav-ioral data in a computational cognitive model which captures the dynamics of memory acquisition and decay. In this paper, we lay out a novel hierarchical Bayesian approach for com-bining neural and behavioral data and present results showing how fMRI signals recorded during the study phase of a mem-ory task can improve our ability to predict (in held-out data) which items will be remembered or forgotten 72 hours later.},
author = {Tubridy, Shannon M and Halpern, David and Davachi, Lila and Gureckis, Todd M},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tubridy et al. - 2018 - A neurocognitive model for predicting the fate of individual memories.pdf:pdf},
keywords = {1991,cognitive neuroscience,dulosky,fmri signals recorded from,individu-,ing,joint modeling,jols,learn-,memory,nelson,own self-reported judgements of,that the model utilizing,to foreshadow,viduals such as their,we find},
title = {{A neurocognitive model for predicting the fate of individual memories}},
year = {2018}
}
@article{Cohen2016,
abstract = {Convolutional rectifier networks, i.e. convolutional neural networks with rectified linear activation and max or average pooling, are the cornerstone of modern deep learning. However, despite their wide use and success, our theoretical understanding of the expressive properties that drive these networks is partial at best. On the other hand, we have a much firmer grasp of these issues in the world of arithmetic circuits. Specifically, it is known that convolutional arithmetic circuits possess the property of "complete depth efficiency", meaning that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be implemented (or even approximated) by a shallow network. In this paper we describe a construction based on generalized tensor decompositions, that transforms convolutional arithmetic circuits into convolutional rectifier networks. We then use mathematical tools available from the world of arithmetic circuits to prove new results. First, we show that convolutional rectifier networks are universal with max pooling but not with average pooling. Second, and more importantly, we show that depth efficiency is weaker with convolutional rectifier networks than it is with convolutional arithmetic circuits. This leads us to believe that developing effective methods for training convolutional arithmetic circuits, thereby fulfilling their expressive potential, may give rise to a deep learning architecture that is provably superior to convolutional rectifier networks but has so far been overlooked by practitioners.},
archivePrefix = {arXiv},
arxivId = {1603.00162},
author = {Cohen, Nadav and Shashua, Amnon},
doi = {10.1109/WISM.2010.164},
eprint = {1603.00162},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cohen, Shashua - 2016 - Convolutional Rectifier Networks as Generalized Tensor Decompositions.pdf:pdf},
isbn = {9781510829008},
issn = {10495258},
month = {mar},
pmid = {1726903},
title = {{Convolutional Rectifier Networks as Generalized Tensor Decompositions}},
url = {http://arxiv.org/abs/1603.00162},
year = {2016}
}
@article{Louizos2017a,
abstract = {Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1705.08665},
author = {Louizos, Christos and Ullrich, Karen and Welling, Max},
eprint = {1705.08665},
isbn = {0272-989X},
issn = {10495258},
title = {{Bayesian Compression for Deep Learning}},
year = {2017}
}
@article{Oord2017,
abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
archivePrefix = {arXiv},
arxivId = {1711.00937},
author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
doi = {10.1186/1741-7007-8-86.Copyright},
eprint = {1711.00937},
isbn = {0973-2063 (Electronic)},
issn = {10495258},
pmid = {19255635},
title = {{Neural Discrete Representation Learning}},
year = {2017}
}
@article{Krause2008,
abstract = {When monitoring spatial phenomena, such as the ecological condition of a river, deciding where to make observations is a challenging task. In these settings, a fundamental question is when an active learning, or sequential design, strategy, where locations are selected based on previous measurements, will perform significantly better than sensing at an a priori specified set of locations. For Gaussian Processes (GPs), which often accurately model spatial phenomena, we present an analysis and efficient algorithms that address this question. Central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategies. We consider GPs with unknown kernel parameters and present a nonmyopic approach for trading off exploration, i.e., decreasing uncertainty about the model parameters, and exploitation, i.e., near-optimally selecting observations when the parameters are (approximately) known. We discuss several exploration strategies, and present logarithmic sample complexity bounds for the exploration phase. We then extend our algorithm to handle nonstationary GPs exploiting local structure in the model. We also present extensive empirical evaluation on several real-world problems.},
author = {Krause, Andreas and Guestrin, Carlos},
doi = {10.1145/1273496.1273553},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Krause, Guestrin - 2008 - Nonmyopic active learning of Gaussian processes.pdf:pdf},
pages = {449--456},
title = {{Nonmyopic active learning of Gaussian processes}},
year = {2008}
}
@article{Ge2017,
abstract = {Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that "all local optima are (approximately) global optima", and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult. In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised learning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant {\$}\backslashepsilon {\textgreater} 0{\$}, among the set of points with function values {\$}(1+\backslashepsilon){\$}-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem. Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local minima of a highly-structured random polynomial with dependent coefficients.},
archivePrefix = {arXiv},
arxivId = {1706.05598},
author = {Ge, Rong and Ma, Tengyu},
eprint = {1706.05598},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ge, Ma - 2017 - On the Optimization Landscape of Tensor Decompositions.pdf:pdf},
issn = {10495258},
title = {{On the Optimization Landscape of Tensor Decompositions}},
url = {http://arxiv.org/abs/1706.05598},
year = {2017}
}
@article{Pion-Tonachini2019,
abstract = {The electroencephalogram (EEG) provides a non-invasive, minimally restrictive, and relatively low cost measure of mesoscale brain dynamics with high temporal resolution. Although signals recorded in parallel by multiple, near-adjacent EEG scalp electrode channels are highly-correlated and combine signals from many different sources, biological and non-biological, independent component analysis (ICA) has been shown to isolate the various source generator processes underlying those recordings. Independent components (IC) found by ICA decomposition can be manually inspected, selected, and interpreted, but doing so requires both time and practice as ICs have no particular order or intrinsic interpretations and therefore require further study of their properties. Alternatively, sufficiently-accurate automated IC classifiers can be used to classify ICs into broad source categories, speeding the analysis of EEG studies with many subjects and enabling the use of ICA decomposition in near-real-time applications. While many such classifiers have been proposed recently, this work presents the ICLabel project comprised of (1) an IC dataset containing spatiotemporal measures for over 200,000 ICs from more than 6,000 EEG recordings, (2) a website for collecting crowdsourced IC labels and educating EEG researchers and practitioners about IC interpretation, and (3) the automated ICLabel classifier. The classifier improves upon existing methods in two ways: by improving the accuracy of the computed label estimates and by enhancing its computational efficiency. The ICLabel classifier outperforms or performs comparably to the previous best publicly available method for all measured IC categories while computing those labels ten times faster than that classifier as shown in a rigorous comparison against all other publicly available EEG IC classifiers.},
archivePrefix = {arXiv},
arxivId = {1901.07915},
author = {Pion-Tonachini, Luca and Kreutz-Delgado, Ken and Makeig, Scott},
eprint = {1901.07915},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Pion-Tonachini, Kreutz-Delgado, Makeig - 2019 - ICLabel An automated electroencephalographic independent component classifier, dataset,.pdf:pdf},
pages = {1--41},
title = {{ICLabel: An automated electroencephalographic independent component classifier, dataset, and website}},
url = {http://arxiv.org/abs/1901.07915},
year = {2019}
}
@article{Barachant2013,
abstract = {The use of spatial covariance matrix as a feature is investigated for motor imagery EEG-based classification in brain-computer interface applications. A new kernel is derived by establishing a connection with the Riemannian geometry of symmetric positive definite matrices. Different kernels are tested, in combination with support vector machines, on a past BCI competition dataset. We demonstrate that this new approach outperforms significantly state of the art results, effectively replacing the traditional spatial filtering approach. {\textcopyright} 2013 Elsevier B.V.},
author = {Barachant, Alexandre and Bonnet, St{\'{e}}phane and Congedo, Marco and Jutten, Christian},
doi = {10.1016/j.neucom.2012.12.039},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Barachant et al. - 2013 - Classification of covariance matrices using a Riemannian-based kernel for BCI applications.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Brain-computer interfaces,Covariance matrix,Kernel,Riemannian geometry,Support vector machine},
pages = {172--178},
title = {{Classification of covariance matrices using a Riemannian-based kernel for BCI applications}},
volume = {112},
year = {2013}
}
@article{Martens2010,
abstract = {There is a strong tendency towards discriminative approaches in brain-computer interface (BCI) research. We argue that generative model-based approaches are worth pursuing and propose a simple generative model for the visual ERP-based BCI speller which incorporates prior knowledge about the brain signals. We show that the proposed generative method needs less training data to reach a given letter prediction performance than the state of the art discriminative approaches.},
author = {Martens, S. M.M. and Leiva, J. M.},
doi = {10.1088/1741-2560/7/2/026003},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Martens, Leiva - 2010 - A generative model approach for decoding in the visual event-related potential-based brain-computer interface sp.pdf:pdf},
issn = {17412560},
journal = {Journal of Neural Engineering},
number = {2},
title = {{A generative model approach for decoding in the visual event-related potential-based brain-computer interface speller}},
volume = {7},
year = {2010}
}
@article{Ellis2019,
abstract = {Background: With advances in methods for collecting and analyzing fMRI data, there is a concurrent need to understand how to reliably evaluate and optimally use these methods. Simulations of fMRI data can aid in both the evaluation of complex designs and the analysis of data. New Method: We present fmrisim, a new Python package for standardized, realistic simulation of fMRI data. This package is part of BrainIAK: a recently released open-source Python toolbox for advanced neuroimaging analyses. We describe how to use fmrisim to extract noise properties from real fMRI data and then create a synthetic dataset with matched noise properties and a user-specified signal. Results: We validate the noise generated by fmrisim to show that it can approximate the noise properties of real data. We further show how fmrisim can help researchers find the optimal design in terms of power. Comparison with other methods: fmrisim ports the functionality of other packages to the Python platform while extending what is available in order to make it seamless to simulate realistic fMRI data. Conclusions: The fmrisim package holds promise for improving the design of fMRI experiments, which may facilitate both the pre-registration},
author = {Ellis, Cameron and Baldassano, Christopher and Schapiro, Anna C and Cai, Ming Bo and Cohen, Jonathan D},
doi = {10.1101/532424},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Ellis et al. - 2019 - Facilitating open-science with realistic fMRI simulation validation and application.pdf:pdf},
journal = {bioRxiv},
keywords = {fMRI,multivariate design,reproducibility,simulation},
pages = {532424},
title = {{Facilitating open-science with realistic fMRI simulation: validation and application}},
url = {http://dx.doi.org/10.1101/532424{\%}0Ahttps://www.biorxiv.org/content/10.1101/532424v2},
year = {2019}
}
@article{Mescheder2017,
abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
archivePrefix = {arXiv},
arxivId = {1701.04722},
author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
doi = {10.1016/j.aqpro.2013.07.003},
eprint = {1701.04722},
isbn = {9781510855144},
issn = {1938-7228},
pmid = {24439530},
title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
year = {2017}
}
@article{Liu2016,
abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
archivePrefix = {arXiv},
arxivId = {1608.04471},
author = {Liu, Qiang and Wang, Dilin},
eprint = {1608.04471},
issn = {10495258},
title = {{Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm}},
year = {2016}
}
@article{Jing2019,
abstract = {Time-series prediction has become a prominent challenge, especially when the data are described as sequences of multiway arrays. Because noise and redundancy may exist in the tensor representation of a time series, we focus on solving the problem of high-order time-series prediction under a tensor decomposition framework and develop two novel multilinear models: 1) the multilinear orthogonal autoregressive (MOAR) model and 2) the multilinear constrained autoregressive (MCAR) model. The MOAR model is designed to preserve as much information as possible from the original tensorial data under orthogonal constraints. The MCAR model is an enhanced version that is developed by replacing orthogonal constraints with an inverse decomposition error term. For both models, we project the original tensor into subspaces spanned by basis matrices to facilitate the discovery of the intrinsic temporal structure embedded in the original tensor. To build connections among consecutive slices of the tensor, we generalize a traditional autoregressive model to tensor form to better preserve the temporal smoothness. Experiments conducted on four publicly available datasets demonstrate that our proposed methods converge within a small number of iterations during the training stage and achieve promising results compared with state-of-the-art methods.},
author = {Jing, Peiguang and Su, Yuting and Jin, Xiao and Zhang, Chengqian},
doi = {10.1109/TCYB.2018.2832085},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Jing et al. - 2019 - High-Order Temporal Correlation Model Learning for Time-Series Prediction.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Autoregressive (AR),temporal correlation,tensor decomposition,time-series prediction},
number = {6},
pages = {2385--2397},
publisher = {IEEE},
title = {{High-Order Temporal Correlation Model Learning for Time-Series Prediction}},
volume = {49},
year = {2019}
}
@article{Bishara2010,
author = {Bishara, Anthony J and Kruschke, John K and Stout, Julie C and Bechara, Antoine and McCabe, David P. and Busemeyer, Jerome R},
doi = {10.1016/j.jmp.2008.10.002},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bishara et al. - 2010 - Sequential learning models for the Wisconsin card sort task Assessing processes in substance dependent individua.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {cognitive model,decision-making,executive,substance dependence,wisconsin card sort},
month = {feb},
number = {1},
pages = {5--13},
title = {{Sequential learning models for the Wisconsin card sort task: Assessing processes in substance dependent individuals}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022249608001004},
volume = {54},
year = {2010}
}
@article{Dzhafarov2011,
abstract = {From the principle that subjective dissimilarity between 2 stimuli is determined by their ratio, Fechner derives his logarithmic law in 2 ways. In one derivation, ignored and forgotten in modern accounts of Fechner's theory, he formulates the principle in question as a functional equation and reduces it to one with a known solution. In the other derivation, well known and often criticized, he solves the same functional equation by differentiation. Both derivations are mathematically valid (the much-derided " expedient principle" mentioned by Fechner can be viewed as merely an inept way of pointing at a certain property of the differentiation he uses). Neither derivation uses the notion of just-noticeable differences. But if Weber's law is accepted in addition to the principle in question, then the dissimilarity between 2 stimuli is approximately proportional to the number of just-noticeable differences that fit between these stimuli: The smaller Weber's fraction the better the approximation, and Weber's fraction can always be made arbitrarily small by an appropriate convention. We argue, however, that neither the 2 derivations of Fechner's law nor the relation of this law to thresholds constitutes the essence of Fechner's approach. We see this essence in the idea of additive cumulation of sensitivity values. Fechner's work contains a surprisingly modern definition of sensitivity at a given stimulus: the rate of growth of the probability-of-greater function, with this stimulus serving as a standard. The idea of additive cumulation of sensitivity values lends itself to sweeping generalizations of Fechnerian scaling. {\textcopyright} 2011 by the Board of Trustees of the University of Illinois.},
author = {Dzhafarov, Ehtibar N. and Colonius, Hans},
doi = {10.5406/amerjpsyc.124.2.0127},
file = {:Users/mshvarts/Downloads/Dzhafarov{\_}Colonius{\_}FechnerianIdea{\_}AJP.pdf:pdf},
issn = {00029556},
journal = {American Journal of Psychology},
number = {2},
pages = {127--140},
pmid = {21834399},
title = {{The Fechnerian idea}},
volume = {124},
year = {2011}
}
@article{Nickerson2017,
abstract = {Independent Component Analysis (ICA) is one of the most popular techniques for the analysis of resting state FMRI data because it has several advantageous properties when compared with other techniques. Most notably, in contrast to a conventional seed-based correlation analysis, it is model-free and multivariate, thus switching the focus from evaluating the functional connectivity of single brain regions identified a priori to evaluating brain connectivity in terms of all brain resting state networks (RSNs) that simultaneously engage in oscillatory activity. Furthermore, typical seed-based analysis characterizes RSNs in terms of spatially distributed patterns of correlation (typically by means of simple Pearson's coefficients) and thereby confounds together amplitude information of oscillatory activity and noise. ICA and other regression techniques, on the other hand, retain magnitude information and therefore can be sensitive to both changes in the spatially distributed nature of correlations (differences in the spatial pattern or "shape") as well as the amplitude of the network activity. Furthermore, motion can mimic amplitude effects so it is crucial to use a technique that retains such information to ensure that connectivity differences are accurately localized. In this work, we investigate the dual regression approach that is frequently applied with group ICA to assess group differences in resting state functional connectivity of brain networks. We show how ignoring amplitude effects and how excessive motion corrupts connectivity maps and results in spurious connectivity differences. We also show how to implement the dual regression to retain amplitude information and how to use dual regression outputs to identify potential motion effects. Two key findings are that using a technique that retains magnitude information, e.g., dual regression, and using strict motion criteria are crucial for controlling both network amplitude and motion-related amplitude effects, respectively, in resting state connectivity analyses. We illustrate these concepts using realistic simulated resting state FMRI data and in vivo data acquired in healthy subjects and patients with bipolar disorder and schizophrenia.},
author = {Nickerson, Lisa D. and Smith, Stephen M. and {\"{O}}ng{\"{u}}r, D{\"{o}}st and Beckmann, Christian F.},
doi = {10.3389/fnins.2017.00115},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Nickerson et al. - 2017 - Using dual regression to investigate network shape and amplitude in functional connectivity analyses.pdf:pdf},
isbn = {1662-4548 (Print) 1662-453X (Linking)},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {Amplitude,Dual regression,Functional connectivity,ICA,Resting state networks},
number = {MAR},
pages = {1--18},
pmid = {28348512},
title = {{Using dual regression to investigate network shape and amplitude in functional connectivity analyses}},
volume = {11},
year = {2017}
}
@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
eprint = {1012.2599},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Brochu, Cora, de Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Model.pdf:pdf},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}
@article{Schaal1999,
abstract = {This review investigates two recent developments in artificial intelligence and neural computation: learning from imitation and the development of humanoid robots. It is postulated that the study of imitation learning offers a promising route to gain new insights into mechanisms of perceptual motor control that could ultimately lead to the creation of autonomous humanoid robots. Imitation learning focuses on three important issues: efficient motor learning, the connection between action and perception, and modular motor control in the form of movement primitives. It is reviewed here how research on representations of, and functional connections between, action and perception have contributed to our understanding of motor acts of other beings. The recent discovery that some areas in the primate brain are active during both movement perception and execution has provided a hypothetical neural basis of imitation. Computational approaches to imitation learning are also described, initially from the perspective of traditional Al and robotics, but also from the perspective of neural network models and statistical-learning research. Parallels and differences between biological and computational approaches to imitation are highlighted and an overview of current projects that actually employ imitation learning for humanoid robots is given.},
author = {Schaal, Stefan},
doi = {10.1016/S1364-6613(99)01327-3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Schaal - 1999 - Is imitation learning the route to humanoid robots.pdf:pdf},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {6},
pages = {233--242},
pmid = {10354577},
title = {{Is imitation learning the route to humanoid robots?}},
volume = {3},
year = {1999}
}
@article{Wilson,
author = {Wilson, Andrew Gordon and Cunningham, John P},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wilson, Cunningham - Unknown - Fast Kernel Learning for Multidimensional Pattern Extrapolation.pdf:pdf},
pages = {1--9},
title = {{Fast Kernel Learning for Multidimensional Pattern Extrapolation}}
}
@article{Foti2014,
abstract = {Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.},
archivePrefix = {arXiv},
arxivId = {1411.1670},
author = {Foti, Nicholas J. and Xu, Jason and Laird, Dillon and Fox, Emily B.},
eprint = {1411.1670},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Foti et al. - 2014 - Stochastic variational inference for hidden Markov models.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {3599--3607},
title = {{Stochastic variational inference for hidden Markov models}},
volume = {4},
year = {2014}
}
@article{Lu2011,
abstract = {Increasingly large amount of multidimensional data are being generated on a daily basis in many applications. This leads to a strong demand for learning algorithms to extract useful information from these massive data. This paper surveys the field of multilinear subspace learning (MSL) for dimensionality reduction of multidimensional data directly from their tensorial representations. It discusses the central issues of MSL, including establishing the foundations of the field via multilinear projections, formulating a unifying MSL framework for systematic treatment of the problem, examining the algorithmic aspects of typical MSL solutions, and categorizing both unsupervised and supervised MSL algorithms into taxonomies. Lastly, the paper summarizes a wide range of MSL applications and concludes with perspectives on future research directions. {\textcopyright} 2011 Elsevier Ltd . All rights reserved.},
author = {Lu, Haiping and Plataniotis, Konstantinos N. and Venetsanopoulos, Anastasios N.},
doi = {10.1016/j.patcog.2011.01.004},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lu, Plataniotis, Venetsanopoulos - 2011 - A survey of multilinear subspace learning for tensor data.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Dimensionality reduction,Feature extraction,Multidimensional data,Multilinear,Subspace learning,Survey,Taxonomy,Tensor},
number = {7},
pages = {1540--1551},
pmid = {1187260},
title = {{A survey of multilinear subspace learning for tensor data}},
volume = {44},
year = {2011}
}
@article{Bates2015,
abstract = {The analysis of experimental data with mixed-effects models requires decisions about the specification of the appropriate random-effects structure. Recently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal' models with all possible random effect components included. Estimation of maximal models, however, may not converge. We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modeling with uninformative or weakly informative priors. Importantly, even under convergence, overparameterization may lead to uninterpretable models. We provide diagnostic tools for detecting overparameterization and guiding model simplification.},
archivePrefix = {arXiv},
arxivId = {1506.04967},
author = {Bates, Douglas and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
eprint = {1506.04967},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Bates et al. - 2015 - Parsimonious Mixed Models.pdf:pdf},
keywords = {crossed random effects,linear mixed models,model,model selection},
number = {2000},
title = {{Parsimonious Mixed Models}},
url = {http://arxiv.org/abs/1506.04967},
year = {2015}
}
@article{Polson2017,
abstract = {Deep learning is a form of machine learning for nonlinear high dimensional pattern matching and prediction. By taking a Bayesian probabilistic perspective, we provide a number of insights into more efficient algorithms for optimisation and hyper-parameter tuning. Traditional high-dimensional data reduction techniques, such as principal component analysis (PCA), partial least squares (PLS), reduced rank regression (RRR), projection pursuit regression (PPR) are all shown to be shallow learners. Their deep learning counterparts exploit multiple deep layers of data reduction which provide predictive performance gains. Stochastic gradient descent (SGD) training optimisation and Dropout (DO) regularization provide estimation and variable selection. Bayesian regularization is central to finding weights and connections in networks to optimize the predictive bias-variance trade-off. To illustrate our methodology, we provide an analysis of international bookings on Airbnb. Finally, we conclude with directions for future research.},
archivePrefix = {arXiv},
arxivId = {1706.00473},
author = {Polson, Nicholas G. and Sokolov, Vadim},
doi = {10.1214/17-BA1082},
eprint = {1706.00473},
isbn = {1551-6709},
issn = {19316690},
journal = {Bayesian Analysis},
number = {4},
pmid = {27315762},
title = {{Deep learning: A Bayesian perspective}},
volume = {12},
year = {2017}
}
@article{Dieng2016,
abstract = {Variational inference (VI) is widely used as an efficient alternative to Markov chain Monte Carlo. It posits a family of approximating distributions {\$}q{\$} and finds the closest member to the exact posterior {\$}p{\$}. Closeness is usually measured via a divergence {\$}D(q || p){\$} from {\$}q{\$} to {\$}p{\$}. While successful, this approach also has problems. Notably, it typically leads to underestimation of the posterior variance. In this paper we propose CHIVI, a black-box variational inference algorithm that minimizes {\$}D{\_}{\{}\backslashchi{\}}(p || q){\$}, the {\$}\backslashchi{\$}-divergence from {\$}p{\$} to {\$}q{\$}. CHIVI minimizes an upper bound of the model evidence, which we term the {\$}\backslashchi{\$} upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty, and it can also be used with the classical VI lower bound (ELBO) to provide a sandwich estimate of the model evidence. We study CHIVI on three models: probit regression, Gaussian process classification, and a Cox process model of basketball plays. When compared to expectation propagation and classical VI, CHIVI produces better error rates and more accurate estimates of posterior variance.},
archivePrefix = {arXiv},
arxivId = {1611.00328},
author = {Dieng, Adji B. and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David M.},
doi = {10.1007/s00261-011-9795-9},
eprint = {1611.00328},
issn = {0963-7214},
pmid = {21904887},
title = {{Variational Inference via {\$}\backslashchi{\$}-Upper Bound Minimization}},
year = {2016}
}
@article{Shahbazi2016,
abstract = {Kernel-based methods, and in particular the so-called kernel trick, which is used in statistical learning theory as a means of avoiding expensive high-dimensional computations, have broad and constructive implications for the cognitive and brain sciences. An equivalent and complementary view of kernels as a measure of similarity highlights their effectiveness in low-dimensional and low-complexity learning and generalization - tasks that are indispensable in cognitive information processing. In this survey, we seek (i) to highlight some parallels between kernels in machine learning on the one hand and similarity in psychology and neuroscience on the other hand, (ii) to sketch out new research directions arising from these parallels, and (iii) to clarify some aspects of the way kernels are presented and discussed in the literature that may have affected their perceived relevance to cognition. In particular, we aim to resolve the tension between the view of kernels as a method of raising the dimensionality, and the various requirements of reducing dimensionality for cognitive purposes. We identify four fundamental constraints that apply to any cognitive system that is charged with learning from the statistics of its world, and argue that kernel-like neural computation is particularly suited to serving such learning and decision making needs, while simultaneously satisfying these constraints.},
author = {Shahbazi, Reza and Raizada, Rajeev and Edelman, Shimon},
doi = {10.1016/j.jmp.2015.11.004},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Shahbazi, Raizada, Edelman - 2016 - Similarity, kernels, and the fundamental constraints on cognition.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Complexity,Dimensionality,Kernel,Linear separability,Nonlinear,RKHS,Similarity},
pages = {21--34},
publisher = {Elsevier Inc.},
title = {{Similarity, kernels, and the fundamental constraints on cognition}},
url = {http://dx.doi.org/10.1016/j.jmp.2015.11.004},
volume = {70},
year = {2016}
}
@article{Wenliang2019,
abstract = {Humans and other animals are frequently near-optimal in their ability to integrate noisy and ambiguous sensory data to form robust percepts—which are informed both by sensory evidence and by prior experience about the causal structure of the environment. It is hypothesized that the brain establishes these structures using an internal model of how the observed patterns can be generated from relevant but unobserved causes. In dynamic environments, such integration often takes the form of postdiction, wherein later sensory evidence affects inferences about earlier percepts. As the brain must operate in current time, without the luxury of acausal propagation of information, how does such postdictive inference come about? Here, we propose a general framework for neural probabilistic inference in dynamic models based on the distributed distributional code (DDC) representation of uncertainty, naturally extending the underlying encoding to incorporate implicit probabilistic beliefs about both present and past. We show that, as in other uses of the DDC, an inferential model can be learnt efficiently using samples from an internal model of the world. Applied to stimuli used in the context of psychophysics experiments, the framework provides an online and plausible mechanism for inference, including postdictive effects.},
author = {Wenliang, Li Kevin and Sahani, Maneesh},
doi = {10.1101/672089},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wenliang, Sahani - 2019 - A neurally plausible model for online recognition and postdiction.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{A neurally plausible model for online recognition and postdiction.}},
url = {https://www.biorxiv.org/content/biorxiv/early/2019/06/14/672089.full.pdf},
year = {2019}
}
@article{Levitt1971,
abstract = {During the past decade a number of variations in the simple up-down procedure have been used in psycho-acoustic testing. A broad class of these methods is described with due emphasis on the related problems of parameter estimation and the efficient placing of observations. The advantages of up-down methods are many, including simplicity, high efficiency, robustness, small-sample reliability, and relative freedom from restrictive assumptions. Several applications of these procedures in psychoacoustics are described, including examples where conventional techniques are inapplicable.},
author = {Levitt, H.},
doi = {10.1121/1.1912375},
file = {:Users/mshvarts/Downloads/psychoacoustics.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {2B},
pages = {467--477},
pmid = {5541744},
title = {{Transformed Up‐Down Methods in Psychoacoustics}},
volume = {49},
year = {1971}
}
@article{Dubey1991,
abstract = {Introducci{\'{o}}n b{\'{a}}sica a los tensores y sus propiedades},
author = {Dubey, P. S.},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Dubey - 1991 - Biomonitoring the environment-Theory, practice and problems.pdf:pdf},
issn = {02548704},
journal = {Journal of Environmental Biology},
number = {SPEC. ISS.},
pages = {233--241},
title = {{Biomonitoring the environment-Theory, practice and problems}},
volume = {12},
year = {1991}
}
@article{Tang2018,
abstract = {Widely used recurrent units, including Long-short Term Memory (LSTM) and Gated Recurrent Unit (GRU), perform well on natural language tasks, but their ability to learn structured representations is still questionable. Exploiting Tensor Product Representations (TPRs) --- distributed representations of symbolic structure in which vector-embedded symbols are bound to vector-embedded structural positions --- we propose the TPRU, a recurrent unit that, at each time step, explicitly executes structural-role binding and unbinding operations to incorporate structural information into learning. Experiments are conducted on both the Logical Entailment task and the Multi-genre Natural Language Inference (MNLI) task, and our TPR-derived recurrent unit provides strong performance with significantly fewer parameters than LSTM and GRU baselines. Furthermore, our learnt TPRU trained on MNLI demonstrates solid generalisation ability on downstream tasks.},
archivePrefix = {arXiv},
arxivId = {1810.12456},
author = {Tang, Shuai and Smolensky, Paul and de Sa, Virginia R.},
doi = {arXiv:1810.12456v1},
eprint = {1810.12456},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Tang, Smolensky, de Sa - 2018 - Learning Distributed Representations of Symbolic Structure Using Binding and Unbinding Operations.pdf:pdf},
number = {Nips},
title = {{Learning Distributed Representations of Symbolic Structure Using Binding and Unbinding Operations}},
url = {http://arxiv.org/abs/1810.12456},
year = {2018}
}
@article{Zadeh2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1707.07250v1},
author = {Zadeh, Amir and Chen, Minghai and Cambria, Erik},
eprint = {arXiv:1707.07250v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zadeh, Chen, Cambria - 2017 - Tensor Fusion Network for Multimodal Sentiment Analysis.pdf:pdf},
keywords = {ek laboratories},
title = {{Tensor Fusion Network for Multimodal Sentiment Analysis}},
year = {2017}
}
@article{Lloyd2014a,
abstract = {This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural- language text. Our approach treats unknown regression functions non- parametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state- of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.},
archivePrefix = {arXiv},
arxivId = {1402.4304},
author = {Lloyd, James Robert and Duvenaud, David and Grosse, Roger and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
eprint = {1402.4304},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Lloyd et al. - 2014 - Automatic construction and natural-language description of nonparametric regression models.pdf:pdf},
isbn = {9781577356783},
journal = {Proceedings of the National Conference on Artificial Intelligence},
pages = {1242--1250},
title = {{Automatic construction and natural-language description of nonparametric regression models}},
volume = {2},
year = {2014}
}
@article{Mulder2014,
abstract = {Executive function (EF) is an important predictor of numerous developmental outcomes, such as academic achievement and behavioral adjustment. Although a plethora of measurement instruments exists to assess executive function in children, only few of these are suitable for toddlers, and even fewer have undergone psychometric evaluation. The present study evaluates the psychometric properties and validity of an assessment battery for measuring EF in two-year-olds. A sample of 2437 children were administered the assessment battery at a mean age of 2;4 years (SD = 0;3 years) in a large-scale field study. Measures of both hot EF (snack and gift delay tasks) and cool EF (six boxes, memory for location, and visual search task) were included. Confirmatory Factor Analyses showed that a two-factor hot and cool EF model fitted the data better than a one-factor model. Measurement invariance was supported across groups differing in age, gender, socioeconomic status (SES), home language, and test setting. Criterion and convergent validity were evaluated by examining relationships between EF and age, gender, SES, home language, and parent and teacher reports of children's attention and inhibitory control. Predictive validity of the test battery was investigated by regressing children's pre-academic skills and behavioral problems at age three on the latent hot and cool EF factors at age 2 years. The test battery showed satisfactory psychometric quality and criterion, convergent, and predictive validity. Whereas cool EF predicted both pre-academic skills and behavior problems 1 year later, hot EF predicted behavior problems only. These results show that EF can be assessed with psychometrically sound instruments in children as young as 2 years, and that EF tasks can be reliably applied in large scale field research. The current instruments offer new opportunities for investigating EF in early childhood, and for evaluating interventions targeted at improving EF from a young age.},
author = {Mulder, Hanna and Hoofs, Huub and Verhagen, Josje and van der Veen, Ineke and Leseman, Paul P.M.},
doi = {10.3389/fpsyg.2014.00733},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Mulder et al. - 2014 - Psychometric properties and convergent and predictive validity of an executive function test battery for two-year.pdf:pdf},
isbn = {1664-1078},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Delay of gratification,Executive function,Psychometrics,Selective attention,Toddlers,Validity,Working memory},
number = {JUL},
pmid = {25101015},
title = {{Psychometric properties and convergent and predictive validity of an executive function test battery for two-year-olds}},
volume = {5},
year = {2014}
}
@article{Sobol1967,
author = {Sobol', I.M},
doi = {10.1016/0041-5553(67)90144-9},
file = {:Users/mshvarts/Downloads/1933-thompson (2).pdf:pdf},
issn = {00415553},
journal = {USSR Computational Mathematics and Mathematical Physics},
month = {jan},
number = {4},
pages = {86--112},
title = {{On the distribution of points in a cube and the approximate evaluation of integrals}},
url = {https://www.jstor.org/stable/2332286?origin=crossref https://linkinghub.elsevier.com/retrieve/pii/0041555367901449},
volume = {7},
year = {1967}
}
@article{Titsias,
author = {Titsias, Michalis K and Miguel, L},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Titsias, Miguel - Unknown - Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning.pdf:pdf},
pages = {1--9},
title = {{Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning}}
}
@article{Cai2018,
abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
archivePrefix = {arXiv},
arxivId = {arXiv:1709.07604v3},
author = {Cai, Hongyun and Zheng, Vincent W. and Chang, Kevin Chen Chuan},
doi = {10.1109/TKDE.2018.2807452},
eprint = {arXiv:1709.07604v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Zheng, Chang - 2018 - A Comprehensive Survey of Graph Embedding Problems, Techniques, and Applications.pdf:pdf},
issn = {15582191},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Graph embedding,graph analytics,graph embedding survey,network embedding},
number = {9},
pages = {1616--1637},
title = {{A Comprehensive Survey of Graph Embedding: Problems, Techniques, and Applications}},
volume = {30},
year = {2018}
}
@article{Yang2017,
abstract = {The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors. To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves. We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
archivePrefix = {arXiv},
arxivId = {1707.01786},
author = {Yang, Yinchong and Krompass, Denis and Tresp, Volker},
eprint = {1707.01786},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yang, Krompass, Tresp - 2017 - Tensor-Train Recurrent Neural Networks for Video Classification.pdf:pdf},
title = {{Tensor-Train Recurrent Neural Networks for Video Classification}},
url = {http://arxiv.org/abs/1707.01786},
year = {2017}
}
@article{Poole2016,
author = {Poole, B and Williams, A and Maheswaranathan, N and Yu, B and Santhanam, G and Ryu, S I and Baccus, S A and Shenoy, K and Ganguli, S},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Poole et al. - 2016 - Time-warped PCA simultaneous alignment and dimensionality reduction of neural data.pdf:pdf},
pages = {2--3},
title = {{Time-warped PCA : simultaneous alignment and dimensionality reduction of neural data}},
year = {2016}
}
@article{Held2017,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
archivePrefix = {arXiv},
arxivId = {1705.06366},
author = {Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
eprint = {1705.06366},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Held et al. - 2017 - Automatic Goal Generation for Reinforcement Learning Agents.pdf:pdf},
isbn = {076453601X},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {http://arxiv.org/abs/1705.06366},
year = {2017}
}
@article{Sun2019a,
abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
archivePrefix = {arXiv},
arxivId = {1903.05779},
author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
eprint = {1903.05779},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Sun et al. - 2019 - Functional variational Bayesian neural networks.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
pages = {1--22},
title = {{Functional variational Bayesian neural networks}},
year = {2019}
}
@article{Cialenco2012,
abstract = {In this paper we present the theoretical framework needed to justify the use of a kernel-based collocation method (meshfree approximation method) to estimate the solution of high-dimensional stochastic partial differential equations (SPDEs). Using an implicit time stepping scheme, we transform stochastic parabolic equations into stochastic elliptic equations. Our main attention is concentrated on the numerical solution of the elliptic equations at each time step. The estimator of the solution of the elliptic equations is given as a linear combination of reproducing kernels derived from the differential and boundary operators of the SPDE centered at collocation points to be chosen by the user. The random expansion coefficients are computed by solving a random system of linear equations. Numerical experiments demonstrate the feasibility of the method.},
author = {Cialenco, Igor and Fasshauer, Gregory E. and Ye, Qi},
doi = {10.1080/00207160.2012.688111},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cialenco, Fasshauer, Ye - 2012 - Approximation of stochastic partial differential equations by a kernel-based collocation method.pdf:pdf},
issn = {00207160},
journal = {International Journal of Computer Mathematics},
keywords = {Gaussian process,Mat{\'{e}}rn function,kernel-based collocation,numerical solutions,reproducing kernel,stochastic partial differential equation},
number = {18},
pages = {2543--2561},
title = {{Approximation of stochastic partial differential equations by a kernel-based collocation method}},
volume = {89},
year = {2012}
}
@article{Moreno-munoz2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1805.07633v2},
author = {Moreno-mu{\~{n}}oz, Pablo and Art{\'{e}}s-rodr{\'{i}}guez, Antonio and {\'{A}}lvarez, Mauricio A},
eprint = {arXiv:1805.07633v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Moreno-mu{\~{n}}oz, Art{\'{e}}s-rodr{\'{i}}guez, {\'{A}}lvarez - 2018 - Heterogeneous Multi-output Gaussian Process Prediction.pdf:pdf},
number = {NeurIPS},
title = {{Heterogeneous Multi-output Gaussian Process Prediction}},
year = {2018}
}
@article{Henderson1981,
abstract = {The vec-permutation matrix Im, n is defined by the equation vecAm×n = Im, n vecA', where vec is the vec operator such that vecA is the vector of columns of A stacked one under the other. The variety of definitions, names and notations for Im, n are discussed, and its properties are developed by simple proofs in contrast to certain lengthy proofs in the literature that are based on descriptive definitions. For example, the role of Im, n in reversing the order of Kronecker products is succinctly derived using the vec operator. The matrix Mm, n is introduced as Mm, n= Im, nM; it is the matrix having as rows, every nth row of M, of order mn × c, starting with the first, then every nth row starting with the second, and so on. Special cases of Mm, n are discussed. {\textcopyright} 1981, Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Henderson, Harold V. and Searle, S. R.},
doi = {10.1080/03081088108817379},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Henderson, Searle - 1981 - The VEC-Permutation Matrix, the VEC Operator and Kronecker Products A Review.pdf:pdf},
issn = {15635139},
journal = {Linear and Multilinear Algebra},
number = {4},
pages = {271--288},
title = {{The VEC-Permutation Matrix, the VEC Operator and Kronecker Products: A Review}},
volume = {9},
year = {1981}
}
@article{Maddison2017,
abstract = {When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.},
archivePrefix = {arXiv},
arxivId = {1705.09279},
author = {Maddison, Chris J. and Lawson, Dieterich and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee Whye},
eprint = {1705.09279},
issn = {10495258},
title = {{Filtering Variational Objectives}},
year = {2017}
}
@article{Lieder2018,
abstract = {People's estimates of numerical quantities are systematically biased towards their initial guess. This anchoring bias is usually interpreted as sign of human irrationality, but it has recently been suggested that the anchoring bias instead results from people's rational use of their finite time and limited cognitive resources. If this were true, then adjustment should decrease with the relative cost of time. To test this hypothesis, we designed a new numerical estimation paradigm that controls people's knowledge and varies the cost of time and error independently while allowing people to invest as much or as little time and effort into refining their estimate as they wish. Two experiments confirmed the prediction that adjustment decreases with time cost but increases with error cost regardless of whether the anchor was self-generated or provided. These results support the hypothesis that people rationally adapt their number of adjustments to achieve a near-optimal speed-accuracy tradeoff. This suggests that the anchoring bias might be a signature of the rational use of finite time and limited cognitive resources rather than a sign of human irrationality.},
author = {Lieder, Falk and Griffiths, Thomas L. and Quentin, Quentin J. and Goodman, Noah D.},
doi = {10.3758/s13423-017-1288-6},
isbn = {1342301712886},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
number = {2},
pmid = {28484951},
title = {{Empirical evidence for resource-rational anchoring and adjustment}},
volume = {25},
year = {2018}
}
@article{Hidaka2010,
author = {Hidaka, Shohei and Yu, Chen},
doi = {10.1145/1891903.1891968},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hidaka, Yu - 2010 - Analyzing multimodal time series as dynamical systems.pdf:pdf},
isbn = {9781450304146},
keywords = {bol dynamics,generating partition,multi-agent communication,multi-stream time series,sym-},
pages = {1},
title = {{Analyzing multimodal time series as dynamical systems}},
year = {2010}
}
@article{Hoffman2012,
abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
archivePrefix = {arXiv},
arxivId = {1206.7051},
author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
doi = {citeulike-article-id:10852147},
eprint = {1206.7051},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Hoffman et al. - 2012 - Stochastic Variational Inference.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
pages = {1303--1347},
pmid = {19926898},
title = {{Stochastic Variational Inference}},
url = {http://arxiv.org/abs/1206.7051},
volume = {14},
year = {2012}
}
@article{CabralFarias2016,
abstract = {A Bayesian framework is proposed to define flexible coupling models for joint tensor decompositions of multiple data sets. Under this framework, a natural formulation of the data fusion problem is to cast it in terms of a joint maximum a posteriori (MAP) estimator. Data driven scenarios of joint posterior distributions are provided, including general Gaussian priors and non Gaussian coupling priors. We present and discuss implementation issues of algorithms used to obtain the joint MAP estimator. We also show how this framework can be adapted to tackle the problem of joint decompositions of large datasets. In the case of a conditional Gaussian coupling with a linear transformation, we give theoretical bounds on the data fusion performance using the Bayesian Cramer-Rao bound. Simulations are reported for hybrid coupling models ranging from simple additive Gaussian models, to Gamma-type models with positive variables and to the coupling of data sets which are inherently of different size due to different resolution of the measurement devices.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.07717v3},
author = {{Cabral Farias}, Rodrigo and Cohen, Jeremy Emile and Comon, Pierre},
doi = {10.1109/TSP.2016.2576425},
eprint = {arXiv:1505.07717v3},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Cabral Farias, Cohen, Comon - 2016 - Exploring multimodal data fusion through joint decompositions with flexible couplings.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Cram{\'{e}}r-Rao bound,Tensor decompositions,big data,coupled decompositions,data fusion,multimodal data},
number = {18},
pages = {4830--4844},
title = {{Exploring multimodal data fusion through joint decompositions with flexible couplings}},
volume = {64},
year = {2016}
}
@article{Agrawal2016,
author = {Agrawal, Pulkit and Abbeel, Pieter},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Agrawal, Abbeel - 2016 - Learning to Poke by Poking Experiential Learning of Intuitive Physics.pdf:pdf},
number = {Nips},
title = {{Learning to Poke by Poking : Experiential Learning of Intuitive Physics}},
year = {2016}
}
@article{Yousefnezhad2017,
abstract = {This paper proposes Deep Hyperalignment (DHA) as a regularized, deep extension, scalable Hyperalignment (HA) method, which is well-suited for applying functional alignment to fMRI datasets with nonlinearity, high-dimensionality (broad ROI), and a large number of subjects. Unlink previous methods, DHA is not limited by a restricted fixed kernel function. Further, it uses a parametric approach, rank-{\$}m{\$} Singular Value Decomposition (SVD), and stochastic gradient descent for optimization. Therefore, DHA has a suitable time complexity for large datasets, and DHA does not require the training data when it computes the functional alignment for a new subject. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms.},
archivePrefix = {arXiv},
arxivId = {1710.03923},
author = {Yousefnezhad, Muhammad and Zhang, Daoqiang},
eprint = {1710.03923},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yousefnezhad, Zhang - 2017 - Deep Hyperalignment.pdf:pdf},
issn = {10495258},
number = {Nips},
title = {{Deep Hyperalignment}},
url = {http://arxiv.org/abs/1710.03923},
year = {2017}
}
@article{Azarnavid2019,
abstract = {In this paper we combine the theory of reproducing kernel Hilbert spaces with the field of collocation methods to solve boundary value problems with special emphasis on reproducing property of kernels. From the reproducing property of kernels we proposed a new efficient algorithm to obtain the cardinal functions of a reproducing kernel Hilbert space which can be apply conveniently for multidimensional domains. The differentiation matrices are constructed and also we drive pointwise error estimate of applying them. In addition we prove the nonsingularity of collocation matrix. The proposed method is truly meshless and can be applied conveniently and accurately for high order and also multidimensional problems. Numerical results are presented for the several problems such as second and fifth order two point boundary value problems, one and two dimensional unsteady Burgers equations and a parabolic partial differential equation in three dimensions. Also we compare the numerical results with those reported in the literature to show the high accuracy and efficiency of the proposed method},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.01364v2},
author = {Azarnavid, Babak and Emamjome, Mahdi and Nabati, Mohammad and Abbasbandy, Saeid},
doi = {10.1007/s40314-019-0838-0},
eprint = {arXiv:1705.01364v2},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Azarnavid et al. - 2019 - A reproducing kernel Hilbert space approach in meshless collocation method.pdf:pdf},
issn = {18070302},
journal = {Computational and Applied Mathematics},
keywords = {Cardinal functions,Collocation method,Differentiation matrix,Meshless method,Reproducing kernel Hilbert space},
number = {2},
title = {{A reproducing kernel Hilbert space approach in meshless collocation method}},
volume = {38},
year = {2019}
}
@article{Yu,
author = {Yu, Byron M and Afshar, Afsheen and Santhanam, Gopal and Ryu, Stephen I and Shenoy, Krishna V},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - Unknown - Extracting Dynamical Structure Embedded in Neural Activity.pdf:pdf},
title = {{Extracting Dynamical Structure Embedded in Neural Activity}}
}
@article{VanderWilk2017,
abstract = {We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, which have both been known to be challenging for Gaussian processes. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBF kernels to further improve performance. We hope that this illustration of the usefulness of a marginal likelihood will help automate discovering architectures in larger models.},
archivePrefix = {arXiv},
arxivId = {1709.01894},
author = {van der Wilk, Mark and Rasmussen, Carl Edward and Hensman, James},
eprint = {1709.01894},
issn = {10495258},
title = {{Convolutional Gaussian Processes}},
year = {2017}
}
@article{Marceau-caron,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.01076v1},
author = {Marceau-caron, Ga{\'{e}}tan},
eprint = {arXiv:1712.01076v1},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Marceau-caron - Unknown - Natural Langevin Dynamics for Neural Networks.pdf:pdf},
number = {1},
pages = {1--11},
title = {{Natural Langevin Dynamics for Neural Networks}}
}
@article{Trivedi2017,
abstract = {The availability of large scale event data with time stamps has given rise to dynamically evolving knowledge graphs that contain temporal information for each edge. Reasoning over time in such dynamic knowledge graphs is not yet well understood. To this end, we present Know-Evolve, a novel deep evolutionary knowledge network that learns non-linearly evolving entity representations over time. The occurrence of a fact (edge) is modeled as a multivariate point process whose intensity function is modulated by the score for that fact computed based on the learned entity embeddings. We demonstrate significantly improved performance over various relational learning approaches on two large scale real-world datasets. Further, our method effectively predicts occurrence or recurrence time of a fact which is novel compared to prior reasoning approaches in multi-relational setting.},
archivePrefix = {arXiv},
arxivId = {1705.05742},
author = {Trivedi, Rakshit and Dai, Hanjun and Wang, Yichen and Song, Le},
eprint = {1705.05742},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Trivedi et al. - 2017 - Know-Evolve Deep Temporal Reasoning for Dynamic Knowledge Graphs.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
title = {{Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs}},
url = {http://arxiv.org/abs/1705.05742},
year = {2017}
}
@article{Torre2012,
author = {Torre, Fernando De},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Torre - 2012 - A Least-Squares Framework for Component Analysis.pdf:pdf},
journal = {Ieee Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
pages = {1041--1055},
title = {{A Least-Squares Framework for Component Analysis}},
url = {http://www.cs.cmu.edu/{~}ftorre/uca{\_}final{\_}revision.pdf},
volume = {34},
year = {2012}
}
@article{Salvucci2003,
abstract = {Cognitive modeling has evolved into a powerful tool for understanding and predicting user behavior. Higher-level modeling frameworks such as GOMS and its variants facilitate fast and easy model development but are sometimes limited in their ability to model detailed user behavior. Lower-level cognitive architectures such as EPIC, ACT-R, and Soar allow for greater precision and direct interaction with real-world systems but require significant modeling training and expertise. In this paper we present a modeling framework, ACT-Simple, that aims to combine the advantages of both approaches to cognitive modeling. ACT-Simple embodies a "compilation" approach in which a simple description language is compiled down to a core lower-level architecture (namely ACT-R). We present theoretical justification and empirical validation of the usefulness of the approach and framework.},
author = {Salvucci, Dario D. and Salvucci, Dario D. and Lee, Frank J. and Lee, Frank J.},
doi = {10.1.1.5.8735},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Salvucci et al. - 2003 - Simple Cognitive Modeling in a Complex Cognitive Architecture.pdf:pdf},
isbn = {1581136307},
journal = {Human Factors in Computing Systems: CHI 2003 Conference Proceedings},
keywords = {act-r,cognitive architectures,cognitive modeling},
pages = {265--272},
title = {{Simple Cognitive Modeling in a Complex Cognitive Architecture}},
year = {2003}
}
@article{Wang2014b,
abstract = {In this article, we propose a generalized Gaussian process concurrent regression model for functional data, where the functional response variable has a binomial, Poisson, or other non-Gaussian distribution from an exponential family, while the covariates are mixed functional and scalar variables. The proposed model offers a nonparametric generalized concurrent regression method for functional data with multidimensional covariates, and provides a natural framework on modeling common mean structure and covariance structure simultaneously for repeatedly observed functional data. The mean structure provides overall information about the observations, while the covariance structure can be used to catch up the characteristic of each individual batch. The prior specification of covariance kernel enables us to accommodate a wide class of nonlinear models. The definition of the model, the inference, and the implementation as well as its asymptotic properties are discussed. Several numerical examples with different non-Gaussian response variables are presented. Some technical details and more numerical examples as well as an extension of the model are provided as supplementary materials.},
archivePrefix = {arXiv},
arxivId = {1401.8189},
author = {Wang, Bo and Shi, Jian Qing},
doi = {10.1080/01621459.2014.889021},
eprint = {1401.8189},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Shi - 2014 - Generalized Gaussian Process Regression Model for Non-Gaussian Functional Data.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Concurrent regression models,Covariance kernel,Exponential family,Nonparametric regression},
number = {507},
pages = {1123--1133},
title = {{Generalized Gaussian Process Regression Model for Non-Gaussian Functional Data}},
volume = {109},
year = {2014}
}
@article{Zhang2017c,
abstract = {We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for non-convex optimization. The algorithm performs stochastic gradient descent, where in each step it injects appropriately scaled Gaussian noise to the update. We analyze the algorithm's hitting time to an arbitrary subset of the parameter space. Two results follow from our general theory: First, we prove that for empirical risk minimization, if the empirical risk is point-wise close to the (smooth) population risk, then the algorithm achieves an approximate local minimum of the population risk in polynomial time, escaping suboptimal local minima that only exist in the empirical risk. Second, we show that SGLD improves on one of the best known learnability results for learning linear classifiers under the zero-one loss.},
archivePrefix = {arXiv},
arxivId = {1702.05575},
author = {Zhang, Yuchen and Liang, Percy and Charikar, Moses},
eprint = {1702.05575},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Liang, Charikar - 2017 - A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics.pdf:pdf},
number = {1},
pages = {1--43},
title = {{A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics}},
url = {http://arxiv.org/abs/1702.05575},
volume = {65},
year = {2017}
}
@article{Takeno2019,
abstract = {Bayesian optimization (BO) is an effective tool for black-box optimization in which objective function evaluation is usually quite expensive. In practice, lower fidelity approximations of the objective function are often available. Recently, multi-fidelity Bayesian optimization (MFBO) has attracted considerable attention because it can dramatically accelerate the optimization process by using those cheaper observations. We propose a novel information theoretic approach to MFBO. Information-based approaches are popular and empirically successful in BO, but existing studies for information-based MFBO are plagued by difficulty for accurately estimating the information gain. Our approach is based on a variant of information-based BO called max-value entropy search (MES), which greatly facilitates evaluation of the information gain in MFBO. In fact, computations of our acquisition function is written analytically except for one dimensional integral and sampling, which can be calculated efficiently and accurately. We demonstrate effectiveness of our approach by using synthetic and benchmark datasets, and further we show a real-world application to materials science data.},
archivePrefix = {arXiv},
arxivId = {1901.08275},
author = {Takeno, Shion and Fukuoka, Hitoshi and Tsukada, Yuhki and Koyama, Toshiyuki and Shiga, Motoki and Takeuchi, Ichiro and Karasuyama, Masayuki},
eprint = {1901.08275},
file = {:Users/mshvarts/Library/Application Support/Mendeley Desktop/Downloaded/Takeno et al. - 2019 - Multi-fidelity Bayesian Optimization with Max-value Entropy Search.pdf:pdf},
pages = {1--26},
title = {{Multi-fidelity Bayesian Optimization with Max-value Entropy Search}},
url = {http://arxiv.org/abs/1901.08275},
year = {2019}
}
@inproceedings{Calandra2016a,
abstract = {Off-the-shelf Gaussian Process (GP) covariance functions encode smoothness assumptions on the structure of the function to be modeled. To model complex and non-differentiable functions, these smoothness assumptions are often too restrictive. One way to alleviate this limitation is to find a different representation of the data by introducing a feature space. This feature space is often learned in an unsupervised way, which might lead to data representations that are not useful for the overall regression task. In this paper, we propose Manifold Gaussian Processes, a novel supervised method that jointly learns a transformation of the data into a feature space and a GP regression from the feature space to observed space. The Manifold GP is a full GP and allows to learn data representations, which are useful for the overall regression task. As a proof-of-concept, we evaluate our approach on complex non-smooth functions where standard GPs perform poorly, such as step functions and robotics tasks with contacts.},
archivePrefix = {arXiv},
arxivId = {1402.5876},
author = {Calandra, Roberto and Peters, Jan and Rasmussen, Carl Edward and Deisenroth, Marc Peter},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2016.7727626},
eprint = {1402.5876},
isbn = {9781509006199},
issn = {10504729},
title = {{Manifold Gaussian Processes for regression}},
volume = {2016-Octob},
year = {2016}
}
@article{Klein2001,
abstract = {The psychometric function, relating the subject's response to the physical stimulus, is fundamental to psychophysics.This paper examines various psychometric function topics, many inspired by this special symposiumissue of Perception{\&} Psychophysics: What are the relativemerits of objective yes/no versus forced choice tasks (including threshold variance)?What are the relativemerits of adaptive versus constant stimulimethods?What are the relativemerits of likelihood versus up–down staircaseadaptivemethods? Is 2AFC free of substantial bias? Is there no efficient adaptive method for objective yes/no tasks? Should adaptive methods aimfor 90{\%} correct? Can adding more responses to forced choice and objective yes/no tasks reduce the threshold variance?What is the best way to dealwith lapses?How is theWeibull function intimately related to the d¢ function? What causes bias in the likelihood goodness-of-fit? What causes bias in slope estimates from adaptive methods? How good are nonparametric methods for estimating psychometric function parameters?Of what value is the psychometric function slope? How are various psychometric functions relatedto each other? The resolution of many of these issues is surprising.},
author = {Klein, Stanley A},
doi = {10.3758/BF03194552},
file = {:Users/mshvarts/Downloads/Klein2001{\_}Article{\_}MeasuringEstimatingAndUndersta.pdf:pdf},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
month = {nov},
number = {8},
pages = {1421--1455},
title = {{Measuring, estimating, and understanding the psychometric function: A commentary}},
url = {https://link.springer.com/content/pdf/10.3758/BF03194552.pdf http://link.springer.com/10.3758/BF03194552},
volume = {63},
year = {2001}
}
@article{Wier1977,
abstract = {Frequency discrimination was measured for frequencies from 200 to 8000 Hz and for sensation levels from 5 to 80 dB using pulsed sinusoids as stimuli in an adaptive two-interval force-choice psychophysical procedure. An analysis of variance indicated significant effects of frequency and sensation level, and of the interaction between frequency and sensation level. The effect of sensation level is greatest at low frequencies and decreases at high frequenices, being quite small at 8000 Hz. The data are used to evaluate the predictions of current theoretical models. {\textcopyright} 1977, Acoustical Society of America. All rights reserved.},
author = {Wier, Craig C. and Jesteadt, Walt and Green, David M.},
doi = {10.1121/1.381251},
file = {:Users/mshvarts/Downloads/1.381251.pdf:pdf},
issn = {NA},
journal = {Journal of the Acoustical Society of America},
number = {1},
pages = {178--184},
pmid = {833369},
title = {{Frequency discrimination as a function of frequency and sensation level}},
volume = {61},
year = {1977}
}
@article{Legge1984,
abstract = {Binocular summation was evaluated for contrast detection and discrimination. Monocular and binocular forced-choice psychometric functions were measured for the detection of 0.5-c/deg sine-wave gratings presented alone (simple detection), or superimposed on identical background gratings (discrimination). The dependence of detectability d′ on signal contrast C could be described by: d′ = (C/C′)n {\textperiodcentered} C′ is threshold contrast, and n is an index of the steepness of the psychometric function, n was near 2 for simple detection, near 1 for discrimination, and was approximately the same for monocular and binocular viewing. Monocular thresholds were about 1.5 times binocular thresholds for detection, but the ratio dropped for suprathreshold discrimination. These results reveal a dependence of binocular summation on background contrast. For simple detection, binocular detectabilities were at least twice monocular detectabilities. For contrast discrimination, the amount of binocular summation decreased. For a 25{\%}-contrast background, there was little or no binocular summation. It is concluded that binocular contrast summation decreases as background contrast rises.},
author = {Legge, Gordon E.},
doi = {10.1016/0042-6989(84)90063-4},
file = {:Users/mshvarts/Downloads/legge84{\_}binocular{\_}contrast{\_}summation.{\_}i.{\_}detection{\_}and{\_}discrimination.pdf:pdf},
issn = {00426989},
journal = {Vision Research},
keywords = {Atherosclerosis,HDL,Hyperlipidemia,LDL,Risk factors},
month = {jan},
number = {4},
pages = {373--383},
title = {{Binocular contrast summation—I. Detection and discrimination}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0042698984900634},
volume = {24},
year = {1984}
}
@article{Mikkelsen2020,
abstract = {Somatosensory processing can be probed empirically through vibrotactile psychophysical experiments. Psychophysical approaches are valuable for investigating both normal and abnormal tactile function in healthy and clinical populations. To date, the test-retest reliability of vibrotactile detection and discrimination thresholds has yet to be established. This study sought to assess the reproducibility of vibrotactile detection and discrimination thresholds in human adults using an established vibrotactile psychophysical battery. Fifteen healthy adults underwent three repeat sessions of an eleven-task battery that measured a range of vibrotactile measures, including reaction time, detection threshold, amplitude and frequency discrimination, and temporal order judgement. Coefficients of variation and intraclass correlation coefficients (ICCs) were calculated for the measures in each task. Linear mixed-effects models were used to test for length and training effects and differences between tasks within the same domain. Reaction times were shown to be the most reproducible (ICC: {\~{}}0.9) followed by detection thresholds (ICC: {\~{}}0.7). Frequency discrimination thresholds were the least reproducible (ICC: {\~{}}0.3). As reported in prior studies, significant differences in measures between related tasks were also found, demonstrating the reproducibility of task-related effects. These findings show that vibrotactile detection and discrimination thresholds are reliable, further supporting the use of psychophysical experiments to probe tactile function.},
author = {Mikkelsen, Mark and He, Jason and Tommerdahl, Mark and Edden, Richard A.E. and Mostofsky, Stewart H. and Puts, Nicolaas A.J.},
doi = {10.1038/s41598-020-63208-z},
file = {:Users/mshvarts/Downloads/s41598-020-63208-z.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--14},
pmid = {32300187},
title = {{Reproducibility of flutter-range vibrotactile detection and discrimination thresholds}},
volume = {10},
year = {2020}
}
@article{Chadwick2015,
abstract = {Gloss is a relatively little studied visual property of objects' surfaces. The earliest recorded scientific reference to gloss appears to have been by Ingersoll in 1921: studies at this time were based on the assumption that gloss could be understood as an inherent physical property of a surface, and the priority was to devise a satisfactory method and scale to measure it reliably. As awareness of the complexity of perception grew, efforts were made to distinguish different types of gloss, although these generally still took the form of a search for objective physical measures to be solved within the visual system by means of inverse optics. It became more widely recognised approximately 20. years ago that models of gloss perception based on inverse optics were intractable and failed to explain experimental findings adequately. A temporary decline in the number of published studies followed; however the last decade or so has seen a renewal of interest in the perception of gloss, in an effort to map what is now understood to be a complex interaction of variables including illumination, surface properties and observer. This appears to have been driven by a number of factors, as the study of gloss re-emerged from research into other surface properties such as colour and texture, with technological advances paving the way for new experimental techniques and measurements. This review describes the main strands of research, tracking the changes in approach and theory which have triggered new avenues of research, to the current state of knowledge.},
author = {Chadwick, A. C. and Kentridge, R. W.},
doi = {10.1016/j.visres.2014.10.026},
file = {:Users/mshvarts/Downloads/1-s2.0-S0042698914002594-main.pdf:pdf},
issn = {18785646},
journal = {Vision Research},
keywords = {Gloss,Materials,Perception,Vision},
number = {PB},
pages = {221--235},
pmid = {25448119},
publisher = {Elsevier Ltd},
title = {{The perception of gloss: A review}},
url = {http://dx.doi.org/10.1016/j.visres.2014.10.026},
volume = {109},
year = {2015}
}
@article{Foley1981,
abstract = {Forced-choice psychometric functions were determined for the detection of sinewave gratings and contrast discrimination of near-threshold gratings at spatial frequencies of 0.5, 2 and 8 c/deg. Detection psychometric functions all had the same S-shaped form. Discrimination functions were almost linear except at the upper end. Both sets of data can be described well by a detection model with a positively accelerating relation between contrast and mean decision variable and a differencing decision rule. Results of a paired comparisons experiment were consistent with the model and indicate that decision variable variance is nearly constant over the range of contrasts used in these experiments. The implications of these results for several models of contrast detection and discrimination are considered. {\textcopyright} 1981.},
author = {Foley, John M. and Legge, Gordon E.},
doi = {10.1016/0042-6989(81)90009-2},
file = {:Users/mshvarts/Downloads/foley81{\_}contrast{\_}detection{\_}and{\_}near-threshold{\_}discrimination{\_}in{\_}human{\_}vision (1).pdf:pdf},
issn = {00426989},
journal = {Vision Research},
number = {7},
pages = {1041--1053},
pmid = {7314485},
title = {{Contrast detection and near-threshold discrimination in human vision}},
volume = {21},
year = {1981}
}
@article{Beck1984,
abstract = {Metelli has proposed a model of the intensity relationships in perceptual transparency based on Talbot's law of color fusion. Four constraints follow from the application of Talbot's law. Experiments 1 and 2 show that violations of constraints i and ii adversely affect the perception of transparency, while violations of constraints iii and iv do not. Many common occurrences of transparency are in terms of subtractive, rather than additive, color mixture. The constraints derived from the Metelli model are also shown to hold for subtractive color mixture with a filter. An assumption of the Metelli model is that the degree of perceived transparency varies linearly with reflectance. Experiment 3 indicates that the degree of perceived transparency with "partim transparency" varied linearly, not with reflectance, but with lightness, a nonlinear function of reflectance. Experiment 4 indicates that judgments of the degree of transparency with "complete transparency" are based on stimulus relations that differ from those that determine whether the perception of transparency occurs. The results are discussed in terms of how the pattern of intensities relates to other stimulus information, such as figural configuration, in producing the perception of transparency. {\textcopyright} 1984 Psychonomic Society, Inc.},
author = {Beck, Jacob and Prazdny, K. and Ivry, Richard},
doi = {10.3758/BF03203917},
file = {:Users/mshvarts/Downloads/Beck1984{\_}Article{\_}ThePerceptionOfTransparencyWit.pdf:pdf},
issn = {00315117},
journal = {Perception {\&} Psychophysics},
number = {5},
pages = {407--422},
pmid = {6462867},
title = {{The perception of transparency with achromatic colors}},
volume = {35},
year = {1984}
}
@article{BergmannTiest2007,
abstract = {In this study, we are interested in the following two questions: (1) how does perceived roughness correlate with physical roughness, and (2) how do visually and haptically perceived roughness compare? We used 96 samples of everyday materials, such as wood, paper, glass, sandpaper, ceramics, foams, textiles, etc. The samples were characterized by various different physical roughness measures, all determined from accurately measured roughness profiles. These measures consisted of spectral densities measured at different spatial scales and industrial roughness standards (Ra, Rq and Rz). In separate haptic and visual conditions, 12 na{\"{i}}ve subjects were instructed to order the 96 samples according to perceived roughness. The rank orders of both conditions were correlated with the various physical roughness measures. With most physical roughness measures, haptic and visual correspondence with the physical ordering was about equal. With others, haptic correspondence was slightly better. It turned out that different subjects ordered the samples using different criteria; for some subjects the correlation was better with roughness measures that were based on higher spatial frequencies, while others seemed to be paying more attention to the lower spatial frequencies. Also, physical roughness was not found to be the same as perceived roughness. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {{Bergmann Tiest}, Wouter M. and Kappers, Astrid M.L.},
doi = {10.1016/j.actpsy.2006.03.002},
file = {:Users/mshvarts/Downloads/1-s2.0-S0001691806000382-main.pdf:pdf},
issn = {00016918},
journal = {Acta Psychologica},
keywords = {Rank order,Roughness,Touch,Vision},
number = {2},
pages = {177--189},
pmid = {16684497},
title = {{Haptic and visual perception of roughness}},
volume = {124},
year = {2007}
}
@article{Wichmann2001,
abstract = {Scope of this contribution is twofold. First, it describes the potential of the global astrometry mission Gaia for detecting and measuring planetary systems based on detailed double-blind mode simulations and on the most recent predictions of the satellite's astrometric payload performances (launch is foreseen for late Summer 2012). Then, the identified capabilities are put in context by highlighting the contribution that the Gaia exoplanet discoveries will be able to bring to the science of extrasolar planets of the next decade.},
author = {Wichmann, Felix A. and Hill, N. Jeremy},
doi = {10.3758/BF03194544},
file = {:Users/mshvarts/Downloads/Wichmann-Hill2001{\_}Article{\_}ThePsychometricFunctionIFittin (1).pdf:pdf},
isbn = {0031-5117},
issn = {0031-5117},
journal = {Perception {\&} Psychophysics},
month = {nov},
number = {8},
pages = {1293--1313},
pmid = {11800458},
title = {{The psychometric function: I. Fitting, sampling, and goodness of fit}},
url = {http://link.springer.com/10.3758/BF03194544},
volume = {63},
year = {2001}
}
@article{Linares2016,
abstract = {quickpsy is a package to parametrically fit psychometric functions. In comparison with previous R packages, quickpsy was built to easily fit and plot data for multiple groups. Here, we describe the standard parametric model used to fit psychometric functions and the standard estimation of its parameters using maximum likelihood. We also provide examples of usage of quickpsy, including how allowing the lapse rate to vary can sometimes eliminate the bias in parameter estimation, but not in general. Finally, we describe some implementation details, such as how to avoid the problems associated to round-off errors in the maximisation of the likelihood or the use of closures and non-standard evaluation functions.},
author = {Linares, Daniel and L{\'{o}}pez-Moliner, Joan},
doi = {10.32614/rj-2016-008},
file = {:Users/mshvarts/Downloads/RJ-2016-008.pdf:pdf},
issn = {20734859},
journal = {R Journal},
number = {1},
pages = {122--131},
title = {{quickpsy: An R package to fit psychometric functions for multiple groups}},
volume = {8},
year = {2016}
}
@article{Prins2012,
abstract = {In their influential paper, Wichmann and Hill (2001) have shown that the threshold and slope estimates of a psychometric function may be severely biased when it is assumed that the lapse rate equals zero but lapses do, in fact, occur. Based on a large number of simulated experiments, Wichmann and Hill claim that threshold and slope estimates are essentially unbiased when one allows the lapse rate to vary within a rectangular prior during the fitting procedure. Here, I replicate Wichmann and Hill's finding that significant bias in parameter estimates results when one assumes that the lapse rate equals zero but lapses do occur, but fail to replicate their finding that freeing the lapse rate eliminates this bias. Instead, I show that significant and systematic bias remains in both threshold and slope estimates even when one frees the lapse rate according to Wichmann and Hill's suggestion. I explain the mechanisms behind the bias and propose an alternative strategy to incorporate the lapse rate into psychometric function models, which does result in essentially unbiased parameter estimates. {\textcopyright} 2012 ARVO.},
author = {Prins, Nicolaas},
doi = {10.1167/12.6.25},
file = {:Users/mshvarts/Downloads/i1534-7362-12-6-25.pdf:pdf},
issn = {15347362},
journal = {Journal of Vision},
keywords = {Lapse rate,Maximum-likelihood,Psychometric function,Psychophysical methods},
number = {6},
pages = {1--16},
pmid = {22715196},
title = {{The psychometric function: The lapse rate revisited}},
volume = {12},
year = {2012}
}
@article{Schlittenlacher2018,
abstract = {{\textcopyright} 2018 Acoustical Society of America. Two methods for estimating audiograms quickly and accurately using Bayesian active learning were developed and evaluated. Both methods provided an estimate of threshold as a continuous function of frequency. For one method, six successive tones with decreasing levels were presented on each trial and the task was to count the number of tones heard. A Gaussian Process was used for classification and maximum-information sampling to determine the frequency and levels of the stimuli for the next trial. The other method was based on a published method using a Yes/No task but extended to account for lapses. The obtained audiograms were compared to conventional audiograms for 40 ears, 19 of which were hearing impaired. The threshold estimates for the active-learning methods were systematically from 2 to 4 dB below (better than) those for the conventional audiograms, which may indicate a less conservative response criterion (a greater willingness to respond for a given amount of sensory information). Both active-learning methods were able to allow for wrong button presses (due to lapses of attention) and provided accurate audiogram estimates in less than 50 trials or 4 min. For a given level of accuracy, the counting task was slightly quicker than the Yes/No task.},
author = {Schlittenlacher, Josef and Turner, Richard E. and Moore, Brian C. J.},
doi = {10.1121/1.5047436},
file = {:Users/mshvarts/Downloads/1.5047436.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {421--430},
pmid = {30075695},
title = {{Audiogram estimation using Bayesian active learning}},
url = {http://dx.doi.org/10.1121/1.5047436},
volume = {144},
year = {2018}
}
@article{Schlittenlacher2020,
abstract = {Time-efficient hearing tests are important in both clinical practice and research studies. This particularly applies to notched-noise tests, which are rarely done in clinical practice because of the time required. Auditory-filter shapes derived from notched-noise data may be useful for diagnosis of the cause of hearing loss and for fitting of hearing aids, especially if measured over a wide range of center frequencies. To reduce the testing time, we applied Bayesian active learning (BAL) to the notched-noise test, picking the most informative stimulus parameters for each trial based on nine Gaussian Processes. A total of 11 hearing-impaired subjects were tested. In 20 to 30 min, the test provided estimates of signal threshold as a continuous function of frequency from 500 to 4000 Hz for nine notch widths and for notches placed both symmetrically and asymmetrically around the signal frequency. The thresholds were found to be consistent with those obtained using a 2-up/1-down forced-choice procedure at a single center frequency. In particular, differences in threshold between the methods did not vary with notch width. An independent second run of the BAL test for one notch width showed that it is reliable. The data derived from the BAL test were used to estimate auditory-filter width and asymmetry and detection efficiency for center frequencies from 500 to 4000 Hz. The results agreed with expectations for cochlear hearing losses that were derived from the audiogram and a hearing model.},
author = {Schlittenlacher, Josef and Turner, Richard E. and Moore, Brian C.J.},
doi = {10.1177/2331216520952992},
file = {:Users/mshvarts/Downloads/10.1177{\_}2331216520952992.pdf:pdf},
issn = {23312165},
journal = {Trends in Hearing},
keywords = {Bayesian active learning,auditory filter,hearing test,notched noise},
pmid = {33073723},
title = {{Application of Bayesian Active Learning to the Estimation of Auditory Filter Shapes Using the Notched-Noise Method}},
volume = {24},
year = {2020}
}
@article{Brand2002,
abstract = {The minimum standard deviations achievable for concurrent estimates of thresholds and psychometric function slopes as well as the optimal target values for adaptive procedures are calculated as functions of stimulus level and track length on the basis of the binomial theory. The optimum pair of targets for a concurrent estimate is found at the correct response probabilities p1 = 0.19 and p2 = 0.81 for the logistic psychometric function. An adaptive procedure that converges at these optimal targets is introduced and tested with Monte Carlo simulations. The efficiency increases rapidly when each subject's response consists of more than one statistically independent Bernoulli trial. Sentence intelligibility tests provide more than one Bernoulli trial per sentence when each word is scored separately. The number of within-sentence trials can be quantified by the j factor [Boothroyd and Nittrouer, J. Acoust. Soc. Am. 84, 101-114 (1988)]. The adaptive procedure was evaluated with 10 normal-hearing and 11 hearing-impaired listeners using two German sentence tests that differ in j factors. The expected advantage of the sentence test with the higher j factor was not observed, possibly due to training effects. Hence, the number of sentences required for a reliable speech reception threshold (approximately 1 dB standard deviation) concurrently with a slope estimate (approximately 20{\%}-30{\%} relative standard deviation) is at least N = 30 if word scoring for short, meaningful sentences (j approximately 2) is performed.},
author = {Brand, Thomas and Kollmeier, Birger},
doi = {10.1121/1.1479152},
file = {:Users/mshvarts/Downloads/1.1479152.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {6},
pages = {2801--2810},
pmid = {12083215},
title = {{Efficient adaptive procedures for threshold and concurrent slope estimates for psychophysics and speech intelligibility tests}},
volume = {111},
year = {2002}
}
@article{DiMattina2015,
abstract = {Recently in vision science there has been great interest in understanding the perceptual representations of complex multidimensional stimuli. Therefore, it is becoming very important to develop methods for performing psychophysical experiments with multidimensional stimuli and efficiently estimating psychometric models that have multiple free parameters. In this methodological study, I analyze three efficient implementations of the popular W method for adaptive data collection, two of which are novel approaches to psychophysical experiments. Although the standard implementation of the $\Psi$ procedure is intractable in higher dimensions, I demonstrate that my implementations generalize well to complex psychometric models defined in multidimensional stimulus spaces and can be implemented very efficiently on standard laboratory computers. I show that my implementations may be of particular use for experiments studying how subjects combine multiple cues to estimate sensory quantities. I discuss strategies for speeding up experiments and suggest directions for future research in this rapidly growing area at the intersection of cognitive science, neuroscience, and machine learning.},
author = {DiMattina, Christopher},
doi = {10.1167/15.9.5},
file = {:Users/mshvarts/Downloads/i1534-7362-15-9-5.pdf:pdf},
issn = {15347362},
journal = {Journal of Vision},
keywords = {Adaptive stimulus generation,Computational modeling,Psychophysical methods},
number = {9},
pages = {1--20},
pmid = {26200886},
title = {{Fast adaptive estimation of multidimensional psychometric functions}},
volume = {15},
year = {2015}
}
@article{Hall1981,
abstract = {Adaptive psychophysical procedures that have been described in the literature generally fall into one of two categories. (1) Simple procedures, such as UDTR and PEST, can be implemented without an on-line computer. The decision to change testing level is based on the outcome of the few most recent trials, and the final estimate of threshold is given by the final testing level or the average of a few testing levels. (2) “Maximum likelihood” methods require an on-line computer. A parametric form of the psychometric function is assumed, and after each trial a maximum-likelihood estimate of the parameters of the psychometric function is made on the basis of all preceding trials. This estimate is used to set the next testing level and to estimate threshold. We describe here a hybrid procedure, in which testing levels are determined by PEST and the final estimate of threshold is made by fitting an assumed psychometric function to all preceding trials. The PEST rules were tuned to yield results that were accurate and insensitive to errors in initial estimates of the psychometric function. These parameters differ from those that yield optimum results with classical PEST. Results of computer simulations and of experiments with human subjects are presented. {\textcopyright} 1981, Acoustical Society of America. All rights reserved.},
author = {Hall, J. L.},
doi = {10.1121/1.385912},
file = {:Users/mshvarts/Downloads/1.385912.pdf:pdf},
issn = {NA},
journal = {Journal of the Acoustical Society of America},
number = {6},
pages = {1763--1769},
pmid = {7240589},
title = {{Hybrid adaptive procedure for estimation of psychometric functions}},
volume = {69},
year = {1981}
}
@article{Kujala2006,
abstract = {We propose a new psychometric model for two-dimensional stimuli, such as color differences, based on parameterizing the threshold of a one-dimensional psychometric function as an ellipse. The $\Psi$ Bayesian adaptive estimation method applied to this model yields trials that vary in multiple stimulus dimensions simultaneously. Simulations indicate that this new procedure can be much more efficient than the more conventional procedure of estimating the psychometric function on one-dimensional lines independently, requiring only one-fourth or less the number of trials for equivalent performance in typical situations. In a real psychophysical experiment with a yes-no task, as few as 22 trials per estimated threshold ellipse were enough to consistently demonstrate certain color appearance phenomena. We discuss the practical implications of the multidimensional adaptation. In order to make the application of the model practical, we present two significantly faster algorithms for running the $\Psi$ method: a discretized algorithm utilizing the Fast Fourier Transform for better scaling with the sampling rates and a Monte Carlo particle filter algorithm that should be able to scale into even more dimensions. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Kujala, Janne V. and Lukka, Tuomas J.},
doi = {10.1016/j.jmp.2005.12.005},
file = {:Users/mshvarts/Downloads/1-s2.0-S0022249605001306-main.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Bayesian adaptive method,Fast Fourier Transform,Markov chain Monte Carlo,Particle filter algorithm,Psychometric function,Two-dimensional stimuli},
number = {4},
pages = {369--389},
title = {{Bayesian adaptive estimation: The next dimension}},
volume = {50},
year = {2006}
}
@article{Lesmes2010,
abstract = {The contrast sensitivity function (CSF) predicts functional vision better than acuity, but long testing times prevent its psychophysical assessment in clinical and practical applications. This study presents the quick CSF (qCSF) method, a Bayesian adaptive procedure that applies a strategy developed to estimate multiple parameters of the psychometric function (A. B. Cobo-Lewis, 1996; L. L. Kontsevich {\&} C. W. Tyler, 1999). Before each trial, a one-step-ahead search finds the grating stimulus (defined by frequency and contrast) that maximizes the expected information gain (J. V. Kujala {\&} T. J. Lukka, 2006; L. A. Lesmes et al., 2006), about four CSF parameters. By directly estimating CSF parameters, data collected at one spatial frequency improves sensitivity estimates across all frequencies. A psychophysical study validated that CSFs obtained with 100 qCSF trials (-10 min) exhibited good precision across spatial frequencies (SD {\textless} 2-3 dB) and excellent agreement with CSFs obtained independently (mean RMSE = 0.86 dB). To estimate the broad sensitivity metric provided by the area under the log CSF (AULCSF), only 25 trials were needed to achieve a coefficient of variation of 15-20{\%}. The current study demonstrates the method's value for basic and clinical investigations. Further studies, applying the qCSF to measure wider ranges of normal and abnormal vision, will determine how its efficiency translates to clinical assessment. {\textcopyright} ARVO.},
author = {Lesmes, Luis Andres and Lu, Zhong Lin and Baek, Jongsoo and Albright, Thomas D.},
doi = {10.1167/10.3.17},
file = {:Users/mshvarts/Downloads/jov-10-3-17.pdf:pdf},
issn = {15347362},
journal = {Journal of Vision},
keywords = {AULCSF,Adaptive,Forced-choice,Psychophysics,Spatial frequency},
number = {3},
pages = {1--21},
pmid = {20377294},
title = {{Bayesian adaptive estimation of the contrast sensitivity function: The quick CSF method}},
volume = {10},
year = {2010}
}
@article{Georgeson1975a,
author = {Georgeson, M A and Sullivan, G D},
doi = {10.1113/jphysiol.1975.sp011162},
file = {:Users/mshvarts/Downloads/jphysiol.1975.sp011162 (1).pdf:pdf},
issn = {00223751},
journal = {The Journal of Physiology},
month = {nov},
number = {3},
pages = {627--656},
title = {{Contrast constancy: deblurring in human vision by spatial frequency channels.}},
url = {http://doi.wiley.com/10.1113/jphysiol.1975.sp011162},
volume = {252},
year = {1975}
}
@article{Schutt2016,
abstract = {The psychometric function describes how an experimental variable, such as stimulus strength, influences the behaviour of an observer. Estimation of psychometric functions from experimental data plays a central role in fields such as psychophysics, experimental psychology and in the behavioural neurosciences. Experimental data may exhibit substantial overdispersion, which may result from non-stationarity in the behaviour of observers. Here we extend the standard binomial model which is typically used for psychometric function estimation to a beta-binomial model. We show that the use of the beta-binomial model makes it possible to determine accurate credible intervals even in data which exhibit substantial overdispersion. This goes beyond classical measures for overdispersion-goodness-of-fit-which can detect overdispersion but provide no method to do correct inference for overdispersed data. We use Bayesian inference methods for estimating the posterior distribution of the parameters of the psychometric function. Unlike previous Bayesian psychometric inference methods our software implementation-psignifit 4-performs numerical integration of the posterior within automatically determined bounds. This avoids the use of Markov chain Monte Carlo (MCMC) methods typically requiring expert knowledge. Extensive numerical tests show the validity of the approach and we discuss implications of overdispersion for experimental design. A comprehensive MATLAB toolbox implementing the method is freely available; a python implementation providing the basic capabilities is also available.},
author = {Sch{\"{u}}tt, Heiko H. and Harmeling, Stefan and Macke, Jakob H. and Wichmann, Felix A.},
doi = {10.1016/j.visres.2016.02.002},
file = {:Users/mshvarts/Downloads/1-s2.0-S0042698916000390-main.pdf:pdf},
issn = {18785646},
journal = {Vision Research},
keywords = {Bayesian inference,Beta-binomial model,Confidence intervals,Credible intervals,Non-stationarity,Overdispersion,Psychometric function,Psychophysical methods},
pages = {105--123},
pmid = {27013261},
title = {{Painfree and accurate Bayesian estimation of psychometric functions for (potentially) overdispersed data}},
volume = {122},
year = {2016}
}
@article{Thompson1933,
author = {Thompson, William R.},
doi = {10.2307/2332286},
file = {:Users/mshvarts/Downloads/1933-thompson.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
month = {dec},
number = {3/4},
pages = {285},
title = {{On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples}},
url = {https://www.jstor.org/stable/2332286?origin=crossref},
volume = {25},
year = {1933}
}
