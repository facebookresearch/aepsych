%! Author = lucyowen
%! Date = 11/18/20

% Preamble
\documentclass[../main.tex]{subfiles}


% Document
\begin{document}

We recognize that a number of methods and ideas we introduce in our work are well-known in the statistics and machine learning communities but relatively unfamiliar to psychology researchers (though see \cite{Schulz2018} for one attempt to improve this). This creates a practical barrier to usage of these new adaptive methods in the field. Furthermore, the most popular adaptive methods share the distinction of there being a robust public implementation of the method available. Therefore, an important component of our contribution is \texttt{AEPsych}, a flexible toolbox that supports the usage and further development of these methods for psychophysics.

\subsection{A state of the art base}

The modeling functionality in \texttt{AEPsych} is wrapping and extending \texttt{gpytorch} \citep{Gardner2018}, one of the main state of the art GP modeling packages. The active learning and acquisition components wrap \texttt{botorch} \citep{Balandat2020}, a state of the art package for Bayesian optimization and active learning. Those packages themselves are built on top of PyTorch, one of the dominant software toolkits for machine learning (ML). Building on these toolkits helps ensure not only that we are relying on the current state of the art in GP modeling and Bayesian optimization, but that further advances and extensions in the broader GP and ML communities are automatically inherited by \texttt{AEPsych}. At the same time, we provide out-of-the-box implementations of the models described in this paper as well as the prior linear-additive GP model so that practitioners do not need to interact with these other packages if this is not their research interest.

\subsection{Meeting researchers where they are}

The dependency on the PyTorch ecosystem means that the core modeling components of \texttt{AEPsych} are implemented in Python. However, we recognize that not all researchers work in Python and not all perceptual experiments can be easily implemented in the language. To support the actual way researchers work, \texttt{AEPsych} is able to operate in a mode that allows the interaction with the participant to happen in another language according to the user's preference. To do this, it uses a Python sever for modeling and selection of the stimulus to display next; the server communicates with a client (in any language) that actually displays the stimulus and collects the response. The client and the server can run on the same machine: we use the network interface as a lightweight way to integrate applications in different languages. We provide client implementations for Python, Unity (C\#) and MATLAB to capture the typical ways perceptual researchers work. The client-server interface is very lightweight, consisting of a small number of message types encoded as text in JSON format. We also provide Docker images of the server components, so that users who otherwise do not use Python can have a single monolithic install of our tooling without needing to build a full Python development environment. In addition, we have a lightweight experiment configuration framework using text-based INI configuration files, so an experiment can be configured without editing any Python code, and we provide example configurations for standard experiments. Finally, \texttt{AEPsych} includes a set of tutorials for typical use cases and a large suite of unit tests to ensure correct functionality.

\subsection{Serving the full experimentation use case: pre- and post-experiment}
An important requirement before any experimental data are collected is to understand the amount of data needed. \texttt{AEPsych} provides benchmarking and power analysis tools that practitioners can use to plan their experiments: given a set of assumptions about the shape of the psychometric field and response noise levels, the benchmark module can simulate a full experiment. By repeating this simulation, the researcher can assess typical estimation error and data needs under different assumptions, and use this to design their real experiment. The configuration language for benchmarks and real experiments is identical, so the best benchmark configuration can then be used in a real experiment. The benchmarking module is available in Python only and supports parallelized simulation for efficiency. It was used to generate all of the simulation results in this paper.

An important component after the experiment is data analysis and visualization. \texttt{AEPsych} includes pre-implemented models for analyzing psychophysical data. These models extend what is possible for real-time experimentation by supporting more accurate inference using Markov chain Monte Carlo (MCMC) techniques rather than variational inference, and also provide hierarchical models that can aggregate across participants to generate average psychometric fields while integrating over subject-specific biases. These models are implemented in Stan \citep{Carpenter2017}, a declarative modeling language in which models are close in notation to the underlying mathematical formulation, and which compiles into highly efficient MCMC samplers.

\subsection{A brief overview of the design of AEPsych API}
For flexible implementation of new models, \texttt{AEPsych} has a simple hierarchy of class interfaces. At the lowest level is a \texttt{Modelbridge}, which combines a \texttt{gpytorch} model and acquisition function in one object. Next, \texttt{Strategy} objects describe ways of sampling new observations, which can be based on data the model has seen so far (a \texttt{ModelWrapperStrategy}) or not based on a model (e.g.\ \texttt{SobolStrategy}). \texttt{Strategy} objects can also be composed together, for example creating a sequential strategy that begins with random or Sobol trials and then switches to a full GP model, or a sequential strategy that begins with a simpler model and switches to a more complex model as more data is acquired. Finally, a server object operates a strategy (or set of strategies, for independent interleaved experiments) and logs all data to a local database. The database supports full replay of experiment sessions, and we additionally provide a utility to output the collected stimuli and responses into a text-based CSV file.


\end{document}
