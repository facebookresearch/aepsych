\documentclass[../main.tex]{subfiles}

\begin{document}

An objective of psychophysicial research is to estimate the transformation of the perceptual system on a stimulus, and to understand how this transformation operates across contexts and different stimuli. This transformation function cannot be measured directly, but is rather inferred from subjective participant responses. Conventional psychophysical experiments do this by averaging participant responses to each stimulus over many repetitions to arrive at average detection probabilities, which are then modeled using some parametric form (often linear). This requires large number of trials per point in the input domain of the psychometric function (e.g.\ stimulus intensity). Furthermore, classical methods attempt to densely sample this input domain, suffering from the \emph{curse of dimensionality} wherein the number of points needed to fill a space grows exponentially in the number of dimensions. Additional challenges arise when extending models to include non-linear interactions between stimulus features (i.e.\ when response probabilities have a complex relationship to multiple stimulus parameters) or when the stimulus features are noisy or coded on the wrong scale (i.e.\ stimulus features must be coded in the correct units for classical models like the Weber-Fechner law to apply). Finally, traditional methods need to perform different experiments to estimate different aspects of the psychometric transfer function such as detection and discrimination thresholds. This is required because the standard model for discrimination, Weber's law, breaks down at low intensities, so so-called \emph{sub-threshold} and \emph{supra-threshold} behavior are modeled separately \citep[e.g.][]{Georgeson1975,Guan2016}.

Our work, while not first in addressing some of these challenges, is among the first in addressing them all together. First, we treat the value of the psychometric transfer function as a latent variable, rather than computing it from averages post-hoc. This is consistent with common practice in psychology and some psychometric fitting toolboxes \citep[e.g.]{Schutt2016}, though less common in experimental usage. Practically, it means we almost never need to sample the exact same point twice, since we can learn more from a closely adjacent point. Second, we sample the input domain adaptively based on participant responses rather than using a predetermined set of stimuli, which allows us to partially mitigate the curse of dimensionality. In particular, we introduce \emph{level set estimation} (LSE), an objective that explicitly targets estimation of psychometric thresholds. Third, we use a nonparametric model for the psychometric transfer function $f$, which allows us to make fewer assumptions about the shape of the psychometric curve. This means we can handle nonlinear interactions between input features, and nonlinear input scaling. And finally, our model jointly covers sub-threshold and supra-threshold behavior in a formal generalization of both detection and discrimination models, and as such allows for estimation of both slopes and intercepts of the psychometric function from a single experiment.

To enable practitioners to apply our method to their own domains, we are making available a public implementation of all the methods in this paper, as well as the evaluation and benchmark code used to generate our results. This code can be used to evaluate new models and test functions, and additionally supports interfacing with Python, Matlab, or Unity for human-in-the-loop experimentation.

To demonstrate the distinct benefits of both our novel modeling and stimulus selection contributions, we show extensive simulation results using both previously-reported and novel test functions derived from real data. Specifically, we demonstrate sample efficiency benefits of up to 10x even relative to previous adaptive methods, and potentially far more relative to the method of constant stimuli, without the strong parametric assumptions of past adaptive methods.

The paper proceeds as follows. First, we provide a longer background review of psychophysics theory and adaptive methods for psychophysics. Next, we show how a particular Bayesian nonparametric model, the Gaussian process (GP) with a probit likelihood, can be thought of as a formal generalization of classical theory, supporting its use for adaptive data collection on theoretical as well as empirical grounds. We then demonstrate the benefits of the method with a thorough empirical study comparing to adaptive psychophysics methods in common use. Finally, we provide an overview of the key features of \texttt{AEPsych}, our new package for adaptive experimentation in psychophysics, and conclude with a discussion and broader outlook.


\end{document}
